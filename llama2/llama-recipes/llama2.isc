isc_project_id = "<isc_project_id>"
experiment_name = "llama2-7b-lora-fsdp"
gpus = 20
command = '''
source ~/.llama2/bin/activate && 
cd ~/isc-demos/llama2/llama-recipes/ && 
torchrun --nnodes=10 --nproc-per-node=6 --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT --node_rank=$RANK 
examples/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' 
--use_fp16 --mixed_precision --model_name <path-to-model> --batch_size_training 1 --dist_checkpoint_root_folder $OUTPUT_PATH 
--dist_checkpoint_folder llama2'''

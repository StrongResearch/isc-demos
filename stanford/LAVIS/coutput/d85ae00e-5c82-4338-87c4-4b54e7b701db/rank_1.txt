WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:31:04.390989] Starting job
[2023-11-13 00:31:04.391005] Starting job
[2023-11-13 00:31:04.391946] Starting job
[2023-11-13 00:31:04.391985] Starting job
| distributed init (rank 8, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 11, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:31:08.958699] Starting job
| distributed init (rank 10, world 12): env://
[2023-11-13 00:31:09.257225] Starting job
| distributed init (rank 9, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 166/4722]  eta: 5:22:56  lr: 0.000010  loss: 6.7508  loss_lm: 6.7508 (6.7508)  time: 4.2529  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 166/4722]  eta: 5:22:52  lr: 0.000010  loss: 6.8710  loss_lm: 6.8710 (6.8710)  time: 4.2521  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 166/4722]  eta: 5:23:00  lr: 0.000010  loss: 6.7238  loss_lm: 6.7238 (6.7238)  time: 4.2539  data: 0.0000  max mem: 18863Train: data epoch: [0]  [ 166/4722]  eta: 5:22:56  lr: 0.000010  loss: 6.5184  loss_lm: 6.5184 (6.5184)  time: 4.2529  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 166/4722]  eta: 5:22:52  lr: 0.000010  loss: 6.5407  loss_lm: 6.5407 (6.5407)  time: 4.2520  data: 0.0000  max mem: 18866


Train: data epoch: [0]  [ 166/4722]  eta: 5:22:46  lr: 0.000010  loss: 6.8021  loss_lm: 6.8021 (6.8021)  time: 4.2507  data: 0.0000  max mem: 18867
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 167/4722]  eta: 3:38:40  lr: 0.000010  loss: 7.1156  loss_lm: 6.7238 (6.9197)  time: 2.8804  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 167/4722]  eta: 3:38:37  lr: 0.000010  loss: 6.8385  loss_lm: 6.5184 (6.6785)  time: 2.8799  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 167/4722]  eta: 3:38:38  lr: 0.000010  loss: 7.0491  loss_lm: 6.7508 (6.8999)  time: 2.8799  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 167/4722]  eta: 3:38:35  lr: 0.000010  loss: 6.7289  loss_lm: 6.5407 (6.6348)  time: 2.8795  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 167/4722]  eta: 3:38:32  lr: 0.000010  loss: 6.6590  loss_lm: 6.6590 (6.7305)  time: 2.8786  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 167/4722]  eta: 3:38:36  lr: 0.000010  loss: 6.8366  loss_lm: 6.8366 (6.8538)  time: 2.8795  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 168/4722]  eta: 3:03:01  lr: 0.000010  loss: 6.4750  loss_lm: 6.5407 (6.5815)  time: 2.4114  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 168/4722]  eta: 3:03:02  lr: 0.000010  loss: 6.8778  loss_lm: 6.8385 (6.7449)  time: 2.4117  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 168/4722]  eta: 3:02:58  lr: 0.000010  loss: 6.9745  loss_lm: 6.8021 (6.8118)  time: 2.4108  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [ 168/4722]  eta: 3:03:01  lr: 0.000010  loss: 7.2875  loss_lm: 6.8710 (6.9984)  time: 2.4114  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 168/4722]  eta: 3:03:04  lr: 0.000010  loss: 6.4440  loss_lm: 6.7238 (6.7611)  time: 2.4120  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 168/4722]  eta: 3:03:02  lr: 0.000010  loss: 6.9269  loss_lm: 6.9269 (6.9089)  time: 2.4117  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 169/4722]  eta: 2:45:07  lr: 0.000010  loss: 6.7923  loss_lm: 6.5407 (6.6342)  time: 2.1761  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 169/4722]  eta: 2:45:07  lr: 0.000010  loss: 6.5844  loss_lm: 6.8366 (6.8949)  time: 2.1761  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 169/4722]  eta: 2:45:05  lr: 0.000010  loss: 6.8202  loss_lm: 6.8021 (6.8139)  time: 2.1757  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [ 169/4722]  eta: 2:45:10  lr: 0.000010  loss: 6.9594  loss_lm: 6.7238 (6.8107)  time: 2.1766  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 169/4722]  eta: 2:45:08  lr: 0.000010  loss: 6.7111  loss_lm: 6.7111 (6.7365)  time: 2.1763  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 169/4722]  eta: 2:45:08  lr: 0.000010  loss: 6.7126  loss_lm: 6.7508 (6.8598)  time: 2.1764  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 170/4722]  eta: 3:54:17  lr: 0.000010  loss: 7.1117  loss_lm: 6.8385 (6.8115)  time: 3.0883  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 170/4722]  eta: 3:54:18  lr: 0.000010  loss: 7.1003  loss_lm: 6.9594 (6.8686)  time: 3.0885  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 170/4722]  eta: 3:54:17  lr: 0.000010  loss: 6.1641  loss_lm: 6.7508 (6.7207)  time: 3.0883  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 170/4722]  eta: 3:54:15  lr: 0.000010  loss: 6.9951  loss_lm: 6.8202 (6.8502)  time: 3.0878  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 170/4722]  eta: 3:54:17  lr: 0.000010  loss: 6.8353  loss_lm: 6.7289 (6.6744)  time: 3.0881  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 170/4722]  eta: 3:54:17  lr: 0.000010  loss: 6.5774  loss_lm: 6.8366 (6.8314)  time: 3.0881  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 171/4722]  eta: 3:33:53  lr: 0.000010  loss: 7.0567  loss_lm: 6.9594 (6.9000)  time: 2.8200  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 171/4722]  eta: 3:33:51  lr: 0.000010  loss: 6.9066  loss_lm: 6.8202 (6.8596)  time: 2.8194  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 171/4722]  eta: 3:33:53  lr: 0.000010  loss: 6.8709  loss_lm: 6.8385 (6.8214)  time: 2.8198  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 171/4722]  eta: 3:33:52  lr: 0.000010  loss: 6.9014  loss_lm: 6.8366 (6.8431)  time: 2.8197  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 171/4722]  eta: 3:33:52  lr: 0.000010  loss: 6.7957  loss_lm: 6.7289 (6.6946)  time: 2.8197  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 171/4722]  eta: 3:33:53  lr: 0.000010  loss: 6.8176  loss_lm: 6.7508 (6.7368)  time: 2.8198  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 172/4722]  eta: 3:19:37  lr: 0.000010  loss: 6.9092  loss_lm: 6.9066 (6.8667)  time: 2.6323  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 172/4722]  eta: 3:19:38  lr: 0.000010  loss: 7.0768  loss_lm: 6.8709 (6.8579)  time: 2.6327  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 172/4722]  eta: 3:19:39  lr: 0.000010  loss: 6.8106  loss_lm: 6.9594 (6.8872)  time: 2.6329  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 172/4722]  eta: 3:19:38  lr: 0.000010  loss: 6.9874  loss_lm: 6.7923 (6.7365)  time: 2.6326  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 172/4722]  eta: 3:19:38  lr: 0.000010  loss: 6.8018  loss_lm: 6.8366 (6.8372)  time: 2.6326  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 172/4722]  eta: 3:19:38  lr: 0.000010  loss: 6.5122  loss_lm: 6.7508 (6.7047)  time: 2.6327  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 173/4722]  eta: 3:08:39  lr: 0.000010  loss: 7.3841  loss_lm: 6.8709 (6.9237)  time: 2.4882  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 173/4722]  eta: 3:08:38  lr: 0.000010  loss: 6.8355  loss_lm: 6.7923 (6.7488)  time: 2.4881  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 173/4722]  eta: 3:08:37  lr: 0.000010  loss: 6.1555  loss_lm: 6.8202 (6.7778)  time: 2.4879  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 173/4722]  eta: 3:08:38  lr: 0.000010  loss: 6.2868  loss_lm: 6.7126 (6.6525)  time: 2.4882  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 173/4722]  eta: 3:08:39  lr: 0.000010  loss: 7.1041  loss_lm: 6.9594 (6.9143)  time: 2.4884  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 173/4722]  eta: 3:08:38  lr: 0.000010  loss: 6.8774  loss_lm: 6.8366 (6.8422)  time: 2.4881  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 174/4722]  eta: 3:00:02  lr: 0.000010  loss: 7.3752  loss_lm: 7.0567 (6.9655)  time: 2.3752  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 174/4722]  eta: 3:00:01  lr: 0.000010  loss: 7.0326  loss_lm: 6.8778 (6.9358)  time: 2.3751  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 174/4722]  eta: 3:00:01  lr: 0.000010  loss: 6.6523  loss_lm: 6.7923 (6.7381)  time: 2.3749  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 174/4722]  eta: 3:00:00  lr: 0.000010  loss: 6.9337  loss_lm: 6.9066 (6.7951)  time: 2.3748  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 174/4722]  eta: 3:00:01  lr: 0.000010  loss: 6.7494  loss_lm: 6.8366 (6.8319)  time: 2.3749  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 174/4722]  eta: 3:00:01  lr: 0.000010  loss: 7.0263  loss_lm: 6.7508 (6.6940)  time: 2.3750  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 175/4722]  eta: 3:34:49  lr: 0.000010  loss: 6.4538  loss_lm: 6.9594 (6.9143)  time: 2.8348  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 175/4722]  eta: 3:34:49  lr: 0.000010  loss: 6.8041  loss_lm: 6.8709 (6.9226)  time: 2.8347  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 175/4722]  eta: 3:34:48  lr: 0.000010  loss: 6.9098  loss_lm: 6.8366 (6.8397)  time: 2.8345  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 175/4722]  eta: 3:34:49  lr: 0.000010  loss: 6.4981  loss_lm: 6.7126 (6.6744)  time: 2.8346  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 175/4722]  eta: 3:34:47  lr: 0.000010  loss: 6.9985  loss_lm: 6.9066 (6.8154)  time: 2.8344  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 175/4722]  eta: 3:34:48  lr: 0.000010  loss: 6.9850  loss_lm: 6.7923 (6.7628)  time: 2.8345  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 176/4722]  eta: 3:25:25  lr: 0.000010  loss: 6.4834  loss_lm: 6.7923 (6.7374)  time: 2.7112  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 176/4722]  eta: 3:25:25  lr: 0.000010  loss: 6.7726  loss_lm: 6.8366 (6.8336)  time: 2.7112  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 176/4722]  eta: 3:25:24  lr: 0.000010  loss: 6.7872  loss_lm: 6.9066 (6.8129)  time: 2.7111  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 176/4722]  eta: 3:25:26  lr: 0.000010  loss: 6.5375  loss_lm: 6.9594 (6.8801)  time: 2.7115  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 176/4722]  eta: 3:25:25  lr: 0.000010  loss: 6.6626  loss_lm: 6.7126 (6.6734)  time: 2.7113  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 176/4722]  eta: 3:25:25  lr: 0.000010  loss: 6.7476  loss_lm: 6.8709 (6.9067)  time: 2.7114  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 177/4722]  eta: 3:17:33  lr: 0.000010  loss: 6.5650  loss_lm: 6.7289 (6.7230)  time: 2.6081  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 177/4722]  eta: 3:17:33  lr: 0.000010  loss: 6.4968  loss_lm: 6.8018 (6.8055)  time: 2.6081  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 177/4722]  eta: 3:17:34  lr: 0.000010  loss: 6.4134  loss_lm: 6.8106 (6.8412)  time: 2.6083  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 177/4722]  eta: 3:17:34  lr: 0.000010  loss: 6.6284  loss_lm: 6.8385 (6.8835)  time: 2.6082  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 177/4722]  eta: 3:17:34  lr: 0.000010  loss: 6.9637  loss_lm: 6.7126 (6.6976)  time: 2.6082  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 177/4722]  eta: 3:17:33  lr: 0.000010  loss: 6.6210  loss_lm: 6.8202 (6.7969)  time: 2.6080  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 178/4722]  eta: 3:10:55  lr: 0.000010  loss: 6.7193  loss_lm: 6.7289 (6.7227)  time: 2.5210  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 178/4722]  eta: 3:10:56  lr: 0.000010  loss: 6.5971  loss_lm: 6.8385 (6.8615)  time: 2.5211  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 178/4722]  eta: 3:10:56  lr: 0.000010  loss: 7.1735  loss_lm: 6.9594 (6.8668)  time: 2.5212  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [ 178/4722]  eta: 3:10:55  lr: 0.000010  loss: 6.5257  loss_lm: 6.7126 (6.6843)  time: 2.5211  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 178/4722]  eta: 3:10:54  lr: 0.000010  loss: 7.1630  loss_lm: 6.9066 (6.8250)  time: 2.5209  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 178/4722]  eta: 3:10:55  lr: 0.000010  loss: 6.9609  loss_lm: 6.8366 (6.8175)  time: 2.5210  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 179/4722]  eta: 3:05:16  lr: 0.000010  loss: 6.5774  loss_lm: 6.7193 (6.7124)  time: 2.4469  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 179/4722]  eta: 3:05:15  lr: 0.000010  loss: 6.5770  loss_lm: 6.8202 (6.8073)  time: 2.4467  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 179/4722]  eta: 3:05:16  lr: 0.000010  loss: 6.4565  loss_lm: 6.8041 (6.8325)  time: 2.4469  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 179/4722]  eta: 3:05:16  lr: 0.000010  loss: 6.7163  loss_lm: 6.8106 (6.8560)  time: 2.4470  data: 0.0000  max mem: 19014Train: data epoch: [0]  [ 179/4722]  eta: 3:05:16  lr: 0.000010  loss: 6.5673  loss_lm: 6.6626 (6.6760)  time: 2.4469  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 179/4722]  eta: 3:05:16  lr: 0.000010  loss: 6.9477  loss_lm: 6.8366 (6.8268)  time: 2.4469  data: 0.0000  max mem: 18910



Train: data epoch: [0]  [ 180/4722]  eta: 3:29:38  lr: 0.000010  loss: 6.3491  loss_lm: 6.8041 (6.8003)  time: 2.7694  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 180/4722]  eta: 3:29:38  lr: 0.000010  loss: 6.6470  loss_lm: 6.7193 (6.7080)  time: 2.7693  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 180/4722]  eta: 3:29:38  lr: 0.000010  loss: 6.3744  loss_lm: 6.8366 (6.7966)  time: 2.7693  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 180/4722]  eta: 3:29:37  lr: 0.000010  loss: 6.6485  loss_lm: 6.8202 (6.7967)  time: 2.7692  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 180/4722]  eta: 3:29:38  lr: 0.000010  loss: 6.8388  loss_lm: 6.7126 (6.6868)  time: 2.7694  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 180/4722]  eta: 3:29:39  lr: 0.000010  loss: 7.0706  loss_lm: 6.9594 (6.8703)  time: 2.7695  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 181/4722]  eta: 3:23:29  lr: 0.000010  loss: 6.6611  loss_lm: 6.6611 (6.7051)  time: 2.6888  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 181/4722]  eta: 3:23:29  lr: 0.000010  loss: 6.5389  loss_lm: 6.8021 (6.7806)  time: 2.6887  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 181/4722]  eta: 3:23:29  lr: 0.000010  loss: 6.8119  loss_lm: 6.8119 (6.7976)  time: 2.6888  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [ 181/4722]  eta: 3:23:30  lr: 0.000010  loss: 6.5992  loss_lm: 6.7476 (6.7877)  time: 2.6889  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 181/4722]  eta: 3:23:29  lr: 0.000010  loss: 6.7801  loss_lm: 6.7126 (6.6927)  time: 2.6888  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 181/4722]  eta: 3:23:30  lr: 0.000010  loss: 6.2812  loss_lm: 6.8106 (6.8335)  time: 2.6889  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 182/4722]  eta: 3:18:03  lr: 0.000010  loss: 6.8673  loss_lm: 6.8202 (6.7857)  time: 2.6174  data: 0.0000  max mem: 18867Train: data epoch: [0]  [ 182/4722]  eta: 3:18:03  lr: 0.000010  loss: 6.2585  loss_lm: 6.8119 (6.7659)  time: 2.6175  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [ 182/4722]  eta: 3:18:04  lr: 0.000010  loss: 6.7996  loss_lm: 6.8106 (6.8315)  time: 2.6176  data: 0.0000  max mem: 19014Train: data epoch: [0]  [ 182/4722]  eta: 3:18:03  lr: 0.000010  loss: 6.6979  loss_lm: 6.7476 (6.7825)  time: 2.6176  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 182/4722]  eta: 3:18:03  lr: 0.000010  loss: 6.6677  loss_lm: 6.7126 (6.6912)  time: 2.6176  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 182/4722]  eta: 3:18:03  lr: 0.000010  loss: 6.9847  loss_lm: 6.7193 (6.7215)  time: 2.6175  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 183/4722]  eta: 3:13:11  lr: 0.000010  loss: 7.0705  loss_lm: 6.8202 (6.8015)  time: 2.5538  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 183/4722]  eta: 3:13:12  lr: 0.000010  loss: 6.9350  loss_lm: 6.7476 (6.7909)  time: 2.5539  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 183/4722]  eta: 3:13:12  lr: 0.000010  loss: 7.3828  loss_lm: 6.8106 (6.8621)  time: 2.5540  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 183/4722]  eta: 3:13:12  lr: 0.000010  loss: 6.7818  loss_lm: 6.7126 (6.6962)  time: 2.5539  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 183/4722]  eta: 3:13:11  lr: 0.000010  loss: 6.4939  loss_lm: 6.6611 (6.7089)  time: 2.5539  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 183/4722]  eta: 3:13:11  lr: 0.000010  loss: 7.2311  loss_lm: 6.8119 (6.7917)  time: 2.5538  data: 0.0000  max mem: 18910


Train: data epoch: [0]  [ 184/4722]  eta: 3:08:49  lr: 0.000010  loss: 6.4613  loss_lm: 6.8202 (6.7836)  time: 2.4966  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 184/4722]  eta: 3:08:50  lr: 0.000010  loss: 7.0582  loss_lm: 6.7193 (6.7273)  time: 2.4967  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 184/4722]  eta: 3:08:50  lr: 0.000010  loss: 6.5354  loss_lm: 6.7476 (6.7775)  time: 2.4968  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 184/4722]  eta: 3:08:50  lr: 0.000010  loss: 6.6698  loss_lm: 6.7126 (6.6948)  time: 2.4968  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 184/4722]  eta: 3:08:50  lr: 0.000010  loss: 6.8731  loss_lm: 6.8366 (6.7960)  time: 2.4967  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [ 184/4722]  eta: 3:08:50  lr: 0.000010  loss: 6.4869  loss_lm: 6.8106 (6.8424)  time: 2.4968  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 185/4722]  eta: 3:24:39  lr: 0.000010  loss: 6.7045  loss_lm: 6.7045 (6.6953)  time: 2.7064  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 185/4722]  eta: 3:24:38  lr: 0.000010  loss: 6.2832  loss_lm: 6.8119 (6.7703)  time: 2.7064  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 185/4722]  eta: 3:24:38  lr: 0.000010  loss: 6.3234  loss_lm: 6.8021 (6.7606)  time: 2.7063  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 185/4722]  eta: 3:24:38  lr: 0.000010  loss: 6.5702  loss_lm: 6.6611 (6.7194)  time: 2.7064  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 185/4722]  eta: 3:24:39  lr: 0.000010  loss: 6.5363  loss_lm: 6.7111 (6.7654)  time: 2.7065  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 185/4722]  eta: 3:24:39  lr: 0.000010  loss: 6.6116  loss_lm: 6.7996 (6.8308)  time: 2.7065  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 186/4722]  eta: 3:20:09  lr: 0.000010  loss: 6.9960  loss_lm: 6.8202 (6.7718)  time: 2.5674  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 186/4722]  eta: 3:20:09  lr: 0.000010  loss: 7.0519  loss_lm: 6.8106 (6.8414)  time: 2.5674  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 186/4722]  eta: 3:20:09  lr: 0.000010  loss: 6.1764  loss_lm: 6.7111 (6.7374)  time: 2.5674  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 186/4722]  eta: 3:20:09  lr: 0.000010  loss: 7.1543  loss_lm: 6.7045 (6.7172)  time: 2.5674  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 186/4722]  eta: 3:20:09  lr: 0.000010  loss: 5.9529  loss_lm: 6.6611 (6.6829)  time: 2.5674  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 186/4722]  eta: 3:20:09  lr: 0.000010  loss: 6.8308  loss_lm: 6.8119 (6.7732)  time: 2.5674  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 187/4722]  eta: 3:16:06  lr: 0.000010  loss: 6.5120  loss_lm: 6.8202 (6.7600)  time: 2.5662  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 187/4722]  eta: 3:16:06  lr: 0.000010  loss: 6.8560  loss_lm: 6.6611 (6.6908)  time: 2.5662  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 187/4722]  eta: 3:16:07  lr: 0.000010  loss: 7.2297  loss_lm: 6.7111 (6.7598)  time: 2.5662  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 187/4722]  eta: 3:16:07  lr: 0.000010  loss: 6.2725  loss_lm: 6.7996 (6.8155)  time: 2.5662  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 187/4722]  eta: 3:16:06  lr: 0.000010  loss: 6.5912  loss_lm: 6.8018 (6.7649)  time: 2.5662  data: 0.0000  max mem: 18910Train: data epoch: [0]  [ 187/4722]  eta: 3:16:07  lr: 0.000010  loss: 6.6078  loss_lm: 6.6698 (6.7122)  time: 2.5662  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 188/4722]  eta: 3:12:23  lr: 0.000010  loss: 6.5686  loss_lm: 6.7872 (6.7517)  time: 2.5663  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 188/4722]  eta: 3:12:24  lr: 0.000010  loss: 6.2190  loss_lm: 6.7996 (6.7896)  time: 2.5663  data: 0.0000  max mem: 19014Train: data epoch: [0]  [ 188/4722]  eta: 3:12:23  lr: 0.000010  loss: 6.8336  loss_lm: 6.7193 (6.6970)  time: 2.5663  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 188/4722]  eta: 3:12:24  lr: 0.000010  loss: 6.4147  loss_lm: 6.6979 (6.7448)  time: 2.5663  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 188/4722]  eta: 3:12:24  lr: 0.000010  loss: 6.6218  loss_lm: 6.6677 (6.7083)  time: 2.5663  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 188/4722]  eta: 3:12:23  lr: 0.000010  loss: 6.6748  loss_lm: 6.7726 (6.7610)  time: 2.5663  data: 0.0000  max mem: 18910
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:38:52.563678] Starting job[2023-11-13 00:38:52.563680] Starting job

[2023-11-13 00:38:52.563687] Starting job
[2023-11-13 00:38:52.563762] Starting job
| distributed init (rank 8, world 12): env://
| distributed init (rank 11, world 12): env://
| distributed init (rank 9, world 12): env://
| distributed init (rank 10, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:38:56.905699] Starting job
| distributed init (rank 6, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:38:57.786554] Starting job
| distributed init (rank 7, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 186/4722]  eta: 5:15:52  lr: 0.000010  loss: 6.9329  loss_lm: 6.9329 (6.9329)  time: 4.1783  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 186/4722]  eta: 5:15:49  lr: 0.000010  loss: 5.9277  loss_lm: 5.9277 (5.9277)  time: 4.1776  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 186/4722]  eta: 5:15:50  lr: 0.000010  loss: 6.1947  loss_lm: 6.1947 (6.1947)  time: 4.1777  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 186/4722]  eta: 5:15:47  lr: 0.000010  loss: 7.1422  loss_lm: 7.1422 (7.1422)  time: 4.1772  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 186/4722]  eta: 5:15:54  lr: 0.000010  loss: 7.0380  loss_lm: 7.0380 (7.0380)  time: 4.1787  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 186/4722]  eta: 5:15:50  lr: 0.000010  loss: 6.8198  loss_lm: 6.8198 (6.8198)  time: 4.1778  data: 0.0000  max mem: 18865

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 187/4722]  eta: 3:34:03  lr: 0.000010  loss: 6.8620  loss_lm: 5.9277 (6.3949)  time: 2.8321  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 187/4722]  eta: 3:34:05  lr: 0.000010  loss: 6.4999  loss_lm: 6.4999 (6.7164)  time: 2.8325  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 187/4722]  eta: 3:34:05  lr: 0.000010  loss: 6.2486  loss_lm: 6.2486 (6.6433)  time: 2.8326  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 187/4722]  eta: 3:34:03  lr: 0.000010  loss: 7.2298  loss_lm: 6.1947 (6.7122)  time: 2.8321  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 187/4722]  eta: 3:34:02  lr: 0.000010  loss: 6.6154  loss_lm: 6.6154 (6.8788)  time: 2.8319  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 187/4722]  eta: 3:34:03  lr: 0.000010  loss: 6.6015  loss_lm: 6.6015 (6.7106)  time: 2.8322  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 188/4722]  eta: 2:59:43  lr: 0.000010  loss: 6.8266  loss_lm: 6.8266 (6.5388)  time: 2.3784  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 188/4722]  eta: 2:59:43  lr: 0.000010  loss: 6.4108  loss_lm: 6.4108 (6.6117)  time: 2.3784  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 188/4722]  eta: 2:59:44  lr: 0.000010  loss: 6.5738  loss_lm: 6.5738 (6.6689)  time: 2.3786  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 188/4722]  eta: 2:59:42  lr: 0.000010  loss: 6.6628  loss_lm: 6.6628 (6.8068)  time: 2.3782  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 188/4722]  eta: 2:59:44  lr: 0.000010  loss: 6.2121  loss_lm: 6.2486 (6.4995)  time: 2.3787  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 188/4722]  eta: 2:59:43  lr: 0.000010  loss: 6.6827  loss_lm: 6.6827 (6.7013)  time: 2.3784  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 189/4722]  eta: 2:42:38  lr: 0.000010  loss: 6.5775  loss_lm: 6.5738 (6.6460)  time: 2.1528  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 189/4722]  eta: 2:42:38  lr: 0.000010  loss: 6.4505  loss_lm: 6.4505 (6.5167)  time: 2.1527  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 189/4722]  eta: 2:42:38  lr: 0.000010  loss: 6.7531  loss_lm: 6.6827 (6.7143)  time: 2.1527  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 189/4722]  eta: 2:42:37  lr: 0.000010  loss: 6.8909  loss_lm: 6.4108 (6.6815)  time: 2.1526  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 189/4722]  eta: 2:42:38  lr: 0.000010  loss: 6.5065  loss_lm: 6.2486 (6.5013)  time: 2.1529  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 189/4722]  eta: 2:42:37  lr: 0.000010  loss: 7.1123  loss_lm: 6.6628 (6.8832)  time: 2.1525  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 190/4722]  eta: 3:54:42  lr: 0.000010  loss: 7.0270  loss_lm: 6.5065 (6.6064)  time: 3.1073  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 190/4722]  eta: 3:54:41  lr: 0.000010  loss: 5.8704  loss_lm: 6.4108 (6.5193)  time: 3.1072  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 190/4722]  eta: 3:54:41  lr: 0.000010  loss: 6.6936  loss_lm: 6.6936 (6.7101)  time: 3.1072  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 190/4722]  eta: 3:54:41  lr: 0.000010  loss: 6.4616  loss_lm: 6.4616 (6.5057)  time: 3.1072  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 190/4722]  eta: 3:54:41  lr: 0.000010  loss: 6.2248  loss_lm: 6.6628 (6.7515)  time: 3.1071  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 190/4722]  eta: 3:54:42  lr: 0.000010  loss: 6.7338  loss_lm: 6.5775 (6.6636)  time: 3.1073  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 191/4722]  eta: 3:34:10  lr: 0.000010  loss: 6.2899  loss_lm: 6.5738 (6.6013)  time: 2.8361  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 191/4722]  eta: 3:34:09  lr: 0.000010  loss: 6.3737  loss_lm: 6.4505 (6.4837)  time: 2.8360  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 191/4722]  eta: 3:34:09  lr: 0.000010  loss: 6.9851  loss_lm: 6.6628 (6.7904)  time: 2.8359  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 191/4722]  eta: 3:34:10  lr: 0.000010  loss: 6.7537  loss_lm: 6.5065 (6.6310)  time: 2.8361  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 191/4722]  eta: 3:34:09  lr: 0.000010  loss: 6.4507  loss_lm: 6.6827 (6.6669)  time: 2.8360  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 191/4722]  eta: 3:34:09  lr: 0.000010  loss: 6.2697  loss_lm: 6.2697 (6.4777)  time: 2.8360  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 192/4722]  eta: 3:19:25  lr: 0.000010  loss: 6.3377  loss_lm: 6.4505 (6.4628)  time: 2.6415  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 192/4722]  eta: 3:19:26  lr: 0.000010  loss: 6.4533  loss_lm: 6.5065 (6.6056)  time: 2.6416  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 192/4722]  eta: 3:19:26  lr: 0.000010  loss: 6.8261  loss_lm: 6.5775 (6.6334)  time: 2.6416  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 192/4722]  eta: 3:19:25  lr: 0.000010  loss: 6.5592  loss_lm: 6.4108 (6.4893)  time: 2.6415  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 192/4722]  eta: 3:19:25  lr: 0.000010  loss: 6.9384  loss_lm: 6.9384 (6.8116)  time: 2.6414  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 192/4722]  eta: 3:19:25  lr: 0.000010  loss: 6.8713  loss_lm: 6.6936 (6.6961)  time: 2.6415  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 193/4722]  eta: 3:08:23  lr: 0.000010  loss: 6.7421  loss_lm: 6.5775 (6.6470)  time: 2.4958  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 193/4722]  eta: 3:08:23  lr: 0.000010  loss: 6.9882  loss_lm: 6.4505 (6.5285)  time: 2.4957  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 193/4722]  eta: 3:08:22  lr: 0.000010  loss: 6.6540  loss_lm: 6.6628 (6.7919)  time: 2.4956  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 193/4722]  eta: 3:08:23  lr: 0.000010  loss: 6.8097  loss_lm: 6.4108 (6.5294)  time: 2.4957  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 193/4722]  eta: 3:08:23  lr: 0.000010  loss: 6.7214  loss_lm: 6.6936 (6.6992)  time: 2.4957  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 193/4722]  eta: 3:08:23  lr: 0.000010  loss: 6.6988  loss_lm: 6.5065 (6.6172)  time: 2.4958  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 194/4722]  eta: 2:59:48  lr: 0.000010  loss: 6.4204  loss_lm: 6.4505 (6.5165)  time: 2.3827  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 194/4722]  eta: 2:59:49  lr: 0.000010  loss: 6.1343  loss_lm: 6.5775 (6.5900)  time: 2.3828  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 194/4722]  eta: 2:59:48  lr: 0.000010  loss: 6.6245  loss_lm: 6.6936 (6.6909)  time: 2.3827  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 194/4722]  eta: 2:59:48  lr: 0.000010  loss: 6.4795  loss_lm: 6.6628 (6.7572)  time: 2.3826  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 194/4722]  eta: 2:59:49  lr: 0.000010  loss: 6.7226  loss_lm: 6.6988 (6.6290)  time: 2.3828  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 194/4722]  eta: 2:59:48  lr: 0.000010  loss: 6.4464  loss_lm: 6.4464 (6.5202)  time: 2.3827  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 195/4722]  eta: 3:32:39  lr: 0.000010  loss: 6.7555  loss_lm: 6.4505 (6.5404)  time: 2.8185  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 195/4722]  eta: 3:32:38  lr: 0.000010  loss: 6.7644  loss_lm: 6.6628 (6.7579)  time: 2.8184  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 195/4722]  eta: 3:32:39  lr: 0.000010  loss: 6.4997  loss_lm: 6.6827 (6.6718)  time: 2.8185  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 195/4722]  eta: 3:32:39  lr: 0.000010  loss: 6.4058  loss_lm: 6.5065 (6.6066)  time: 2.8186  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 195/4722]  eta: 3:32:39  lr: 0.000010  loss: 6.4669  loss_lm: 6.4464 (6.5148)  time: 2.8185  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 195/4722]  eta: 3:32:39  lr: 0.000010  loss: 6.2610  loss_lm: 6.5738 (6.5571)  time: 2.8186  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 196/4722]  eta: 3:23:26  lr: 0.000010  loss: 6.5225  loss_lm: 6.6628 (6.7365)  time: 2.6969  data: 0.0000  max mem: 18888Train: data epoch: [0]  [ 196/4722]  eta: 3:23:26  lr: 0.000010  loss: 6.2114  loss_lm: 6.5738 (6.5257)  time: 2.6971  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 196/4722]  eta: 3:23:26  lr: 0.000010  loss: 6.4192  loss_lm: 6.4505 (6.5294)  time: 2.6970  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 196/4722]  eta: 3:23:26  lr: 0.000010  loss: 6.4736  loss_lm: 6.5065 (6.5945)  time: 2.6971  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 196/4722]  eta: 3:23:26  lr: 0.000010  loss: 6.6532  loss_lm: 6.4669 (6.5274)  time: 2.6970  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 196/4722]  eta: 3:23:26  lr: 0.000010  loss: 6.6662  loss_lm: 6.6827 (6.6713)  time: 2.6970  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 197/4722]  eta: 3:15:45  lr: 0.000010  loss: 6.3855  loss_lm: 6.4204 (6.5174)  time: 2.5957  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 197/4722]  eta: 3:15:45  lr: 0.000010  loss: 7.0548  loss_lm: 6.5738 (6.5698)  time: 2.5958  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 197/4722]  eta: 3:15:45  lr: 0.000010  loss: 6.8236  loss_lm: 6.6628 (6.7437)  time: 2.5956  data: 0.0000  max mem: 18888Train: data epoch: [0]  [ 197/4722]  eta: 3:15:45  lr: 0.000010  loss: 6.6558  loss_lm: 6.4669 (6.5381)  time: 2.5957  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 197/4722]  eta: 3:15:45  lr: 0.000010  loss: 6.4918  loss_lm: 6.4918 (6.5860)  time: 2.5957  data: 0.0000  max mem: 18865


Train: data epoch: [0]  [ 197/4722]  eta: 3:15:45  lr: 0.000010  loss: 6.5695  loss_lm: 6.6662 (6.6628)  time: 2.5957  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 198/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.8455  loss_lm: 6.4505 (6.5426)  time: 2.5094  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 198/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.9086  loss_lm: 6.7644 (6.7564)  time: 2.5093  data: 0.0000  max mem: 18888

Train: data epoch: [0]  [ 198/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.1047  loss_lm: 6.5738 (6.5340)  time: 2.5094  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 198/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.6569  loss_lm: 6.6662 (6.6624)  time: 2.5094  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 198/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.9323  loss_lm: 6.5592 (6.5684)  time: 2.5093  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 198/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.8168  loss_lm: 6.5065 (6.6037)  time: 2.5094  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 199/4722]  eta: 3:03:34  lr: 0.000010  loss: 6.6047  loss_lm: 6.4505 (6.5471)  time: 2.4353  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 199/4722]  eta: 3:03:35  lr: 0.000010  loss: 6.9251  loss_lm: 6.5738 (6.5619)  time: 2.4353  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 199/4722]  eta: 3:03:34  lr: 0.000010  loss: 6.7004  loss_lm: 6.7004 (6.7524)  time: 2.4352  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 199/4722]  eta: 3:03:34  lr: 0.000010  loss: 6.8186  loss_lm: 6.6662 (6.6735)  time: 2.4353  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 199/4722]  eta: 3:03:34  lr: 0.000010  loss: 6.7462  loss_lm: 6.5065 (6.6139)  time: 2.4353  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 199/4722]  eta: 3:03:34  lr: 0.000010  loss: 6.4536  loss_lm: 6.4669 (6.5602)  time: 2.4353  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 200/4722]  eta: 3:25:39  lr: 0.000010  loss: 6.3547  loss_lm: 6.7004 (6.7259)  time: 2.7288  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 200/4722]  eta: 3:25:39  lr: 0.000010  loss: 6.4470  loss_lm: 6.4669 (6.5527)  time: 2.7288  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 200/4722]  eta: 3:25:39  lr: 0.000010  loss: 6.5521  loss_lm: 6.5521 (6.6098)  time: 2.7289  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 200/4722]  eta: 3:25:39  lr: 0.000010  loss: 6.3013  loss_lm: 6.6662 (6.6487)  time: 2.7288  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 200/4722]  eta: 3:25:39  lr: 0.000010  loss: 6.6982  loss_lm: 6.4616 (6.5571)  time: 2.7288  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 200/4722]  eta: 3:25:39  lr: 0.000010  loss: 6.5340  loss_lm: 6.5738 (6.5601)  time: 2.7289  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 201/4722]  eta: 3:20:01  lr: 0.000010  loss: 6.6434  loss_lm: 6.4616 (6.5625)  time: 2.6547  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 201/4722]  eta: 3:20:01  lr: 0.000010  loss: 6.7421  loss_lm: 6.5738 (6.5715)  time: 2.6547  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 201/4722]  eta: 3:20:01  lr: 0.000010  loss: 6.2659  loss_lm: 6.5065 (6.5883)  time: 2.6547  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 201/4722]  eta: 3:20:01  lr: 0.000010  loss: 6.4231  loss_lm: 6.6628 (6.7070)  time: 2.6546  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 201/4722]  eta: 3:20:01  lr: 0.000010  loss: 6.4169  loss_lm: 6.4536 (6.5442)  time: 2.6546  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 201/4722]  eta: 3:20:01  lr: 0.000010  loss: 6.7569  loss_lm: 6.6662 (6.6555)  time: 2.6546  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 202/4722]  eta: 3:14:49  lr: 0.000010  loss: 6.4603  loss_lm: 6.6628 (6.6925)  time: 2.5861  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 202/4722]  eta: 3:14:49  lr: 0.000010  loss: 6.6465  loss_lm: 6.6047 (6.5675)  time: 2.5861  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 202/4722]  eta: 3:14:49  lr: 0.000010  loss: 6.5254  loss_lm: 6.5738 (6.5688)  time: 2.5861  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 202/4722]  eta: 3:14:49  lr: 0.000010  loss: 6.3794  loss_lm: 6.4536 (6.5345)  time: 2.5861  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 202/4722]  eta: 3:14:49  lr: 0.000010  loss: 6.5471  loss_lm: 6.6662 (6.6491)  time: 2.5861  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 202/4722]  eta: 3:14:49  lr: 0.000010  loss: 6.7362  loss_lm: 6.5521 (6.5970)  time: 2.5861  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 203/4722]  eta: 3:10:06  lr: 0.000010  loss: 6.5481  loss_lm: 6.5481 (6.5676)  time: 2.5242  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 203/4722]  eta: 3:10:06  lr: 0.000010  loss: 7.0052  loss_lm: 6.6628 (6.7098)  time: 2.5241  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 203/4722]  eta: 3:10:06  lr: 0.000010  loss: 6.4503  loss_lm: 6.4616 (6.5610)  time: 2.5242  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 203/4722]  eta: 3:10:06  lr: 0.000010  loss: 6.7353  loss_lm: 6.5521 (6.6047)  time: 2.5242  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 203/4722]  eta: 3:10:06  lr: 0.000010  loss: 6.1522  loss_lm: 6.4470 (6.5133)  time: 2.5241  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 203/4722]  eta: 3:10:06  lr: 0.000010  loss: 6.4642  loss_lm: 6.6569 (6.6388)  time: 2.5242  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 204/4722]  eta: 3:05:57  lr: 0.000010  loss: 6.2492  loss_lm: 6.5481 (6.5508)  time: 2.4695  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 204/4722]  eta: 3:05:57  lr: 0.000010  loss: 6.2187  loss_lm: 6.5521 (6.5844)  time: 2.4695  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 204/4722]  eta: 3:05:57  lr: 0.000010  loss: 6.6237  loss_lm: 6.6047 (6.5643)  time: 2.4695  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 204/4722]  eta: 3:05:56  lr: 0.000010  loss: 6.1417  loss_lm: 6.6628 (6.6799)  time: 2.4694  data: 0.0000  max mem: 18888

Train: data epoch: [0]  [ 204/4722]  eta: 3:05:56  lr: 0.000010  loss: 6.9280  loss_lm: 6.6662 (6.6541)  time: 2.4694  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 204/4722]  eta: 3:05:56  lr: 0.000010  loss: 6.2400  loss_lm: 6.4470 (6.4989)  time: 2.4694  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 205/4722]  eta: 3:21:50  lr: 0.000010  loss: 6.1179  loss_lm: 6.6540 (6.6518)  time: 2.6811  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 205/4722]  eta: 3:21:50  lr: 0.000010  loss: 6.5153  loss_lm: 6.6569 (6.6471)  time: 2.6811  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 205/4722]  eta: 3:21:50  lr: 0.000010  loss: 6.6479  loss_lm: 6.5481 (6.5557)  time: 2.6812  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 205/4722]  eta: 3:21:50  lr: 0.000010  loss: 6.9693  loss_lm: 6.6047 (6.5845)  time: 2.6811  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 205/4722]  eta: 3:21:50  lr: 0.000010  loss: 7.0105  loss_lm: 6.4470 (6.5245)  time: 2.6811  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 205/4722]  eta: 3:21:50  lr: 0.000010  loss: 6.5592  loss_lm: 6.5521 (6.5831)  time: 2.6811  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 206/4722]  eta: 3:17:30  lr: 0.000010  loss: 6.4376  loss_lm: 6.4470 (6.5203)  time: 2.5464  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 206/4722]  eta: 3:17:30  lr: 0.000010  loss: 6.7160  loss_lm: 6.6540 (6.6549)  time: 2.5464  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 206/4722]  eta: 3:17:30  lr: 0.000010  loss: 6.8651  loss_lm: 6.5521 (6.5965)  time: 2.5464  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 206/4722]  eta: 3:17:30  lr: 0.000010  loss: 6.5709  loss_lm: 6.6047 (6.5839)  time: 2.5464  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 206/4722]  eta: 3:17:30  lr: 0.000010  loss: 6.5968  loss_lm: 6.6245 (6.6447)  time: 2.5464  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 206/4722]  eta: 3:17:30  lr: 0.000010  loss: 6.5956  loss_lm: 6.5481 (6.5576)  time: 2.5464  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 207/4722]  eta: 3:13:31  lr: 0.000010  loss: 6.7730  loss_lm: 6.4470 (6.5318)  time: 2.5458  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 207/4722]  eta: 3:13:31  lr: 0.000010  loss: 6.3005  loss_lm: 6.5481 (6.5459)  time: 2.5458  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 207/4722]  eta: 3:13:31  lr: 0.000010  loss: 6.7072  loss_lm: 6.6047 (6.5895)  time: 2.5458  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 207/4722]  eta: 3:13:31  lr: 0.000010  loss: 6.8376  loss_lm: 6.5592 (6.6075)  time: 2.5458  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 207/4722]  eta: 3:13:31  lr: 0.000010  loss: 6.5764  loss_lm: 6.6540 (6.6513)  time: 2.5458  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 207/4722]  eta: 3:13:31  lr: 0.000010  loss: 6.4533  loss_lm: 6.6245 (6.6360)  time: 2.5457  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 208/4722]  eta: 3:09:53  lr: 0.000010  loss: 6.4112  loss_lm: 6.4470 (6.5266)  time: 2.5460  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 208/4722]  eta: 3:09:54  lr: 0.000010  loss: 6.7449  loss_lm: 6.5481 (6.5546)  time: 2.5460  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 208/4722]  eta: 3:09:53  lr: 0.000010  loss: 6.1327  loss_lm: 6.5592 (6.5868)  time: 2.5460  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 208/4722]  eta: 3:09:53  lr: 0.000010  loss: 6.4440  loss_lm: 6.5764 (6.6423)  time: 2.5460  data: 0.0000  max mem: 18888Train: data epoch: [0]  [ 208/4722]  eta: 3:09:53  lr: 0.000010  loss: 6.4661  loss_lm: 6.5709 (6.5841)  time: 2.5460  data: 0.0000  max mem: 18866


Train: data epoch: [0]  [ 208/4722]  eta: 3:09:53  lr: 0.000010  loss: 7.0077  loss_lm: 6.6245 (6.6522)  time: 2.5460  data: 0.0000  max mem: 18865
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:45:09.534845] Starting job[2023-11-13 00:45:09.534840] Starting job

[2023-11-13 00:45:09.534851] Starting job[2023-11-13 00:45:09.534859] Starting job

[2023-11-13 00:45:09.534930] Starting job
[2023-11-13 00:45:09.537889] Starting job
| distributed init (rank 8, world 12): env://
| distributed init (rank 9, world 12): env://
| distributed init (rank 10, world 12): env://
| distributed init (rank 11, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 7, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 206/4722]  eta: 5:19:50  lr: 0.000010  loss: 6.5846  loss_lm: 6.5846 (6.5846)  time: 4.2495  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 206/4722]  eta: 5:19:57  lr: 0.000010  loss: 6.4532  loss_lm: 6.4532 (6.4532)  time: 4.2509  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 206/4722]  eta: 5:19:58  lr: 0.000010  loss: 6.8314  loss_lm: 6.8314 (6.8314)  time: 4.2512  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 206/4722]  eta: 5:19:53  lr: 0.000010  loss: 6.5795  loss_lm: 6.5795 (6.5795)  time: 4.2500  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 206/4722]  eta: 5:19:52  lr: 0.000010  loss: 6.6054  loss_lm: 6.6054 (6.6054)  time: 4.2500  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 206/4722]  eta: 5:20:05  lr: 0.000010  loss: 6.7234  loss_lm: 6.7234 (6.7234)  time: 4.2527  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 207/4722]  eta: 3:38:19  lr: 0.000010  loss: 6.4398  loss_lm: 6.4398 (6.5122)  time: 2.9014  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 207/4722]  eta: 3:38:22  lr: 0.000010  loss: 6.7859  loss_lm: 6.4532 (6.6195)  time: 2.9021  data: 0.0000  max mem: 18863

Train: data epoch: [0]  [ 207/4722]  eta: 3:38:20  lr: 0.000010  loss: 6.7044  loss_lm: 6.5795 (6.6419)  time: 2.9016  data: 0.0000  max mem: 18863Train: data epoch: [0]  [ 207/4722]  eta: 3:38:23  lr: 0.000010  loss: 6.8242  loss_lm: 6.8242 (6.8278)  time: 2.9022  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 207/4722]  eta: 3:38:26  lr: 0.000010  loss: 6.5575  loss_lm: 6.5575 (6.6405)  time: 2.9029  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 207/4722]  eta: 3:38:20  lr: 0.000010  loss: 6.2902  loss_lm: 6.2902 (6.4478)  time: 2.9016  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 208/4722]  eta: 3:04:15  lr: 0.000010  loss: 6.9748  loss_lm: 6.5846 (6.6664)  time: 2.4491  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 208/4722]  eta: 3:04:17  lr: 0.000010  loss: 6.4054  loss_lm: 6.4532 (6.5482)  time: 2.4496  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 208/4722]  eta: 3:04:19  lr: 0.000010  loss: 6.4760  loss_lm: 6.5575 (6.5856)  time: 2.4501  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 208/4722]  eta: 3:04:17  lr: 0.000010  loss: 6.1428  loss_lm: 6.8242 (6.5994)  time: 2.4497  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 208/4722]  eta: 3:04:16  lr: 0.000010  loss: 6.4733  loss_lm: 6.5795 (6.5857)  time: 2.4493  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 208/4722]  eta: 3:04:15  lr: 0.000010  loss: 6.7554  loss_lm: 6.6054 (6.5503)  time: 2.4493  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 209/4722]  eta: 2:47:08  lr: 0.000010  loss: 6.7589  loss_lm: 6.5846 (6.6895)  time: 2.2221  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 209/4722]  eta: 2:47:10  lr: 0.000010  loss: 6.5412  loss_lm: 6.4532 (6.5464)  time: 2.2225  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 209/4722]  eta: 2:47:08  lr: 0.000010  loss: 6.3652  loss_lm: 6.4733 (6.5306)  time: 2.2222  data: 0.0000  max mem: 18863Train: data epoch: [0]  [ 209/4722]  eta: 2:47:11  lr: 0.000010  loss: 6.3707  loss_lm: 6.4760 (6.5319)  time: 2.2229  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 209/4722]  eta: 2:47:08  lr: 0.000010  loss: 6.5124  loss_lm: 6.5124 (6.5409)  time: 2.2222  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 209/4722]  eta: 2:47:10  lr: 0.000010  loss: 6.5754  loss_lm: 6.5754 (6.5934)  time: 2.2225  data: 0.0000  max mem: 18864

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 210/4722]  eta: 3:58:38  lr: 0.000010  loss: 6.7406  loss_lm: 6.5575 (6.5736)  time: 3.1734  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 210/4722]  eta: 3:58:35  lr: 0.000010  loss: 6.6737  loss_lm: 6.6737 (6.6864)  time: 3.1728  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 210/4722]  eta: 3:58:36  lr: 0.000010  loss: 6.5429  loss_lm: 6.5754 (6.5833)  time: 3.1731  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 210/4722]  eta: 3:58:35  lr: 0.000010  loss: 6.4338  loss_lm: 6.5124 (6.5194)  time: 3.1728  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 210/4722]  eta: 3:58:35  lr: 0.000010  loss: 6.6149  loss_lm: 6.5795 (6.5474)  time: 3.1729  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 210/4722]  eta: 3:58:36  lr: 0.000010  loss: 6.5540  loss_lm: 6.5412 (6.5479)  time: 3.1731  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 211/4722]  eta: 3:37:15  lr: 0.000010  loss: 6.9253  loss_lm: 6.6737 (6.7262)  time: 2.8896  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 211/4722]  eta: 3:37:15  lr: 0.000010  loss: 6.6286  loss_lm: 6.5412 (6.5614)  time: 2.8898  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 211/4722]  eta: 3:37:15  lr: 0.000010  loss: 6.6410  loss_lm: 6.5795 (6.5630)  time: 2.8896  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 211/4722]  eta: 3:37:16  lr: 0.000010  loss: 6.5384  loss_lm: 6.5429 (6.5758)  time: 2.8898  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 211/4722]  eta: 3:37:17  lr: 0.000010  loss: 6.4100  loss_lm: 6.4760 (6.5464)  time: 2.8901  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 211/4722]  eta: 3:37:15  lr: 0.000010  loss: 6.7892  loss_lm: 6.5124 (6.5644)  time: 2.8896  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 212/4722]  eta: 3:22:00  lr: 0.000010  loss: 6.4512  loss_lm: 6.5412 (6.5456)  time: 2.6875  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 212/4722]  eta: 3:21:59  lr: 0.000010  loss: 6.5599  loss_lm: 6.6737 (6.7024)  time: 2.6874  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 212/4722]  eta: 3:22:00  lr: 0.000010  loss: 6.4558  loss_lm: 6.5429 (6.5587)  time: 2.6875  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 212/4722]  eta: 3:21:59  lr: 0.000010  loss: 6.5337  loss_lm: 6.5795 (6.5589)  time: 2.6874  data: 0.0000  max mem: 18863Train: data epoch: [0]  [ 212/4722]  eta: 3:22:01  lr: 0.000010  loss: 6.3228  loss_lm: 6.4760 (6.5144)  time: 2.6877  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 212/4722]  eta: 3:21:59  lr: 0.000010  loss: 6.7687  loss_lm: 6.6054 (6.5936)  time: 2.6873  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 213/4722]  eta: 3:10:34  lr: 0.000010  loss: 6.0917  loss_lm: 6.5846 (6.6261)  time: 2.5360  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 213/4722]  eta: 3:10:35  lr: 0.000010  loss: 6.8449  loss_lm: 6.5412 (6.5830)  time: 2.5362  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 213/4722]  eta: 3:10:36  lr: 0.000010  loss: 6.4816  loss_lm: 6.4760 (6.5103)  time: 2.5364  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 213/4722]  eta: 3:10:34  lr: 0.000010  loss: 6.8566  loss_lm: 6.5795 (6.5961)  time: 2.5360  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 213/4722]  eta: 3:10:34  lr: 0.000010  loss: 6.6341  loss_lm: 6.6054 (6.5986)  time: 2.5360  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 213/4722]  eta: 3:10:35  lr: 0.000010  loss: 6.3742  loss_lm: 6.5384 (6.5356)  time: 2.5362  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 214/4722]  eta: 3:01:40  lr: 0.000010  loss: 6.8104  loss_lm: 6.5540 (6.6083)  time: 2.4181  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 214/4722]  eta: 3:01:40  lr: 0.000010  loss: 6.6609  loss_lm: 6.6609 (6.6300)  time: 2.4180  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 214/4722]  eta: 3:01:40  lr: 0.000010  loss: 6.7226  loss_lm: 6.6149 (6.6101)  time: 2.4180  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 214/4722]  eta: 3:01:41  lr: 0.000010  loss: 6.3832  loss_lm: 6.4760 (6.4962)  time: 2.4183  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 214/4722]  eta: 3:01:41  lr: 0.000010  loss: 6.0943  loss_lm: 6.5384 (6.4866)  time: 2.4182  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 214/4722]  eta: 3:01:40  lr: 0.000010  loss: 6.5304  loss_lm: 6.6054 (6.5911)  time: 2.4180  data: 0.0000  max mem: 18864



Train: data epoch: [0]  [ 215/4722]  eta: 3:32:50  lr: 0.000010  loss: 6.2589  loss_lm: 6.5846 (6.5929)  time: 2.8334  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 215/4722]  eta: 3:32:50  lr: 0.000010  loss: 6.9889  loss_lm: 6.6149 (6.6480)  time: 2.8334  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 215/4722]  eta: 3:32:50  lr: 0.000010  loss: 6.5785  loss_lm: 6.5785 (6.5898)  time: 2.8334  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 215/4722]  eta: 3:32:50  lr: 0.000010  loss: 6.5137  loss_lm: 6.5137 (6.4893)  time: 2.8335  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 215/4722]  eta: 3:32:51  lr: 0.000010  loss: 6.6328  loss_lm: 6.4760 (6.5099)  time: 2.8337  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 215/4722]  eta: 3:32:50  lr: 0.000010  loss: 6.9769  loss_lm: 6.5540 (6.6452)  time: 2.8335  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 216/4722]  eta: 3:23:34  lr: 0.000010  loss: 5.9579  loss_lm: 6.5846 (6.5351)  time: 2.7107  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 216/4722]  eta: 3:23:34  lr: 0.000010  loss: 6.5979  loss_lm: 6.5384 (6.4992)  time: 2.7107  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 216/4722]  eta: 3:23:34  lr: 0.000010  loss: 6.8764  loss_lm: 6.6286 (6.6662)  time: 2.7107  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 216/4722]  eta: 3:23:34  lr: 0.000010  loss: 6.7234  loss_lm: 6.6410 (6.6549)  time: 2.7106  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 216/4722]  eta: 3:23:33  lr: 0.000010  loss: 6.5660  loss_lm: 6.5785 (6.5876)  time: 2.7106  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 216/4722]  eta: 3:23:35  lr: 0.000010  loss: 6.1445  loss_lm: 6.4760 (6.4767)  time: 2.7109  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 217/4722]  eta: 3:15:46  lr: 0.000010  loss: 6.3356  loss_lm: 6.5599 (6.5185)  time: 2.6074  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 217/4722]  eta: 3:15:46  lr: 0.000010  loss: 6.4111  loss_lm: 6.5540 (6.6449)  time: 2.6074  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 217/4722]  eta: 3:15:46  lr: 0.000010  loss: 6.1141  loss_lm: 6.6149 (6.6098)  time: 2.6074  data: 0.0000  max mem: 19004Train: data epoch: [0]  [ 217/4722]  eta: 3:15:46  lr: 0.000010  loss: 6.6257  loss_lm: 6.5785 (6.5908)  time: 2.6073  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 217/4722]  eta: 3:15:46  lr: 0.000010  loss: 6.1808  loss_lm: 6.5137 (6.4726)  time: 2.6075  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 217/4722]  eta: 3:15:47  lr: 0.000010  loss: 6.5930  loss_lm: 6.4760 (6.4864)  time: 2.6076  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 218/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.8273  loss_lm: 6.5846 (6.5423)  time: 2.5205  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 218/4722]  eta: 3:09:13  lr: 0.000010  loss: 6.7673  loss_lm: 6.4816 (6.5080)  time: 2.5207  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 218/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.5833  loss_lm: 6.6149 (6.6077)  time: 2.5205  data: 0.0000  max mem: 19004Train: data epoch: [0]  [ 218/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.2786  loss_lm: 6.5540 (6.6168)  time: 2.5205  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 218/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.3294  loss_lm: 6.5137 (6.4616)  time: 2.5206  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 218/4722]  eta: 3:09:12  lr: 0.000010  loss: 6.4121  loss_lm: 6.5785 (6.5771)  time: 2.5205  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 219/4722]  eta: 3:03:36  lr: 0.000010  loss: 6.7919  loss_lm: 6.5137 (6.4852)  time: 2.4466  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 219/4722]  eta: 3:03:36  lr: 0.000010  loss: 6.5871  loss_lm: 6.5846 (6.5455)  time: 2.4465  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 219/4722]  eta: 3:03:36  lr: 0.000010  loss: 6.1160  loss_lm: 6.5833 (6.5726)  time: 2.4465  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 219/4722]  eta: 3:03:36  lr: 0.000010  loss: 6.4486  loss_lm: 6.5660 (6.5679)  time: 2.4465  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 219/4722]  eta: 3:03:37  lr: 0.000010  loss: 6.6934  loss_lm: 6.4816 (6.5212)  time: 2.4467  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 219/4722]  eta: 3:03:36  lr: 0.000010  loss: 6.4868  loss_lm: 6.5412 (6.6075)  time: 2.4465  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 220/4722]  eta: 3:24:23  lr: 0.000010  loss: 6.2752  loss_lm: 6.5833 (6.5528)  time: 2.7239  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 220/4722]  eta: 3:24:23  lr: 0.000010  loss: 6.8012  loss_lm: 6.5871 (6.5625)  time: 2.7240  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 220/4722]  eta: 3:24:23  lr: 0.000010  loss: 7.0843  loss_lm: 6.5384 (6.5252)  time: 2.7240  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 220/4722]  eta: 3:24:23  lr: 0.000010  loss: 6.3823  loss_lm: 6.5412 (6.5925)  time: 2.7239  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 220/4722]  eta: 3:24:23  lr: 0.000010  loss: 6.5629  loss_lm: 6.5660 (6.5675)  time: 2.7239  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 220/4722]  eta: 3:24:23  lr: 0.000010  loss: 6.6502  loss_lm: 6.5575 (6.5298)  time: 2.7241  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 221/4722]  eta: 3:18:30  lr: 0.000010  loss: 6.4311  loss_lm: 6.4816 (6.5236)  time: 2.6462  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 221/4722]  eta: 3:18:30  lr: 0.000010  loss: 6.3516  loss_lm: 6.5795 (6.5402)  time: 2.6461  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 221/4722]  eta: 3:18:29  lr: 0.000010  loss: 6.4891  loss_lm: 6.4891 (6.5860)  time: 2.6461  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 221/4722]  eta: 3:18:30  lr: 0.000010  loss: 6.1547  loss_lm: 6.5137 (6.5020)  time: 2.6461  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 221/4722]  eta: 3:18:30  lr: 0.000010  loss: 6.4140  loss_lm: 6.5846 (6.5532)  time: 2.6461  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 221/4722]  eta: 3:18:29  lr: 0.000010  loss: 6.2621  loss_lm: 6.5629 (6.5485)  time: 2.6460  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 222/4722]  eta: 3:13:18  lr: 0.000010  loss: 6.7709  loss_lm: 6.5833 (6.5538)  time: 2.5775  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 222/4722]  eta: 3:13:19  lr: 0.000010  loss: 6.8240  loss_lm: 6.5575 (6.5413)  time: 2.5776  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 222/4722]  eta: 3:13:18  lr: 0.000010  loss: 6.3959  loss_lm: 6.5846 (6.5440)  time: 2.5775  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 222/4722]  eta: 3:13:18  lr: 0.000010  loss: 6.7000  loss_lm: 6.5412 (6.5927)  time: 2.5775  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 222/4722]  eta: 3:13:18  lr: 0.000010  loss: 6.4019  loss_lm: 6.5629 (6.5398)  time: 2.5775  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 222/4722]  eta: 3:13:18  lr: 0.000010  loss: 6.2279  loss_lm: 6.5137 (6.4859)  time: 2.5775  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 223/4722]  eta: 3:08:40  lr: 0.000010  loss: 6.5697  loss_lm: 6.5575 (6.5429)  time: 2.5162  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 223/4722]  eta: 3:08:39  lr: 0.000010  loss: 6.7071  loss_lm: 6.5833 (6.5623)  time: 2.5160  data: 0.0000  max mem: 19004

Train: data epoch: [0]  [ 223/4722]  eta: 3:08:39  lr: 0.000010  loss: 6.7140  loss_lm: 6.5137 (6.4985)  time: 2.5161  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 223/4722]  eta: 3:08:39  lr: 0.000010  loss: 6.0307  loss_lm: 6.5599 (6.5155)  time: 2.5160  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 223/4722]  eta: 3:08:39  lr: 0.000010  loss: 6.1913  loss_lm: 6.4891 (6.5704)  time: 2.5160  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 223/4722]  eta: 3:08:39  lr: 0.000010  loss: 6.4759  loss_lm: 6.5304 (6.5363)  time: 2.5160  data: 0.0000  max mem: 18864

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 224/4722]  eta: 3:04:29  lr: 0.000010  loss: 5.9804  loss_lm: 6.5137 (6.4713)  time: 2.4610  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 224/4722]  eta: 3:04:30  lr: 0.000010  loss: 6.6698  loss_lm: 6.5697 (6.5496)  time: 2.4611  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 224/4722]  eta: 3:04:29  lr: 0.000010  loss: 6.8008  loss_lm: 6.6149 (6.5749)  time: 2.4610  data: 0.0000  max mem: 19004

Train: data epoch: [0]  [ 224/4722]  eta: 3:04:29  lr: 0.000010  loss: 6.3041  loss_lm: 6.5304 (6.5241)  time: 2.4609  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 224/4722]  eta: 3:04:29  lr: 0.000010  loss: 6.1494  loss_lm: 6.4891 (6.5482)  time: 2.4610  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 224/4722]  eta: 3:04:29  lr: 0.000010  loss: 6.1654  loss_lm: 6.5599 (6.4970)  time: 2.4610  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 225/4722]  eta: 3:23:32  lr: 0.000010  loss: 6.4065  loss_lm: 6.5124 (6.5182)  time: 2.7157  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 225/4722]  eta: 3:23:32  lr: 0.000010  loss: 7.0663  loss_lm: 6.5137 (6.5010)  time: 2.7158  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 225/4722]  eta: 3:23:33  lr: 0.000010  loss: 6.1466  loss_lm: 6.5575 (6.5294)  time: 2.7158  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 225/4722]  eta: 3:23:32  lr: 0.000010  loss: 6.4913  loss_lm: 6.4913 (6.4967)  time: 2.7157  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 225/4722]  eta: 3:23:32  lr: 0.000010  loss: 6.4592  loss_lm: 6.4868 (6.5438)  time: 2.7157  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 225/4722]  eta: 3:23:32  lr: 0.000010  loss: 6.8577  loss_lm: 6.6149 (6.5890)  time: 2.7157  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 226/4722]  eta: 3:19:04  lr: 0.000010  loss: 6.3613  loss_lm: 6.6149 (6.5782)  time: 2.5769  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 226/4722]  eta: 3:19:04  lr: 0.000010  loss: 6.5719  loss_lm: 6.5575 (6.5314)  time: 2.5769  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 226/4722]  eta: 3:19:04  lr: 0.000010  loss: 6.4197  loss_lm: 6.4558 (6.4972)  time: 2.5769  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 226/4722]  eta: 3:19:03  lr: 0.000010  loss: 6.3215  loss_lm: 6.4868 (6.5332)  time: 2.5768  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 226/4722]  eta: 3:19:03  lr: 0.000010  loss: 6.8150  loss_lm: 6.5124 (6.5323)  time: 2.5769  data: 0.0000  max mem: 18864


Train: data epoch: [0]  [ 226/4722]  eta: 3:19:04  lr: 0.000010  loss: 6.9628  loss_lm: 6.4913 (6.5189)  time: 2.5770  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 227/4722]  eta: 3:15:01  lr: 0.000010  loss: 6.2457  loss_lm: 6.4816 (6.5184)  time: 2.5732  data: 0.0000  max mem: 19014Train: data epoch: [0]  [ 227/4722]  eta: 3:15:00  lr: 0.000010  loss: 6.7263  loss_lm: 6.6149 (6.5849)  time: 2.5732  data: 0.0000  max mem: 19004

Train: data epoch: [0]  [ 227/4722]  eta: 3:15:00  lr: 0.000010  loss: 6.8399  loss_lm: 6.4558 (6.5127)  time: 2.5732  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 227/4722]  eta: 3:15:00  lr: 0.000010  loss: 6.1337  loss_lm: 6.4592 (6.5150)  time: 2.5731  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 227/4722]  eta: 3:15:00  lr: 0.000010  loss: 6.0801  loss_lm: 6.4913 (6.4990)  time: 2.5732  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 227/4722]  eta: 3:15:00  lr: 0.000010  loss: 6.3446  loss_lm: 6.5124 (6.5238)  time: 2.5732  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 228/4722]  eta: 3:11:17  lr: 0.000010  loss: 6.3113  loss_lm: 6.4816 (6.5094)  time: 2.5696  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 228/4722]  eta: 3:11:17  lr: 0.000010  loss: 6.2644  loss_lm: 6.6149 (6.5710)  time: 2.5696  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 228/4722]  eta: 3:11:17  lr: 0.000010  loss: 6.4703  loss_lm: 6.4703 (6.4977)  time: 2.5696  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 228/4722]  eta: 3:11:17  lr: 0.000010  loss: 6.5641  loss_lm: 6.5137 (6.5150)  time: 2.5696  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 228/4722]  eta: 3:11:17  lr: 0.000010  loss: 6.3922  loss_lm: 6.4592 (6.5097)  time: 2.5695  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 228/4722]  eta: 3:11:17  lr: 0.000010  loss: 6.6079  loss_lm: 6.5124 (6.5274)  time: 2.5696  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 229/4722]  eta: 3:07:53  lr: 0.000010  loss: 6.8154  loss_lm: 6.5697 (6.5222)  time: 2.5663  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 229/4722]  eta: 3:07:52  lr: 0.000010  loss: 6.3969  loss_lm: 6.6149 (6.5637)  time: 2.5663  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 229/4722]  eta: 3:07:52  lr: 0.000010  loss: 6.7467  loss_lm: 6.4703 (6.5081)  time: 2.5663  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 229/4722]  eta: 3:07:52  lr: 0.000010  loss: 6.8065  loss_lm: 6.5304 (6.5391)  time: 2.5662  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 229/4722]  eta: 3:07:52  lr: 0.000010  loss: 6.5754  loss_lm: 6.5137 (6.5175)  time: 2.5662  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 229/4722]  eta: 3:07:52  lr: 0.000010  loss: 6.6770  loss_lm: 6.4592 (6.5167)  time: 2.5662  data: 0.0000  max mem: 18865

WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:51:25.773427] Starting job
[2023-11-13 00:51:25.773530] Starting job
[2023-11-13 00:51:25.773566] Starting job
[2023-11-13 00:51:25.776202] Starting job
| distributed init (rank 11, world 12): env://
| distributed init (rank 9, world 12): env://
| distributed init (rank 10, world 12): env://
| distributed init (rank 8, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:51:30.298265] Starting job
| distributed init (rank 7, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:51:30.953860] Starting job
| distributed init (rank 6, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 226/4722]  eta: 5:15:52  lr: 0.000010  loss: 6.5807  loss_lm: 6.5807 (6.5807)  time: 4.2155  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 226/4722]  eta: 5:15:58  lr: 0.000010  loss: 6.4544  loss_lm: 6.4544 (6.4544)  time: 4.2168  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 226/4722]  eta: 5:15:54  lr: 0.000010  loss: 6.7838  loss_lm: 6.7838 (6.7838)  time: 4.2158  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 226/4722]  eta: 5:15:54  lr: 0.000010  loss: 6.9610  loss_lm: 6.9610 (6.9610)  time: 4.2158  data: 0.0000  max mem: 18875
Train: data epoch: [0]  [ 226/4722]  eta: 5:15:55  lr: 0.000010  loss: 6.3803  loss_lm: 6.3803 (6.3803)  time: 4.2161  data: 0.0000  max mem: 18863Train: data epoch: [0]  [ 226/4722]  eta: 5:16:02  lr: 0.000010  loss: 6.2936  loss_lm: 6.2936 (6.2936)  time: 4.2176  data: 0.0000  max mem: 18864

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 227/4722]  eta: 3:33:08  lr: 0.000010  loss: 6.8323  loss_lm: 6.4544 (6.6434)  time: 2.8452  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 227/4722]  eta: 3:33:10  lr: 0.000010  loss: 6.1302  loss_lm: 6.1302 (6.2119)  time: 2.8455  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 227/4722]  eta: 3:33:06  lr: 0.000010  loss: 6.2590  loss_lm: 6.2590 (6.4198)  time: 2.8445  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 227/4722]  eta: 3:33:06  lr: 0.000010  loss: 6.3238  loss_lm: 6.3238 (6.5538)  time: 2.8447  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 227/4722]  eta: 3:33:06  lr: 0.000010  loss: 6.7187  loss_lm: 6.3803 (6.5495)  time: 2.8447  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 227/4722]  eta: 3:33:06  lr: 0.000010  loss: 6.1061  loss_lm: 6.1061 (6.5335)  time: 2.8446  data: 0.0000  max mem: 18875
Train: data epoch: [0]  [ 228/4722]  eta: 2:58:43  lr: 0.000010  loss: 6.5960  loss_lm: 6.5960 (6.6276)  time: 2.3863  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 228/4722]  eta: 2:58:42  lr: 0.000010  loss: 6.6287  loss_lm: 6.6287 (6.5788)  time: 2.3859  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 228/4722]  eta: 2:58:42  lr: 0.000010  loss: 6.4264  loss_lm: 6.4264 (6.4978)  time: 2.3859  data: 0.0000  max mem: 18875
Train: data epoch: [0]  [ 228/4722]  eta: 2:58:41  lr: 0.000010  loss: 6.3219  loss_lm: 6.3219 (6.3872)  time: 2.3858  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 228/4722]  eta: 2:58:44  lr: 0.000010  loss: 6.3790  loss_lm: 6.2936 (6.2676)  time: 2.3865  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 228/4722]  eta: 2:58:42  lr: 0.000010  loss: 6.2922  loss_lm: 6.3803 (6.4638)  time: 2.3859  data: 0.0000  max mem: 19004
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 229/4722]  eta: 2:41:28  lr: 0.000010  loss: 6.6304  loss_lm: 6.2936 (6.3583)  time: 2.1564  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 229/4722]  eta: 2:41:28  lr: 0.000010  loss: 6.5986  loss_lm: 6.5960 (6.6204)  time: 2.1562  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 229/4722]  eta: 2:41:26  lr: 0.000010  loss: 6.7598  loss_lm: 6.3219 (6.4803)  time: 2.1559  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 229/4722]  eta: 2:41:26  lr: 0.000010  loss: 6.7123  loss_lm: 6.4264 (6.5514)  time: 2.1559  data: 0.0000  max mem: 18875
Train: data epoch: [0]  [ 229/4722]  eta: 2:41:26  lr: 0.000010  loss: 6.4111  loss_lm: 6.3803 (6.4506)  time: 2.1560  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 229/4722]  eta: 2:41:26  lr: 0.000010  loss: 6.8399  loss_lm: 6.6287 (6.6441)  time: 2.1560  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 230/4722]  eta: 3:50:16  lr: 0.000010  loss: 6.6135  loss_lm: 6.3790 (6.4093)  time: 3.0759  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 230/4722]  eta: 3:50:15  lr: 0.000010  loss: 6.4026  loss_lm: 6.4026 (6.4410)  time: 3.0755  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 230/4722]  eta: 3:50:15  lr: 0.000010  loss: 6.6206  loss_lm: 6.6287 (6.6394)  time: 3.0755  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 230/4722]  eta: 3:50:15  lr: 0.000010  loss: 6.6014  loss_lm: 6.5807 (6.5046)  time: 3.0755  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 230/4722]  eta: 3:50:15  lr: 0.000010  loss: 6.1424  loss_lm: 6.4264 (6.4696)  time: 3.0755  data: 0.0000  max mem: 18875
Train: data epoch: [0]  [ 230/4722]  eta: 3:50:16  lr: 0.000010  loss: 6.7295  loss_lm: 6.5986 (6.6422)  time: 3.0758  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 231/4722]  eta: 3:30:12  lr: 0.000010  loss: 6.4276  loss_lm: 6.5960 (6.6064)  time: 2.8084  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 231/4722]  eta: 3:30:12  lr: 0.000010  loss: 5.8092  loss_lm: 6.2936 (6.3093)  time: 2.8085  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 231/4722]  eta: 3:30:11  lr: 0.000010  loss: 6.0545  loss_lm: 6.3803 (6.3766)  time: 2.8082  data: 0.0000  max mem: 19004


Train: data epoch: [0]  [ 231/4722]  eta: 3:30:11  lr: 0.000010  loss: 6.5127  loss_lm: 6.5127 (6.5059)  time: 2.8081  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 231/4722]  eta: 3:30:11  lr: 0.000010  loss: 6.3169  loss_lm: 6.3169 (6.4442)  time: 2.8082  data: 0.0000  max mem: 18875
Train: data epoch: [0]  [ 231/4722]  eta: 3:30:11  lr: 0.000010  loss: 6.1828  loss_lm: 6.6206 (6.5633)  time: 2.8082  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 232/4722]  eta: 3:15:48  lr: 0.000010  loss: 6.6068  loss_lm: 6.5986 (6.6065)  time: 2.6166  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 232/4722]  eta: 3:15:48  lr: 0.000010  loss: 6.3747  loss_lm: 6.3747 (6.3187)  time: 2.6167  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 232/4722]  eta: 3:15:47  lr: 0.000010  loss: 6.8393  loss_lm: 6.5807 (6.5535)  time: 2.6164  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 232/4722]  eta: 3:15:47  lr: 0.000010  loss: 6.4023  loss_lm: 6.4023 (6.4382)  time: 2.6164  data: 0.0000  max mem: 18875
Train: data epoch: [0]  [ 232/4722]  eta: 3:15:47  lr: 0.000010  loss: 6.0633  loss_lm: 6.3803 (6.3318)  time: 2.6164  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 232/4722]  eta: 3:15:47  lr: 0.000010  loss: 6.1542  loss_lm: 6.6206 (6.5048)  time: 2.6164  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 233/4722]  eta: 3:04:58  lr: 0.000010  loss: 6.2176  loss_lm: 6.5960 (6.5579)  time: 2.4723  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 233/4722]  eta: 3:04:58  lr: 0.000010  loss: 6.4965  loss_lm: 6.3747 (6.3409)  time: 2.4724  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 233/4722]  eta: 3:04:57  lr: 0.000010  loss: 6.6092  loss_lm: 6.5807 (6.5605)  time: 2.4721  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 233/4722]  eta: 3:04:57  lr: 0.000010  loss: 6.3455  loss_lm: 6.3455 (6.4849)  time: 2.4721  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 233/4722]  eta: 3:04:57  lr: 0.000010  loss: 6.4676  loss_lm: 6.3803 (6.3488)  time: 2.4721  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 233/4722]  eta: 3:04:57  lr: 0.000010  loss: 6.8125  loss_lm: 6.4023 (6.4850)  time: 2.4721  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 234/4722]  eta: 2:56:35  lr: 0.000010  loss: 6.4999  loss_lm: 6.3790 (6.3586)  time: 2.3608  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 234/4722]  eta: 2:56:35  lr: 0.000010  loss: 6.5946  loss_lm: 6.5960 (6.5619)  time: 2.3608  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 234/4722]  eta: 2:56:34  lr: 0.000010  loss: 6.7997  loss_lm: 6.4026 (6.3989)  time: 2.3606  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 234/4722]  eta: 2:56:34  lr: 0.000010  loss: 6.1666  loss_lm: 6.4023 (6.4496)  time: 2.3606  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 234/4722]  eta: 2:56:34  lr: 0.000010  loss: 6.8392  loss_lm: 6.6014 (6.5915)  time: 2.3606  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 234/4722]  eta: 2:56:34  lr: 0.000010  loss: 6.3522  loss_lm: 6.3522 (6.4702)  time: 2.3606  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 235/4722]  eta: 3:28:49  lr: 0.000010  loss: 6.1331  loss_lm: 6.3747 (6.3360)  time: 2.7923  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 235/4722]  eta: 3:28:48  lr: 0.000010  loss: 6.2542  loss_lm: 6.5807 (6.5577)  time: 2.7921  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 235/4722]  eta: 3:28:48  lr: 0.000010  loss: 6.6230  loss_lm: 6.4023 (6.4669)  time: 2.7921  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 235/4722]  eta: 3:28:48  lr: 0.000010  loss: 6.6184  loss_lm: 6.4026 (6.4209)  time: 2.7921  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 235/4722]  eta: 3:28:48  lr: 0.000010  loss: 6.2060  loss_lm: 6.3455 (6.4438)  time: 2.7921  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 235/4722]  eta: 3:28:48  lr: 0.000010  loss: 6.1597  loss_lm: 6.5946 (6.5217)  time: 2.7922  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 236/4722]  eta: 3:19:46  lr: 0.000010  loss: 6.5010  loss_lm: 6.3790 (6.3510)  time: 2.6720  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 236/4722]  eta: 3:19:45  lr: 0.000010  loss: 5.8702  loss_lm: 6.3455 (6.3916)  time: 2.6718  data: 0.0000  max mem: 18867Train: data epoch: [0]  [ 236/4722]  eta: 3:19:45  lr: 0.000010  loss: 5.9996  loss_lm: 6.5946 (6.4743)  time: 2.6719  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 236/4722]  eta: 3:19:45  lr: 0.000010  loss: 6.6381  loss_lm: 6.4264 (6.4825)  time: 2.6717  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 236/4722]  eta: 3:19:45  lr: 0.000010  loss: 6.8714  loss_lm: 6.6014 (6.5863)  time: 2.6718  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 236/4722]  eta: 3:19:45  lr: 0.000010  loss: 6.4921  loss_lm: 6.4111 (6.4273)  time: 2.6718  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 237/4722]  eta: 3:12:11  lr: 0.000010  loss: 6.1238  loss_lm: 6.4544 (6.4451)  time: 2.5711  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 237/4722]  eta: 3:12:11  lr: 0.000010  loss: 5.9785  loss_lm: 6.3747 (6.3200)  time: 2.5712  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 237/4722]  eta: 3:12:10  lr: 0.000010  loss: 6.3763  loss_lm: 6.5807 (6.5688)  time: 2.5710  data: 0.0000  max mem: 19003Train: data epoch: [0]  [ 237/4722]  eta: 3:12:10  lr: 0.000010  loss: 6.7380  loss_lm: 6.3455 (6.4205)  time: 2.5710  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [ 237/4722]  eta: 3:12:11  lr: 0.000010  loss: 6.2918  loss_lm: 6.4026 (6.4160)  time: 2.5710  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 237/4722]  eta: 3:12:10  lr: 0.000010  loss: 6.0354  loss_lm: 6.4023 (6.4452)  time: 2.5710  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 238/4722]  eta: 3:05:47  lr: 0.000010  loss: 6.3004  loss_lm: 6.4544 (6.4339)  time: 2.4861  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 238/4722]  eta: 3:05:48  lr: 0.000010  loss: 6.5247  loss_lm: 6.3790 (6.3357)  time: 2.4862  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 238/4722]  eta: 3:05:47  lr: 0.000010  loss: 6.3427  loss_lm: 6.4026 (6.4104)  time: 2.4860  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 238/4722]  eta: 3:05:47  lr: 0.000010  loss: 6.2782  loss_lm: 6.3455 (6.4095)  time: 2.4860  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 238/4722]  eta: 3:05:47  lr: 0.000010  loss: 5.9514  loss_lm: 6.5807 (6.5213)  time: 2.4860  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 238/4722]  eta: 3:05:47  lr: 0.000010  loss: 6.3241  loss_lm: 6.4023 (6.4359)  time: 2.4860  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 239/4722]  eta: 3:00:19  lr: 0.000010  loss: 6.7518  loss_lm: 6.4544 (6.4566)  time: 2.4134  data: 0.0000  max mem: 18878Train: data epoch: [0]  [ 239/4722]  eta: 3:00:19  lr: 0.000010  loss: 6.4396  loss_lm: 6.3790 (6.3431)  time: 2.4134  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 239/4722]  eta: 3:00:18  lr: 0.000010  loss: 6.6860  loss_lm: 6.3455 (6.4293)  time: 2.4133  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 239/4722]  eta: 3:00:18  lr: 0.000010  loss: 6.4788  loss_lm: 6.4026 (6.4153)  time: 2.4133  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 239/4722]  eta: 3:00:18  lr: 0.000010  loss: 6.4509  loss_lm: 6.5127 (6.5162)  time: 2.4133  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 239/4722]  eta: 3:00:18  lr: 0.000010  loss: 6.5444  loss_lm: 6.4023 (6.4437)  time: 2.4133  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 240/4722]  eta: 3:22:56  lr: 0.000010  loss: 6.6478  loss_lm: 6.4396 (6.3634)  time: 2.7167  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 240/4722]  eta: 3:22:55  lr: 0.000010  loss: 6.5579  loss_lm: 6.3522 (6.4379)  time: 2.7166  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 240/4722]  eta: 3:22:55  lr: 0.000010  loss: 5.8472  loss_lm: 6.4026 (6.3774)  time: 2.7166  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 240/4722]  eta: 3:22:55  lr: 0.000010  loss: 6.3537  loss_lm: 6.5127 (6.5054)  time: 2.7166  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 240/4722]  eta: 3:22:55  lr: 0.000010  loss: 6.2794  loss_lm: 6.4023 (6.4327)  time: 2.7166  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 240/4722]  eta: 3:22:56  lr: 0.000010  loss: 6.5673  loss_lm: 6.5673 (6.4640)  time: 2.7167  data: 0.0000  max mem: 18878
Train: data epoch: [0]  [ 241/4722]  eta: 3:17:04  lr: 0.000010  loss: 6.6958  loss_lm: 6.4396 (6.3842)  time: 2.6388  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 241/4722]  eta: 3:17:03  lr: 0.000010  loss: 6.3167  loss_lm: 6.4544 (6.4548)  time: 2.6387  data: 0.0000  max mem: 18878
Train: data epoch: [0]  [ 241/4722]  eta: 3:17:03  lr: 0.000010  loss: 6.7853  loss_lm: 6.4023 (6.4548)  time: 2.6386  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 241/4722]  eta: 3:17:03  lr: 0.000010  loss: 6.0450  loss_lm: 6.3803 (6.3566)  time: 2.6386  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 241/4722]  eta: 3:17:03  lr: 0.000010  loss: 6.1389  loss_lm: 6.3455 (6.4192)  time: 2.6386  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 241/4722]  eta: 3:17:03  lr: 0.000010  loss: 6.4573  loss_lm: 6.4573 (6.5024)  time: 2.6386  data: 0.0000  max mem: 19003
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 242/4722]  eta: 3:11:53  lr: 0.000010  loss: 6.0035  loss_lm: 6.4396 (6.3618)  time: 2.5699  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 242/4722]  eta: 3:11:52  lr: 0.000010  loss: 6.0072  loss_lm: 6.3455 (6.3949)  time: 2.5698  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 242/4722]  eta: 3:11:52  lr: 0.000010  loss: 6.4891  loss_lm: 6.4891 (6.4568)  time: 2.5698  data: 0.0000  max mem: 18878
Train: data epoch: [0]  [ 242/4722]  eta: 3:11:52  lr: 0.000010  loss: 6.0540  loss_lm: 6.4023 (6.4312)  time: 2.5697  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 242/4722]  eta: 3:11:52  lr: 0.000010  loss: 6.0269  loss_lm: 6.3803 (6.3372)  time: 2.5698  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 242/4722]  eta: 3:11:52  lr: 0.000010  loss: 6.9782  loss_lm: 6.5127 (6.5304)  time: 2.5697  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 243/4722]  eta: 3:07:25  lr: 0.000010  loss: 6.6374  loss_lm: 6.4396 (6.3771)  time: 2.5107  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 243/4722]  eta: 3:07:25  lr: 0.000010  loss: 6.5918  loss_lm: 6.4891 (6.4643)  time: 2.5106  data: 0.0000  max mem: 18878
Train: data epoch: [0]  [ 243/4722]  eta: 3:07:24  lr: 0.000010  loss: 6.8379  loss_lm: 6.5127 (6.5475)  time: 2.5106  data: 0.0000  max mem: 19003Train: data epoch: [0]  [ 243/4722]  eta: 3:07:24  lr: 0.000010  loss: 5.9092  loss_lm: 6.3238 (6.3680)  time: 2.5106  data: 0.0000  max mem: 18867Train: data epoch: [0]  [ 243/4722]  eta: 3:07:24  lr: 0.000010  loss: 6.6448  loss_lm: 6.3803 (6.3543)  time: 2.5106  data: 0.0000  max mem: 19004


Train: data epoch: [0]  [ 243/4722]  eta: 3:07:24  lr: 0.000010  loss: 6.3725  loss_lm: 6.3725 (6.4279)  time: 2.5105  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 244/4722]  eta: 3:03:19  lr: 0.000010  loss: 6.3308  loss_lm: 6.4891 (6.4573)  time: 2.4564  data: 0.0000  max mem: 18878
Train: data epoch: [0]  [ 244/4722]  eta: 3:03:20  lr: 0.000010  loss: 6.4749  loss_lm: 6.4749 (6.3823)  time: 2.4565  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 244/4722]  eta: 3:03:19  lr: 0.000010  loss: 6.3155  loss_lm: 6.3803 (6.3523)  time: 2.4563  data: 0.0000  max mem: 19004

Train: data epoch: [0]  [ 244/4722]  eta: 3:03:19  lr: 0.000010  loss: 6.4436  loss_lm: 6.5127 (6.5420)  time: 2.4563  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 244/4722]  eta: 3:03:19  lr: 0.000010  loss: 6.4589  loss_lm: 6.3455 (6.3727)  time: 2.4563  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 244/4722]  eta: 3:03:19  lr: 0.000010  loss: 6.6412  loss_lm: 6.4023 (6.4391)  time: 2.4563  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 245/4722]  eta: 3:18:52  lr: 0.000010  loss: 6.6565  loss_lm: 6.4749 (6.3960)  time: 2.6653  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 245/4722]  eta: 3:18:52  lr: 0.000010  loss: 6.3007  loss_lm: 6.3427 (6.3497)  time: 2.6652  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 245/4722]  eta: 3:18:52  lr: 0.000010  loss: 6.5225  loss_lm: 6.3455 (6.3802)  time: 2.6652  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 245/4722]  eta: 3:18:52  lr: 0.000010  loss: 7.0386  loss_lm: 6.5127 (6.5668)  time: 2.6652  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 245/4722]  eta: 3:18:52  lr: 0.000010  loss: 6.6811  loss_lm: 6.4023 (6.4512)  time: 2.6652  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 245/4722]  eta: 3:18:52  lr: 0.000010  loss: 6.6423  loss_lm: 6.4891 (6.4665)  time: 2.6653  data: 0.0000  max mem: 18878
Train: data epoch: [0]  [ 246/4722]  eta: 3:14:33  lr: 0.000010  loss: 6.3576  loss_lm: 6.4891 (6.4613)  time: 2.5275  data: 0.0000  max mem: 18878
Train: data epoch: [0]  [ 246/4722]  eta: 3:14:33  lr: 0.000010  loss: 5.8720  loss_lm: 6.4749 (6.3710)  time: 2.5275  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 246/4722]  eta: 3:14:32  lr: 0.000010  loss: 6.7526  loss_lm: 6.4023 (6.4656)  time: 2.5274  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 246/4722]  eta: 3:14:32  lr: 0.000010  loss: 6.7413  loss_lm: 6.3427 (6.3684)  time: 2.5275  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 246/4722]  eta: 3:14:32  lr: 0.000010  loss: 6.4437  loss_lm: 6.3455 (6.3833)  time: 2.5275  data: 0.0000  max mem: 18867Train: data epoch: [0]  [ 246/4722]  eta: 3:14:32  lr: 0.000010  loss: 6.5948  loss_lm: 6.5127 (6.5682)  time: 2.5275  data: 0.0000  max mem: 19003

Train: data epoch: [0]  [ 247/4722]  eta: 3:10:39  lr: 0.000010  loss: 6.0331  loss_lm: 6.4749 (6.3557)  time: 2.5274  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 247/4722]  eta: 3:10:39  lr: 0.000010  loss: 6.5506  loss_lm: 6.4891 (6.4654)  time: 2.5273  data: 0.0000  max mem: 18878
Train: data epoch: [0]  [ 247/4722]  eta: 3:10:38  lr: 0.000010  loss: 6.0328  loss_lm: 6.3155 (6.3531)  time: 2.5273  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 247/4722]  eta: 3:10:38  lr: 0.000010  loss: 6.0858  loss_lm: 6.5127 (6.5462)  time: 2.5273  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 247/4722]  eta: 3:10:38  lr: 0.000010  loss: 6.1019  loss_lm: 6.3455 (6.3705)  time: 2.5273  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 247/4722]  eta: 3:10:38  lr: 0.000010  loss: 6.5006  loss_lm: 6.4264 (6.4672)  time: 2.5273  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [ 248/4722]  eta: 3:07:05  lr: 0.000010  loss: 6.1568  loss_lm: 6.4749 (6.3470)  time: 2.5274  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 248/4722]  eta: 3:07:04  lr: 0.000010  loss: 6.9269  loss_lm: 6.3427 (6.3781)  time: 2.5274  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [ 248/4722]  eta: 3:07:05  lr: 0.000010  loss: 5.9757  loss_lm: 6.4276 (6.4441)  time: 2.5274  data: 0.0000  max mem: 18878
Train: data epoch: [0]  [ 248/4722]  eta: 3:07:04  lr: 0.000010  loss: 5.9446  loss_lm: 6.5127 (6.5201)  time: 2.5274  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [ 248/4722]  eta: 3:07:04  lr: 0.000010  loss: 6.8489  loss_lm: 6.3455 (6.3913)  time: 2.5274  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 248/4722]  eta: 3:07:04  lr: 0.000010  loss: 6.1224  loss_lm: 6.4023 (6.4522)  time: 2.5273  data: 0.0000  max mem: 18908
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:57:42.733630] Starting job[2023-11-13 00:57:42.733633] Starting job

[2023-11-13 00:57:42.733634] Starting job[2023-11-13 00:57:42.733633] Starting job

[2023-11-13 00:57:42.733750] Starting job
[2023-11-13 00:57:42.745841] Starting job
| distributed init (rank 8, world 12): env://
| distributed init (rank 10, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 11, world 12): env://
| distributed init (rank 9, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 246/4722]  eta: 5:17:26  lr: 0.000010  loss: 6.3995  loss_lm: 6.3995 (6.3995)  time: 4.2551  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 246/4722]  eta: 5:17:17  lr: 0.000010  loss: 6.4267  loss_lm: 6.4267 (6.4267)  time: 4.2533  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 246/4722]  eta: 5:17:21  lr: 0.000010  loss: 6.7511  loss_lm: 6.7511 (6.7511)  time: 4.2541  data: 0.0000  max mem: 18865


Train: data epoch: [0]  [ 246/4722]  eta: 5:17:24  lr: 0.000010  loss: 6.7486  loss_lm: 6.7486 (6.7486)  time: 4.2549  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 246/4722]  eta: 5:17:24  lr: 0.000010  loss: 6.6368  loss_lm: 6.6368 (6.6368)  time: 4.2549  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 246/4722]  eta: 5:17:06  lr: 0.000010  loss: 5.8890  loss_lm: 5.8890 (5.8890)  time: 4.2509  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 247/4722]  eta: 3:36:44  lr: 0.000010  loss: 6.5142  loss_lm: 6.5142 (6.6326)  time: 2.9061  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 247/4722]  eta: 3:36:42  lr: 0.000010  loss: 6.0794  loss_lm: 6.0794 (6.2531)  time: 2.9057  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 247/4722]  eta: 3:36:36  lr: 0.000010  loss: 6.0361  loss_lm: 5.8890 (5.9626)  time: 2.9043  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 247/4722]  eta: 3:36:47  lr: 0.000010  loss: 6.5759  loss_lm: 6.3995 (6.4877)  time: 2.9066  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 247/4722]  eta: 3:36:46  lr: 0.000010  loss: 6.0495  loss_lm: 6.0495 (6.3991)  time: 2.9065  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 247/4722]  eta: 3:36:46  lr: 0.000010  loss: 6.0934  loss_lm: 6.0934 (6.3651)  time: 2.9064  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 248/4722]  eta: 3:00:58  lr: 0.000010  loss: 6.1169  loss_lm: 6.5142 (6.4607)  time: 2.4269  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 248/4722]  eta: 3:00:57  lr: 0.000010  loss: 6.8192  loss_lm: 6.4267 (6.4418)  time: 2.4267  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 248/4722]  eta: 3:00:52  lr: 0.000010  loss: 6.1628  loss_lm: 6.0361 (6.0293)  time: 2.4258  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 248/4722]  eta: 3:00:59  lr: 0.000010  loss: 6.8807  loss_lm: 6.7486 (6.5596)  time: 2.4272  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 248/4722]  eta: 3:00:59  lr: 0.000010  loss: 5.9606  loss_lm: 6.3995 (6.3120)  time: 2.4273  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 248/4722]  eta: 3:00:58  lr: 0.000010  loss: 5.9337  loss_lm: 6.0934 (6.2213)  time: 2.4271  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 249/4722]  eta: 2:43:09  lr: 0.000010  loss: 6.2702  loss_lm: 6.2702 (6.4131)  time: 2.1887  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 249/4722]  eta: 2:43:09  lr: 0.000010  loss: 6.7175  loss_lm: 6.4267 (6.5107)  time: 2.1885  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 249/4722]  eta: 2:43:05  lr: 0.000010  loss: 6.1963  loss_lm: 6.0361 (6.0711)  time: 2.1877  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 249/4722]  eta: 2:43:10  lr: 0.000010  loss: 6.6573  loss_lm: 6.3995 (6.3983)  time: 2.1888  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 249/4722]  eta: 2:43:10  lr: 0.000010  loss: 5.9178  loss_lm: 6.0495 (6.3991)  time: 2.1888  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 249/4722]  eta: 2:43:10  lr: 0.000010  loss: 6.1285  loss_lm: 6.0934 (6.1981)  time: 2.1887  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 250/4722]  eta: 3:48:42  lr: 0.000010  loss: 5.9444  loss_lm: 6.0495 (6.3082)  time: 3.0685  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 250/4722]  eta: 3:48:42  lr: 0.000010  loss: 6.2556  loss_lm: 6.3995 (6.3698)  time: 3.0686  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 250/4722]  eta: 3:48:41  lr: 0.000010  loss: 6.3790  loss_lm: 6.3790 (6.4063)  time: 3.0684  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 250/4722]  eta: 3:48:38  lr: 0.000010  loss: 6.6111  loss_lm: 6.1628 (6.1791)  time: 3.0677  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 250/4722]  eta: 3:48:41  lr: 0.000010  loss: 6.4463  loss_lm: 6.4463 (6.4978)  time: 3.0683  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 250/4722]  eta: 3:48:42  lr: 0.000010  loss: 6.4579  loss_lm: 6.1285 (6.2501)  time: 3.0684  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 251/4722]  eta: 3:28:53  lr: 0.000010  loss: 7.0442  loss_lm: 6.3790 (6.5126)  time: 2.8034  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 251/4722]  eta: 3:28:53  lr: 0.000010  loss: 6.4562  loss_lm: 6.1285 (6.2844)  time: 2.8034  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 251/4722]  eta: 3:28:51  lr: 0.000010  loss: 6.5404  loss_lm: 6.1628 (6.2393)  time: 2.8027  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 251/4722]  eta: 3:28:54  lr: 0.000010  loss: 6.4602  loss_lm: 6.0495 (6.3335)  time: 2.8034  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 251/4722]  eta: 3:28:54  lr: 0.000010  loss: 6.2768  loss_lm: 6.2768 (6.3543)  time: 2.8035  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 251/4722]  eta: 3:28:53  lr: 0.000010  loss: 6.0371  loss_lm: 6.4267 (6.4210)  time: 2.8032  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 252/4722]  eta: 3:15:03  lr: 0.000010  loss: 6.5393  loss_lm: 6.5142 (6.5164)  time: 2.6182  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 252/4722]  eta: 3:15:03  lr: 0.000010  loss: 6.5545  loss_lm: 6.3995 (6.3829)  time: 2.6183  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 252/4722]  eta: 3:15:03  lr: 0.000010  loss: 5.9714  loss_lm: 6.0495 (6.2818)  time: 2.6183  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 252/4722]  eta: 3:15:00  lr: 0.000010  loss: 6.6058  loss_lm: 6.1963 (6.2917)  time: 2.6177  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 252/4722]  eta: 3:15:02  lr: 0.000010  loss: 6.6309  loss_lm: 6.4463 (6.4510)  time: 2.6181  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 252/4722]  eta: 3:15:03  lr: 0.000010  loss: 6.4055  loss_lm: 6.4055 (6.3017)  time: 2.6182  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 253/4722]  eta: 3:04:24  lr: 0.000010  loss: 6.4121  loss_lm: 6.4121 (6.5034)  time: 2.4759  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 253/4722]  eta: 3:04:25  lr: 0.000010  loss: 6.1639  loss_lm: 6.2768 (6.3555)  time: 2.4760  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 253/4722]  eta: 3:04:24  lr: 0.000010  loss: 6.3140  loss_lm: 6.0495 (6.2858)  time: 2.4759  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 253/4722]  eta: 3:04:22  lr: 0.000010  loss: 6.2785  loss_lm: 6.1963 (6.2900)  time: 2.4754  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 253/4722]  eta: 3:04:24  lr: 0.000010  loss: 6.0320  loss_lm: 6.4267 (6.3986)  time: 2.4758  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 253/4722]  eta: 3:04:24  lr: 0.000010  loss: 6.2334  loss_lm: 6.2334 (6.2932)  time: 2.4759  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 254/4722]  eta: 2:56:28  lr: 0.000010  loss: 6.1389  loss_lm: 6.4121 (6.4629)  time: 2.3698  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 254/4722]  eta: 2:56:28  lr: 0.000010  loss: 6.3140  loss_lm: 6.3140 (6.2955)  time: 2.3698  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 254/4722]  eta: 2:56:27  lr: 0.000010  loss: 6.4993  loss_lm: 6.4463 (6.4098)  time: 2.3697  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 254/4722]  eta: 2:56:28  lr: 0.000010  loss: 5.9246  loss_lm: 6.2768 (6.3076)  time: 2.3699  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 254/4722]  eta: 2:56:28  lr: 0.000010  loss: 6.6403  loss_lm: 6.3140 (6.3252)  time: 2.3699  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 254/4722]  eta: 2:56:26  lr: 0.000010  loss: 6.1225  loss_lm: 6.1963 (6.2714)  time: 2.3694  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 255/4722]  eta: 3:28:06  lr: 0.000010  loss: 5.9832  loss_lm: 6.2556 (6.2752)  time: 2.7953  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 255/4722]  eta: 3:28:06  lr: 0.000010  loss: 5.8403  loss_lm: 6.0495 (6.2767)  time: 2.7953  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 255/4722]  eta: 3:28:06  lr: 0.000010  loss: 6.2846  loss_lm: 6.3790 (6.4450)  time: 2.7953  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 255/4722]  eta: 3:28:06  lr: 0.000010  loss: 6.2512  loss_lm: 6.2512 (6.2911)  time: 2.7952  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 255/4722]  eta: 3:28:05  lr: 0.000010  loss: 6.2726  loss_lm: 6.4267 (6.3961)  time: 2.7951  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 255/4722]  eta: 3:28:04  lr: 0.000010  loss: 6.4263  loss_lm: 6.1963 (6.2869)  time: 2.7949  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 256/4722]  eta: 3:19:11  lr: 0.000010  loss: 5.4647  loss_lm: 6.3790 (6.3559)  time: 2.6761  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 256/4722]  eta: 3:19:11  lr: 0.000010  loss: 6.3868  loss_lm: 6.3140 (6.2998)  time: 2.6760  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 256/4722]  eta: 3:19:10  lr: 0.000010  loss: 6.3139  loss_lm: 6.4267 (6.3886)  time: 2.6759  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 256/4722]  eta: 3:19:11  lr: 0.000010  loss: 6.4214  loss_lm: 6.3140 (6.2899)  time: 2.6761  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 256/4722]  eta: 3:19:11  lr: 0.000010  loss: 5.8861  loss_lm: 6.2556 (6.2398)  time: 2.6761  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 256/4722]  eta: 3:19:09  lr: 0.000010  loss: 6.3421  loss_lm: 6.2785 (6.2919)  time: 2.6757  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 257/4722]  eta: 3:11:40  lr: 0.000010  loss: 5.7466  loss_lm: 6.2846 (6.3051)  time: 2.5757  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 257/4722]  eta: 3:11:40  lr: 0.000010  loss: 6.4938  loss_lm: 6.3140 (6.3068)  time: 2.5757  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 257/4722]  eta: 3:11:40  lr: 0.000010  loss: 6.2989  loss_lm: 6.2556 (6.2447)  time: 2.5757  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 257/4722]  eta: 3:11:40  lr: 0.000010  loss: 6.5822  loss_lm: 6.3140 (6.3233)  time: 2.5756  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 257/4722]  eta: 3:11:39  lr: 0.000010  loss: 6.0368  loss_lm: 6.3139 (6.3593)  time: 2.5755  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 257/4722]  eta: 3:11:38  lr: 0.000010  loss: 5.8972  loss_lm: 6.1963 (6.2590)  time: 2.5753  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 258/4722]  eta: 3:05:20  lr: 0.000010  loss: 5.8777  loss_lm: 6.3140 (6.2890)  time: 2.4911  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 258/4722]  eta: 3:05:20  lr: 0.000010  loss: 6.4767  loss_lm: 6.3790 (6.3183)  time: 2.4911  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 258/4722]  eta: 3:05:18  lr: 0.000010  loss: 6.0920  loss_lm: 6.1963 (6.2462)  time: 2.4908  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 258/4722]  eta: 3:05:20  lr: 0.000010  loss: 5.7209  loss_lm: 6.3140 (6.2618)  time: 2.4911  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 258/4722]  eta: 3:05:20  lr: 0.000010  loss: 6.0575  loss_lm: 6.2556 (6.2303)  time: 2.4912  data: 0.0000  max mem: 18866


Train: data epoch: [0]  [ 258/4722]  eta: 3:05:19  lr: 0.000010  loss: 6.6164  loss_lm: 6.4267 (6.3791)  time: 2.4910  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 259/4722]  eta: 2:59:53  lr: 0.000010  loss: 6.4118  loss_lm: 6.3790 (6.3250)  time: 2.4184  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 259/4722]  eta: 2:59:51  lr: 0.000010  loss: 6.0424  loss_lm: 6.1628 (6.2316)  time: 2.4181  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 259/4722]  eta: 2:59:53  lr: 0.000010  loss: 6.3606  loss_lm: 6.3140 (6.2688)  time: 2.4184  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 259/4722]  eta: 2:59:53  lr: 0.000010  loss: 5.9669  loss_lm: 6.1639 (6.2115)  time: 2.4185  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 259/4722]  eta: 2:59:53  lr: 0.000010  loss: 6.1566  loss_lm: 6.2512 (6.2796)  time: 2.4184  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 259/4722]  eta: 2:59:52  lr: 0.000010  loss: 5.7672  loss_lm: 6.3139 (6.3354)  time: 2.4183  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 260/4722]  eta: 3:21:56  lr: 0.000010  loss: 6.5332  loss_lm: 6.4118 (6.3389)  time: 2.7155  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 260/4722]  eta: 3:21:55  lr: 0.000010  loss: 6.0196  loss_lm: 6.1628 (6.2175)  time: 2.7152  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 260/4722]  eta: 3:21:56  lr: 0.000010  loss: 6.0113  loss_lm: 6.1639 (6.1982)  time: 2.7156  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 260/4722]  eta: 3:21:56  lr: 0.000010  loss: 6.2120  loss_lm: 6.3140 (6.2650)  time: 2.7155  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 260/4722]  eta: 3:21:56  lr: 0.000010  loss: 6.3850  loss_lm: 6.3850 (6.3387)  time: 2.7154  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 260/4722]  eta: 3:21:56  lr: 0.000010  loss: 6.0241  loss_lm: 6.2512 (6.2625)  time: 2.7155  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 261/4722]  eta: 3:16:09  lr: 0.000010  loss: 6.5677  loss_lm: 6.4118 (6.3532)  time: 2.6383  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 261/4722]  eta: 3:16:09  lr: 0.000010  loss: 6.0788  loss_lm: 6.2334 (6.2510)  time: 2.6383  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 261/4722]  eta: 3:16:09  lr: 0.000010  loss: 6.4773  loss_lm: 6.3140 (6.2783)  time: 2.6383  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 261/4722]  eta: 3:16:08  lr: 0.000010  loss: 6.8077  loss_lm: 6.3850 (6.3680)  time: 2.6382  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 261/4722]  eta: 3:16:08  lr: 0.000010  loss: 5.9656  loss_lm: 6.1225 (6.2017)  time: 2.6380  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 261/4722]  eta: 3:16:09  lr: 0.000010  loss: 6.5698  loss_lm: 6.1639 (6.2214)  time: 2.6383  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 262/4722]  eta: 3:11:01  lr: 0.000010  loss: 6.0410  loss_lm: 6.4118 (6.3348)  time: 2.5698  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 262/4722]  eta: 3:11:00  lr: 0.000010  loss: 5.8556  loss_lm: 6.2334 (6.2278)  time: 2.5697  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 262/4722]  eta: 3:11:01  lr: 0.000010  loss: 5.9999  loss_lm: 6.3140 (6.2619)  time: 2.5698  data: 0.0000  max mem: 18910Train: data epoch: [0]  [ 262/4722]  eta: 3:11:01  lr: 0.000010  loss: 5.9551  loss_lm: 6.1639 (6.2057)  time: 2.5698  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 262/4722]  eta: 3:11:00  lr: 0.000010  loss: 5.7734  loss_lm: 6.3850 (6.3330)  time: 2.5697  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 262/4722]  eta: 3:11:00  lr: 0.000010  loss: 6.6014  loss_lm: 6.1628 (6.2253)  time: 2.5695  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 263/4722]  eta: 3:06:26  lr: 0.000010  loss: 6.2184  loss_lm: 6.3790 (6.3284)  time: 2.5087  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 263/4722]  eta: 3:06:26  lr: 0.000010  loss: 6.6714  loss_lm: 6.3140 (6.2847)  time: 2.5087  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 263/4722]  eta: 3:06:26  lr: 0.000010  loss: 6.2109  loss_lm: 6.2109 (6.2268)  time: 2.5087  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 263/4722]  eta: 3:06:26  lr: 0.000010  loss: 6.3818  loss_lm: 6.1639 (6.2155)  time: 2.5088  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 263/4722]  eta: 3:06:25  lr: 0.000010  loss: 6.2128  loss_lm: 6.1628 (6.2246)  time: 2.5085  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 263/4722]  eta: 3:06:25  lr: 0.000010  loss: 6.3056  loss_lm: 6.3139 (6.3315)  time: 2.5086  data: 0.0000  max mem: 18976
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 264/4722]  eta: 3:02:31  lr: 0.000010  loss: 5.9019  loss_lm: 6.2109 (6.2097)  time: 2.4565  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 264/4722]  eta: 3:02:31  lr: 0.000010  loss: 6.2472  loss_lm: 6.3790 (6.3241)  time: 2.4565  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 264/4722]  eta: 3:02:31  lr: 0.000010  loss: 5.8867  loss_lm: 6.1639 (6.1982)  time: 2.4566  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 264/4722]  eta: 3:02:31  lr: 0.000010  loss: 6.1258  loss_lm: 6.3140 (6.2763)  time: 2.4565  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 264/4722]  eta: 3:02:30  lr: 0.000010  loss: 6.3688  loss_lm: 6.1963 (6.2322)  time: 2.4563  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 264/4722]  eta: 3:02:30  lr: 0.000010  loss: 6.1257  loss_lm: 6.3139 (6.3207)  time: 2.4564  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 265/4722]  eta: 3:18:52  lr: 0.000010  loss: 5.8809  loss_lm: 6.2846 (6.3019)  time: 2.6773  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 265/4722]  eta: 3:18:52  lr: 0.000010  loss: 6.1981  loss_lm: 6.1639 (6.1982)  time: 2.6773  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 265/4722]  eta: 3:18:52  lr: 0.000010  loss: 5.9277  loss_lm: 6.2120 (6.2589)  time: 2.6773  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 265/4722]  eta: 3:18:51  lr: 0.000010  loss: 6.3603  loss_lm: 6.1963 (6.2386)  time: 2.6771  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 265/4722]  eta: 3:18:52  lr: 0.000010  loss: 6.6393  loss_lm: 6.3139 (6.3366)  time: 2.6772  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 265/4722]  eta: 3:18:52  lr: 0.000010  loss: 5.6535  loss_lm: 6.1566 (6.1819)  time: 2.6773  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 266/4722]  eta: 3:14:34  lr: 0.000010  loss: 6.8212  loss_lm: 6.1566 (6.2124)  time: 2.5382  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 266/4722]  eta: 3:14:33  lr: 0.000010  loss: 6.2302  loss_lm: 6.2128 (6.2382)  time: 2.5382  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 266/4722]  eta: 3:14:34  lr: 0.000010  loss: 5.8535  loss_lm: 6.1258 (6.2396)  time: 2.5383  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 266/4722]  eta: 3:14:34  lr: 0.000010  loss: 5.9060  loss_lm: 6.2702 (6.2831)  time: 2.5384  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 266/4722]  eta: 3:14:34  lr: 0.000010  loss: 6.4964  loss_lm: 6.1639 (6.2124)  time: 2.5383  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 266/4722]  eta: 3:14:34  lr: 0.000010  loss: 6.3015  loss_lm: 6.3056 (6.3349)  time: 2.5382  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 267/4722]  eta: 3:10:41  lr: 0.000010  loss: 6.3041  loss_lm: 6.2702 (6.2840)  time: 2.5346  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 267/4722]  eta: 3:10:41  lr: 0.000010  loss: 6.1904  loss_lm: 6.1904 (6.2114)  time: 2.5345  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 267/4722]  eta: 3:10:40  lr: 0.000010  loss: 6.6295  loss_lm: 6.2302 (6.2560)  time: 2.5345  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 267/4722]  eta: 3:10:41  lr: 0.000010  loss: 6.2241  loss_lm: 6.1639 (6.2129)  time: 2.5345  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 267/4722]  eta: 3:10:41  lr: 0.000010  loss: 6.3734  loss_lm: 6.2120 (6.2457)  time: 2.5345  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 267/4722]  eta: 3:10:41  lr: 0.000010  loss: 6.6025  loss_lm: 6.3139 (6.3471)  time: 2.5345  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [ 268/4722]  eta: 3:07:07  lr: 0.000010  loss: 6.3334  loss_lm: 6.2846 (6.2862)  time: 2.5349  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 268/4722]  eta: 3:07:07  lr: 0.000010  loss: 6.0859  loss_lm: 6.1904 (6.2059)  time: 2.5348  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 268/4722]  eta: 3:07:07  lr: 0.000010  loss: 6.2480  loss_lm: 6.1981 (6.2145)  time: 2.5348  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 268/4722]  eta: 3:07:07  lr: 0.000010  loss: 6.0749  loss_lm: 6.1258 (6.2382)  time: 2.5348  data: 0.0000  max mem: 18910Train: data epoch: [0]  [ 268/4722]  eta: 3:07:06  lr: 0.000010  loss: 6.2335  loss_lm: 6.2335 (6.2550)  time: 2.5348  data: 0.0000  max mem: 18865


Train: data epoch: [0]  [ 268/4722]  eta: 3:07:06  lr: 0.000010  loss: 5.6563  loss_lm: 6.3056 (6.3171)  time: 2.5347  data: 0.0000  max mem: 18976
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 269/4722]  eta: 3:03:50  lr: 0.000010  loss: 5.9270  loss_lm: 6.3015 (6.3008)  time: 2.5347  data: 0.0000  max mem: 18976Train: data epoch: [0]  [ 269/4722]  eta: 3:03:50  lr: 0.000010  loss: 6.1104  loss_lm: 6.2846 (6.2789)  time: 2.5349  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 269/4722]  eta: 3:03:50  lr: 0.000010  loss: 6.4223  loss_lm: 6.2109 (6.2149)  time: 2.5348  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 269/4722]  eta: 3:03:50  lr: 0.000010  loss: 6.2980  loss_lm: 6.2120 (6.2407)  time: 2.5348  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 269/4722]  eta: 3:03:50  lr: 0.000010  loss: 6.0745  loss_lm: 6.1639 (6.2086)  time: 2.5348  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 269/4722]  eta: 3:03:49  lr: 0.000010  loss: 5.7643  loss_lm: 6.2335 (6.2345)  time: 2.5348  data: 0.0000  max mem: 18865
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 01:03:58.852294] Starting job[2023-11-13 01:03:58.852297] Starting job[2023-11-13 01:03:58.852298] Starting job


[2023-11-13 01:03:58.852391] Starting job
| distributed init (rank 6, world 12): env://
| distributed init (rank 9, world 12): env://
| distributed init (rank 11, world 12): env://
| distributed init (rank 7, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 01:04:03.286413] Starting job
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
| distributed init (rank 10, world 12): env://
[2023-11-13 01:04:03.541625] Starting job
| distributed init (rank 8, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 266/4722]  eta: 5:30:14  lr: 0.000010  loss: 5.8254  loss_lm: 5.8254 (5.8254)  time: 4.4467  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 266/4722]  eta: 5:30:15  lr: 0.000010  loss: 6.5265  loss_lm: 6.5265 (6.5265)  time: 4.4469  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 266/4722]  eta: 5:30:18  lr: 0.000010  loss: 6.2450  loss_lm: 6.2450 (6.2450)  time: 4.4476  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 266/4722]  eta: 5:30:16  lr: 0.000010  loss: 6.3096  loss_lm: 6.3096 (6.3096)  time: 4.4472  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 266/4722]  eta: 5:30:14  lr: 0.000010  loss: 6.8510  loss_lm: 6.8510 (6.8510)  time: 4.4468  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 266/4722]  eta: 5:30:13  lr: 0.000010  loss: 5.8981  loss_lm: 5.8981 (5.8981)  time: 4.4464  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [ 267/4722]  eta: 3:40:26  lr: 0.000010  loss: 6.6089  loss_lm: 6.3096 (6.4592)  time: 2.9690  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 267/4722]  eta: 3:40:27  lr: 0.000010  loss: 6.6630  loss_lm: 6.2450 (6.4540)  time: 2.9692  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 267/4722]  eta: 3:40:26  lr: 0.000010  loss: 6.2273  loss_lm: 6.2273 (6.3769)  time: 2.9688  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 267/4722]  eta: 3:40:25  lr: 0.000010  loss: 6.2066  loss_lm: 6.2066 (6.5288)  time: 2.9687  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 267/4722]  eta: 3:40:25  lr: 0.000010  loss: 6.3555  loss_lm: 5.8254 (6.0905)  time: 2.9688  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 267/4722]  eta: 3:40:24  lr: 0.000010  loss: 6.3182  loss_lm: 5.8981 (6.1082)  time: 2.9685  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 268/4722]  eta: 3:03:25  lr: 0.000010  loss: 6.2169  loss_lm: 6.2450 (6.3749)  time: 2.4709  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 268/4722]  eta: 3:03:24  lr: 0.000010  loss: 5.6262  loss_lm: 6.3096 (6.1816)  time: 2.4707  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 268/4722]  eta: 3:03:24  lr: 0.000010  loss: 6.2529  loss_lm: 6.2529 (6.3356)  time: 2.4706  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 268/4722]  eta: 3:03:23  lr: 0.000010  loss: 6.0637  loss_lm: 6.0637 (6.0815)  time: 2.4706  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 268/4722]  eta: 3:03:23  lr: 0.000010  loss: 6.0686  loss_lm: 6.2066 (6.3754)  time: 2.4705  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 268/4722]  eta: 3:03:23  lr: 0.000010  loss: 6.3705  loss_lm: 6.3182 (6.1956)  time: 2.4704  data: 0.0000  max mem: 18867

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 269/4722]  eta: 2:44:56  lr: 0.000010  loss: 5.7613  loss_lm: 6.2169 (6.2215)  time: 2.2224  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 269/4722]  eta: 2:44:55  lr: 0.000010  loss: 6.0735  loss_lm: 6.2273 (6.2700)  time: 2.2222  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 269/4722]  eta: 2:44:55  lr: 0.000010  loss: 5.8845  loss_lm: 5.8845 (6.1073)  time: 2.2223  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 269/4722]  eta: 2:44:55  lr: 0.000010  loss: 6.2736  loss_lm: 6.0637 (6.1296)  time: 2.2222  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 269/4722]  eta: 2:44:55  lr: 0.000010  loss: 6.4466  loss_lm: 6.2066 (6.3932)  time: 2.2221  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 269/4722]  eta: 2:44:54  lr: 0.000010  loss: 6.0916  loss_lm: 6.0916 (6.1696)  time: 2.2220  data: 0.0000  max mem: 18867
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 270/4722]  eta: 3:52:06  lr: 0.000010  loss: 6.0242  loss_lm: 6.0242 (6.0907)  time: 3.1281  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 270/4722]  eta: 3:52:05  lr: 0.000010  loss: 6.1765  loss_lm: 6.1765 (6.1710)  time: 3.1279  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 270/4722]  eta: 3:52:06  lr: 0.000010  loss: 5.8933  loss_lm: 6.2273 (6.1947)  time: 3.1280  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 270/4722]  eta: 3:52:05  lr: 0.000010  loss: 6.1565  loss_lm: 6.1565 (6.1350)  time: 3.1280  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 270/4722]  eta: 3:52:05  lr: 0.000010  loss: 6.1931  loss_lm: 6.2066 (6.3532)  time: 3.1280  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 270/4722]  eta: 3:52:06  lr: 0.000010  loss: 6.9469  loss_lm: 6.2450 (6.3666)  time: 3.1282  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 271/4722]  eta: 3:31:39  lr: 0.000010  loss: 6.3055  loss_lm: 6.2273 (6.2132)  time: 2.8531  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 271/4722]  eta: 3:31:39  lr: 0.000010  loss: 6.0183  loss_lm: 6.2169 (6.3086)  time: 2.8532  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 271/4722]  eta: 3:31:39  lr: 0.000010  loss: 6.1579  loss_lm: 6.1565 (6.1388)  time: 2.8531  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 271/4722]  eta: 3:31:39  lr: 0.000010  loss: 5.9651  loss_lm: 5.9651 (6.0697)  time: 2.8532  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 271/4722]  eta: 3:31:39  lr: 0.000010  loss: 6.2610  loss_lm: 6.2066 (6.3378)  time: 2.8531  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 271/4722]  eta: 3:31:38  lr: 0.000010  loss: 5.7969  loss_lm: 6.0916 (6.1086)  time: 2.8530  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 272/4722]  eta: 3:17:00  lr: 0.000010  loss: 6.4567  loss_lm: 6.2529 (6.2479)  time: 2.6563  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 272/4722]  eta: 3:17:00  lr: 0.000010  loss: 6.4590  loss_lm: 6.2450 (6.3300)  time: 2.6564  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 272/4722]  eta: 3:17:00  lr: 0.000010  loss: 6.1640  loss_lm: 6.2066 (6.3130)  time: 2.6562  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 272/4722]  eta: 3:17:00  lr: 0.000010  loss: 6.1580  loss_lm: 6.0242 (6.0824)  time: 2.6564  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 272/4722]  eta: 3:17:00  lr: 0.000010  loss: 6.2529  loss_lm: 6.1579 (6.1551)  time: 2.6563  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 272/4722]  eta: 3:17:00  lr: 0.000010  loss: 6.5006  loss_lm: 6.1765 (6.1646)  time: 2.6562  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 273/4722]  eta: 3:06:00  lr: 0.000010  loss: 6.0326  loss_lm: 6.2273 (6.2210)  time: 2.5086  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 273/4722]  eta: 3:06:00  lr: 0.000010  loss: 5.7737  loss_lm: 6.2169 (6.2605)  time: 2.5086  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 273/4722]  eta: 3:06:00  lr: 0.000010  loss: 6.2502  loss_lm: 6.0242 (6.1033)  time: 2.5086  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 273/4722]  eta: 3:06:00  lr: 0.000010  loss: 5.9632  loss_lm: 6.1565 (6.1311)  time: 2.5085  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 273/4722]  eta: 3:06:00  lr: 0.000010  loss: 6.2950  loss_lm: 6.2066 (6.3107)  time: 2.5085  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 273/4722]  eta: 3:06:00  lr: 0.000010  loss: 6.3440  loss_lm: 6.1765 (6.1871)  time: 2.5084  data: 0.0000  max mem: 18867
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 274/4722]  eta: 2:57:25  lr: 0.000010  loss: 5.7369  loss_lm: 6.2273 (6.1672)  time: 2.3934  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 274/4722]  eta: 2:57:25  lr: 0.000010  loss: 6.0733  loss_lm: 6.0733 (6.1000)  time: 2.3934  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [ 274/4722]  eta: 2:57:25  lr: 0.000010  loss: 5.8760  loss_lm: 6.2169 (6.2178)  time: 2.3934  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 274/4722]  eta: 2:57:25  lr: 0.000010  loss: 6.4268  loss_lm: 6.1579 (6.1639)  time: 2.3933  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 274/4722]  eta: 2:57:25  lr: 0.000010  loss: 6.4391  loss_lm: 6.2610 (6.3250)  time: 2.3933  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 274/4722]  eta: 2:57:25  lr: 0.000010  loss: 6.2564  loss_lm: 6.2564 (6.1948)  time: 2.3933  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 275/4722]  eta: 3:29:33  lr: 0.000010  loss: 6.1652  loss_lm: 6.1652 (6.1670)  time: 2.8274  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 275/4722]  eta: 3:29:33  lr: 0.000010  loss: 6.2478  loss_lm: 6.0733 (6.1148)  time: 2.8274  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 275/4722]  eta: 3:29:33  lr: 0.000010  loss: 6.1707  loss_lm: 6.2066 (6.3096)  time: 2.8273  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 275/4722]  eta: 3:29:32  lr: 0.000010  loss: 6.0405  loss_lm: 6.1765 (6.1793)  time: 2.8273  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 275/4722]  eta: 3:29:33  lr: 0.000010  loss: 6.0945  loss_lm: 6.1565 (6.1570)  time: 2.8273  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 275/4722]  eta: 3:29:33  lr: 0.000010  loss: 5.9863  loss_lm: 6.0183 (6.1946)  time: 2.8274  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 276/4722]  eta: 3:20:26  lr: 0.000010  loss: 6.0628  loss_lm: 6.0733 (6.1100)  time: 2.7051  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 276/4722]  eta: 3:20:26  lr: 0.000010  loss: 6.0430  loss_lm: 6.1652 (6.1558)  time: 2.7051  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 276/4722]  eta: 3:20:26  lr: 0.000010  loss: 5.9673  loss_lm: 6.0183 (6.1740)  time: 2.7051  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 276/4722]  eta: 3:20:26  lr: 0.000010  loss: 5.7912  loss_lm: 6.2066 (6.2624)  time: 2.7050  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 276/4722]  eta: 3:20:26  lr: 0.000010  loss: 6.0332  loss_lm: 6.1565 (6.1457)  time: 2.7050  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 276/4722]  eta: 3:20:26  lr: 0.000010  loss: 6.4000  loss_lm: 6.2564 (6.1994)  time: 2.7050  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 277/4722]  eta: 3:12:51  lr: 0.000010  loss: 5.8355  loss_lm: 6.0628 (6.0872)  time: 2.6032  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 277/4722]  eta: 3:12:50  lr: 0.000010  loss: 6.1660  loss_lm: 6.0183 (6.1733)  time: 2.6031  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 277/4722]  eta: 3:12:50  lr: 0.000010  loss: 6.2006  loss_lm: 6.1565 (6.1503)  time: 2.6031  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 277/4722]  eta: 3:12:51  lr: 0.000010  loss: 6.1613  loss_lm: 6.1613 (6.1562)  time: 2.6032  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 277/4722]  eta: 3:12:50  lr: 0.000010  loss: 6.0890  loss_lm: 6.1931 (6.2480)  time: 2.6031  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 277/4722]  eta: 3:12:50  lr: 0.000010  loss: 6.0402  loss_lm: 6.1765 (6.1861)  time: 2.6030  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 278/4722]  eta: 3:06:27  lr: 0.000010  loss: 6.6075  loss_lm: 6.1660 (6.2067)  time: 2.5174  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 278/4722]  eta: 3:06:27  lr: 0.000010  loss: 6.4600  loss_lm: 6.0733 (6.1158)  time: 2.5174  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [ 278/4722]  eta: 3:06:27  lr: 0.000010  loss: 5.8826  loss_lm: 6.1613 (6.1352)  time: 2.5174  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 278/4722]  eta: 3:06:26  lr: 0.000010  loss: 6.0754  loss_lm: 6.1565 (6.1445)  time: 2.5173  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 278/4722]  eta: 3:06:26  lr: 0.000010  loss: 6.0949  loss_lm: 6.1765 (6.1791)  time: 2.5173  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 278/4722]  eta: 3:06:26  lr: 0.000010  loss: 5.8944  loss_lm: 6.1931 (6.2208)  time: 2.5173  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 279/4722]  eta: 3:00:53  lr: 0.000010  loss: 5.8793  loss_lm: 6.0183 (6.1833)  time: 2.4428  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 279/4722]  eta: 3:00:53  lr: 0.000010  loss: 6.5543  loss_lm: 6.0733 (6.1472)  time: 2.4428  data: 0.0000  max mem: 18888

Train: data epoch: [0]  [ 279/4722]  eta: 3:00:53  lr: 0.000010  loss: 6.3355  loss_lm: 6.1613 (6.1495)  time: 2.4428  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 279/4722]  eta: 3:00:52  lr: 0.000010  loss: 6.0403  loss_lm: 6.0945 (6.1371)  time: 2.4427  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 279/4722]  eta: 3:00:52  lr: 0.000010  loss: 6.5332  loss_lm: 6.1931 (6.2431)  time: 2.4427  data: 0.0000  max mem: 18866


Train: data epoch: [0]  [ 279/4722]  eta: 3:00:52  lr: 0.000010  loss: 6.2034  loss_lm: 6.1765 (6.1809)  time: 2.4426  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 280/4722]  eta: 3:22:06  lr: 0.000010  loss: 6.3740  loss_lm: 6.1652 (6.1644)  time: 2.7300  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 280/4722]  eta: 3:22:06  lr: 0.000010  loss: 6.0773  loss_lm: 6.1765 (6.1739)  time: 2.7299  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 280/4722]  eta: 3:22:06  lr: 0.000010  loss: 6.3461  loss_lm: 6.1565 (6.1510)  time: 2.7299  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 280/4722]  eta: 3:22:06  lr: 0.000010  loss: 6.0624  loss_lm: 6.0733 (6.1415)  time: 2.7300  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 280/4722]  eta: 3:22:06  lr: 0.000010  loss: 6.5843  loss_lm: 6.2066 (6.2659)  time: 2.7299  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 280/4722]  eta: 3:22:06  lr: 0.000010  loss: 6.2009  loss_lm: 6.1660 (6.1845)  time: 2.7300  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 281/4722]  eta: 3:16:16  lr: 0.000010  loss: 5.7748  loss_lm: 6.0628 (6.1186)  time: 2.6519  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 281/4722]  eta: 3:16:16  lr: 0.000010  loss: 6.0023  loss_lm: 6.0183 (6.1731)  time: 2.6518  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 281/4722]  eta: 3:16:16  lr: 0.000010  loss: 6.2512  loss_lm: 6.2066 (6.2649)  time: 2.6518  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 281/4722]  eta: 3:16:16  lr: 0.000010  loss: 6.0022  loss_lm: 6.0949 (6.1632)  time: 2.6517  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 281/4722]  eta: 3:16:16  lr: 0.000010  loss: 6.6070  loss_lm: 6.1652 (6.1921)  time: 2.6518  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 281/4722]  eta: 3:16:16  lr: 0.000010  loss: 6.1722  loss_lm: 6.1565 (6.1524)  time: 2.6518  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 282/4722]  eta: 3:11:07  lr: 0.000010  loss: 6.1716  loss_lm: 6.1660 (6.1730)  time: 2.5829  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 282/4722]  eta: 3:11:08  lr: 0.000010  loss: 6.1220  loss_lm: 6.0733 (6.1188)  time: 2.5829  data: 0.0000  max mem: 18888

Train: data epoch: [0]  [ 282/4722]  eta: 3:11:07  lr: 0.000010  loss: 6.1917  loss_lm: 6.2066 (6.2606)  time: 2.5828  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 282/4722]  eta: 3:11:07  lr: 0.000010  loss: 6.0017  loss_lm: 6.0949 (6.1537)  time: 2.5828  data: 0.0000  max mem: 18867Train: data epoch: [0]  [ 282/4722]  eta: 3:11:07  lr: 0.000010  loss: 5.9264  loss_lm: 6.1565 (6.1391)  time: 2.5828  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 282/4722]  eta: 3:11:07  lr: 0.000010  loss: 6.1026  loss_lm: 6.1652 (6.1868)  time: 2.5829  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 283/4722]  eta: 3:06:31  lr: 0.000010  loss: 6.0862  loss_lm: 6.0733 (6.1170)  time: 2.5211  data: 0.0000  max mem: 18888Train: data epoch: [0]  [ 283/4722]  eta: 3:06:31  lr: 0.000010  loss: 6.5531  loss_lm: 6.1652 (6.2072)  time: 2.5211  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 283/4722]  eta: 3:06:31  lr: 0.000010  loss: 6.1690  loss_lm: 6.1660 (6.1728)  time: 2.5211  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 283/4722]  eta: 3:06:30  lr: 0.000010  loss: 6.1883  loss_lm: 6.1565 (6.1418)  time: 2.5210  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 283/4722]  eta: 3:06:30  lr: 0.000010  loss: 5.9848  loss_lm: 6.1931 (6.2453)  time: 2.5210  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 283/4722]  eta: 3:06:30  lr: 0.000010  loss: 5.9630  loss_lm: 6.0916 (6.1431)  time: 2.5210  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 284/4722]  eta: 3:02:25  lr: 0.000010  loss: 6.6714  loss_lm: 6.1690 (6.1990)  time: 2.4662  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 284/4722]  eta: 3:02:25  lr: 0.000010  loss: 5.8210  loss_lm: 6.0733 (6.1014)  time: 2.4663  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 284/4722]  eta: 3:02:24  lr: 0.000010  loss: 6.1794  loss_lm: 6.1579 (6.1438)  time: 2.4662  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 284/4722]  eta: 3:02:24  lr: 0.000010  loss: 5.7771  loss_lm: 6.0916 (6.1239)  time: 2.4661  data: 0.0000  max mem: 18867Train: data epoch: [0]  [ 284/4722]  eta: 3:02:25  lr: 0.000010  loss: 5.6967  loss_lm: 6.1652 (6.1803)  time: 2.4662  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 284/4722]  eta: 3:02:24  lr: 0.000010  loss: 6.6605  loss_lm: 6.2066 (6.2672)  time: 2.4662  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 285/4722]  eta: 3:17:51  lr: 0.000010  loss: 6.1056  loss_lm: 6.1613 (6.1766)  time: 2.6756  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 285/4722]  eta: 3:17:51  lr: 0.000010  loss: 5.8821  loss_lm: 6.0773 (6.1118)  time: 2.6755  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 285/4722]  eta: 3:17:51  lr: 0.000010  loss: 5.8357  loss_lm: 6.1565 (6.1284)  time: 2.6755  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 285/4722]  eta: 3:17:51  lr: 0.000010  loss: 6.0404  loss_lm: 6.1931 (6.2558)  time: 2.6755  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 285/4722]  eta: 3:17:51  lr: 0.000010  loss: 6.0062  loss_lm: 6.0628 (6.0966)  time: 2.6756  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 285/4722]  eta: 3:17:51  lr: 0.000010  loss: 6.5859  loss_lm: 6.1690 (6.2184)  time: 2.6756  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 286/4722]  eta: 3:13:35  lr: 0.000010  loss: 5.6851  loss_lm: 6.1660 (6.1930)  time: 2.5270  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 286/4722]  eta: 3:13:35  lr: 0.000010  loss: 6.3531  loss_lm: 6.0628 (6.1089)  time: 2.5271  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 286/4722]  eta: 3:13:35  lr: 0.000010  loss: 5.9563  loss_lm: 6.1056 (6.1661)  time: 2.5271  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 286/4722]  eta: 3:13:35  lr: 0.000010  loss: 5.9334  loss_lm: 6.1565 (6.1191)  time: 2.5270  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 286/4722]  eta: 3:13:35  lr: 0.000010  loss: 6.2949  loss_lm: 6.1931 (6.2577)  time: 2.5270  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 286/4722]  eta: 3:13:35  lr: 0.000010  loss: 6.1415  loss_lm: 6.0916 (6.1132)  time: 2.5270  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 287/4722]  eta: 3:09:44  lr: 0.000010  loss: 6.3171  loss_lm: 6.1660 (6.1986)  time: 2.5267  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 287/4722]  eta: 3:09:44  lr: 0.000010  loss: 5.5793  loss_lm: 6.0624 (6.0848)  time: 2.5267  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 287/4722]  eta: 3:09:43  lr: 0.000010  loss: 6.5051  loss_lm: 6.1565 (6.1366)  time: 2.5267  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 287/4722]  eta: 3:09:44  lr: 0.000010  loss: 6.1684  loss_lm: 6.1056 (6.1662)  time: 2.5267  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 287/4722]  eta: 3:09:43  lr: 0.000010  loss: 6.4207  loss_lm: 6.1931 (6.2651)  time: 2.5266  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 287/4722]  eta: 3:09:43  lr: 0.000010  loss: 6.0812  loss_lm: 6.0812 (6.1117)  time: 2.5266  data: 0.0000  max mem: 18867
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 288/4722]  eta: 3:06:11  lr: 0.000010  loss: 6.4871  loss_lm: 6.1660 (6.2112)  time: 2.5269  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 288/4722]  eta: 3:06:11  lr: 0.000010  loss: 6.0772  loss_lm: 6.0773 (6.1102)  time: 2.5269  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 288/4722]  eta: 3:06:11  lr: 0.000010  loss: 6.4901  loss_lm: 6.1579 (6.1520)  time: 2.5269  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 288/4722]  eta: 3:06:11  lr: 0.000010  loss: 6.2569  loss_lm: 6.0628 (6.0923)  time: 2.5269  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 288/4722]  eta: 3:06:11  lr: 0.000010  loss: 6.3321  loss_lm: 6.1056 (6.1734)  time: 2.5269  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 288/4722]  eta: 3:06:11  lr: 0.000010  loss: 5.9985  loss_lm: 6.1931 (6.2535)  time: 2.5269  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 289/4722]  eta: 3:02:56  lr: 0.000010  loss: 6.3052  loss_lm: 6.1690 (6.2151)  time: 2.5267  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 289/4722]  eta: 3:02:55  lr: 0.000010  loss: 6.2296  loss_lm: 6.0773 (6.1152)  time: 2.5267  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [ 289/4722]  eta: 3:02:56  lr: 0.000010  loss: 6.0576  loss_lm: 6.0628 (6.0908)  time: 2.5268  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 289/4722]  eta: 3:02:56  lr: 0.000010  loss: 6.6219  loss_lm: 6.1613 (6.1921)  time: 2.5267  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 289/4722]  eta: 3:02:55  lr: 0.000010  loss: 6.5619  loss_lm: 6.1579 (6.1691)  time: 2.5267  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 289/4722]  eta: 3:02:55  lr: 0.000010  loss: 6.0667  loss_lm: 6.1917 (6.2457)  time: 2.5267  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 01:10:14.889207] Starting job[2023-11-13 01:10:14.889209] Starting job

[2023-11-13 01:10:14.889513] Starting job
[2023-11-13 01:10:14.890944] Starting job
[2023-11-13 01:10:14.891163] Starting job
[2023-11-13 01:10:14.916717] Starting job
| distributed init (rank 10, world 12): env://
| distributed init (rank 11, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 8, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 9, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 286/4722]  eta: 5:01:54  lr: 0.000010  loss: 5.9432  loss_lm: 5.9432 (5.9432)  time: 4.0834  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 286/4722]  eta: 5:01:55  lr: 0.000010  loss: 5.7194  loss_lm: 5.7194 (5.7194)  time: 4.0836  data: 0.0000  max mem: 18863

Train: data epoch: [0]  [ 286/4722]  eta: 5:01:57  lr: 0.000010  loss: 5.9664  loss_lm: 5.9664 (5.9664)  time: 4.0843  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 286/4722]  eta: 5:02:00  lr: 0.000010  loss: 6.2995  loss_lm: 6.2995 (6.2995)  time: 4.0849  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 286/4722]  eta: 5:01:54  lr: 0.000010  loss: 6.3366  loss_lm: 6.3366 (6.3366)  time: 4.0836  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 286/4722]  eta: 5:02:07  lr: 0.000010  loss: 6.1343  loss_lm: 6.1343 (6.1343)  time: 4.0864  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 287/4722]  eta: 3:28:40  lr: 0.000010  loss: 6.5159  loss_lm: 5.9432 (6.2295)  time: 2.8230  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 287/4722]  eta: 3:28:43  lr: 0.000010  loss: 6.4351  loss_lm: 6.2995 (6.3673)  time: 2.8237  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 287/4722]  eta: 3:28:40  lr: 0.000010  loss: 6.3168  loss_lm: 5.7194 (6.0181)  time: 2.8231  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 287/4722]  eta: 3:28:41  lr: 0.000010  loss: 6.1613  loss_lm: 5.9664 (6.0638)  time: 2.8234  data: 0.0000  max mem: 18863Train: data epoch: [0]  [ 287/4722]  eta: 3:28:40  lr: 0.000010  loss: 5.5539  loss_lm: 5.5539 (5.9453)  time: 2.8230  data: 0.0000  max mem: 18863

Train: data epoch: [0]  [ 287/4722]  eta: 3:28:46  lr: 0.000010  loss: 6.0491  loss_lm: 6.0491 (6.0917)  time: 2.8244  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 288/4722]  eta: 2:56:40  lr: 0.000010  loss: 6.4738  loss_lm: 6.4738 (6.3110)  time: 2.3907  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 288/4722]  eta: 2:56:42  lr: 0.000010  loss: 5.9809  loss_lm: 6.2995 (6.2385)  time: 2.3912  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 288/4722]  eta: 2:56:40  lr: 0.000010  loss: 6.4407  loss_lm: 6.3168 (6.1589)  time: 2.3908  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 288/4722]  eta: 2:56:44  lr: 0.000010  loss: 6.0476  loss_lm: 6.0491 (6.0770)  time: 2.3916  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 288/4722]  eta: 2:56:41  lr: 0.000010  loss: 6.3851  loss_lm: 6.1613 (6.1709)  time: 2.3910  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 288/4722]  eta: 2:56:40  lr: 0.000010  loss: 6.2809  loss_lm: 6.2809 (6.0571)  time: 2.3907  data: 0.0000  max mem: 18863

Train: data epoch: [0]  [ 289/4722]  eta: 2:39:44  lr: 0.000010  loss: 6.5702  loss_lm: 6.4738 (6.3758)  time: 2.1622  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 289/4722]  eta: 2:39:46  lr: 0.000010  loss: 6.0753  loss_lm: 6.0753 (6.1977)  time: 2.1625  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 289/4722]  eta: 2:39:44  lr: 0.000010  loss: 6.2624  loss_lm: 6.2624 (6.1848)  time: 2.1622  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 289/4722]  eta: 2:39:44  lr: 0.000010  loss: 6.0544  loss_lm: 6.0544 (6.0565)  time: 2.1621  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 289/4722]  eta: 2:39:47  lr: 0.000010  loss: 6.2299  loss_lm: 6.0491 (6.1152)  time: 2.1628  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 289/4722]  eta: 2:39:45  lr: 0.000010  loss: 6.5884  loss_lm: 6.1613 (6.2753)  time: 2.1623  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 290/4722]  eta: 3:48:00  lr: 0.000010  loss: 6.4971  loss_lm: 6.3168 (6.2473)  time: 3.0868  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 290/4722]  eta: 3:48:00  lr: 0.000010  loss: 5.9092  loss_lm: 6.4738 (6.2825)  time: 3.0868  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 290/4722]  eta: 3:48:03  lr: 0.000010  loss: 6.1718  loss_lm: 6.1343 (6.1265)  time: 3.0873  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 290/4722]  eta: 3:48:00  lr: 0.000010  loss: 6.2016  loss_lm: 6.2016 (6.0855)  time: 3.0868  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 290/4722]  eta: 3:48:01  lr: 0.000010  loss: 6.0406  loss_lm: 6.1613 (6.2284)  time: 3.0869  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 290/4722]  eta: 3:48:02  lr: 0.000010  loss: 6.0851  loss_lm: 6.0851 (6.1752)  time: 3.0871  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 291/4722]  eta: 3:28:09  lr: 0.000010  loss: 6.4709  loss_lm: 6.4709 (6.3139)  time: 2.8187  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 291/4722]  eta: 3:28:10  lr: 0.000010  loss: 6.4319  loss_lm: 6.0851 (6.2180)  time: 2.8188  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 291/4722]  eta: 3:28:11  lr: 0.000010  loss: 6.1895  loss_lm: 6.1343 (6.1370)  time: 2.8191  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 291/4722]  eta: 3:28:09  lr: 0.000010  loss: 6.1119  loss_lm: 6.2624 (6.2247)  time: 2.8187  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 291/4722]  eta: 3:28:09  lr: 0.000010  loss: 6.0061  loss_lm: 6.0406 (6.1913)  time: 2.8187  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 291/4722]  eta: 3:28:09  lr: 0.000010  loss: 6.0178  loss_lm: 6.0544 (6.0742)  time: 2.8186  data: 0.0000  max mem: 18864

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 292/4722]  eta: 3:13:57  lr: 0.000010  loss: 5.9025  loss_lm: 6.4709 (6.2551)  time: 2.6271  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 292/4722]  eta: 3:13:58  lr: 0.000010  loss: 5.8067  loss_lm: 6.0851 (6.1592)  time: 2.6272  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 292/4722]  eta: 3:13:57  lr: 0.000010  loss: 6.2123  loss_lm: 6.2624 (6.2229)  time: 2.6271  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 292/4722]  eta: 3:13:58  lr: 0.000010  loss: 5.9606  loss_lm: 6.0406 (6.1584)  time: 2.6271  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 292/4722]  eta: 3:13:57  lr: 0.000010  loss: 6.3327  loss_lm: 6.2016 (6.1111)  time: 2.6270  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 292/4722]  eta: 3:13:59  lr: 0.000010  loss: 5.8353  loss_lm: 6.1343 (6.0939)  time: 2.6274  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 293/4722]  eta: 3:03:16  lr: 0.000010  loss: 6.1039  loss_lm: 6.1039 (6.2362)  time: 2.4828  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 293/4722]  eta: 3:03:17  lr: 0.000010  loss: 6.2039  loss_lm: 6.0851 (6.1648)  time: 2.4830  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 293/4722]  eta: 3:03:16  lr: 0.000010  loss: 5.8944  loss_lm: 6.2123 (6.1818)  time: 2.4828  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 293/4722]  eta: 3:03:17  lr: 0.000010  loss: 6.2242  loss_lm: 6.1343 (6.1102)  time: 2.4831  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 293/4722]  eta: 3:03:16  lr: 0.000010  loss: 5.8545  loss_lm: 6.0061 (6.1204)  time: 2.4829  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 293/4722]  eta: 3:03:16  lr: 0.000010  loss: 5.8687  loss_lm: 6.0544 (6.0808)  time: 2.4828  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 294/4722]  eta: 2:55:01  lr: 0.000010  loss: 6.0188  loss_lm: 6.1039 (6.2121)  time: 2.3716  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 294/4722]  eta: 2:55:01  lr: 0.000010  loss: 6.1865  loss_lm: 6.2123 (6.1824)  time: 2.3716  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 294/4722]  eta: 2:55:01  lr: 0.000010  loss: 6.3048  loss_lm: 6.2039 (6.1803)  time: 2.3717  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 294/4722]  eta: 2:55:01  lr: 0.000010  loss: 6.5908  loss_lm: 6.0406 (6.1726)  time: 2.3716  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 294/4722]  eta: 2:55:01  lr: 0.000010  loss: 5.9993  loss_lm: 6.0544 (6.0718)  time: 2.3715  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 294/4722]  eta: 2:55:02  lr: 0.000010  loss: 6.3539  loss_lm: 6.1718 (6.1373)  time: 2.3718  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 295/4722]  eta: 3:28:31  lr: 0.000010  loss: 5.9802  loss_lm: 6.1865 (6.1622)  time: 2.8262  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 295/4722]  eta: 3:28:31  lr: 0.000010  loss: 6.3041  loss_lm: 6.0406 (6.1858)  time: 2.8263  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 295/4722]  eta: 3:28:31  lr: 0.000010  loss: 5.9830  loss_lm: 6.0178 (6.0629)  time: 2.8262  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 295/4722]  eta: 3:28:32  lr: 0.000010  loss: 5.8533  loss_lm: 6.1343 (6.1089)  time: 2.8265  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 295/4722]  eta: 3:28:31  lr: 0.000010  loss: 6.1399  loss_lm: 6.1039 (6.2048)  time: 2.8263  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 295/4722]  eta: 3:28:32  lr: 0.000010  loss: 6.1956  loss_lm: 6.1956 (6.1819)  time: 2.8263  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 296/4722]  eta: 3:19:31  lr: 0.000010  loss: 5.7870  loss_lm: 6.1956 (6.1460)  time: 2.7047  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 296/4722]  eta: 3:19:30  lr: 0.000010  loss: 5.9231  loss_lm: 6.1865 (6.1404)  time: 2.7047  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 296/4722]  eta: 3:19:30  lr: 0.000010  loss: 6.0944  loss_lm: 6.1039 (6.1948)  time: 2.7047  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 296/4722]  eta: 3:19:31  lr: 0.000010  loss: 6.4139  loss_lm: 6.1718 (6.1366)  time: 2.7049  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 296/4722]  eta: 3:19:30  lr: 0.000010  loss: 5.9925  loss_lm: 6.0178 (6.0565)  time: 2.7046  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 296/4722]  eta: 3:19:30  lr: 0.000010  loss: 6.0952  loss_lm: 6.0952 (6.1775)  time: 2.7047  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 297/4722]  eta: 3:11:55  lr: 0.000010  loss: 6.5442  loss_lm: 6.1039 (6.2239)  time: 2.6024  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 297/4722]  eta: 3:11:55  lr: 0.000010  loss: 5.8656  loss_lm: 6.1119 (6.1175)  time: 2.6024  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 297/4722]  eta: 3:11:55  lr: 0.000010  loss: 5.7906  loss_lm: 6.0406 (6.1453)  time: 2.6024  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 297/4722]  eta: 3:11:55  lr: 0.000010  loss: 5.9981  loss_lm: 6.0851 (6.1336)  time: 2.6024  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 297/4722]  eta: 3:11:55  lr: 0.000010  loss: 6.3333  loss_lm: 6.0178 (6.0796)  time: 2.6023  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 297/4722]  eta: 3:11:56  lr: 0.000010  loss: 6.0526  loss_lm: 6.1343 (6.1296)  time: 2.6025  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 298/4722]  eta: 3:05:30  lr: 0.000010  loss: 6.0873  loss_lm: 6.0873 (6.1301)  time: 2.5160  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 298/4722]  eta: 3:05:30  lr: 0.000010  loss: 6.3228  loss_lm: 6.1865 (6.1333)  time: 2.5160  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 298/4722]  eta: 3:05:30  lr: 0.000010  loss: 6.2183  loss_lm: 6.1399 (6.2235)  time: 2.5160  data: 0.0000  max mem: 18962
Train: data epoch: [0]  [ 298/4722]  eta: 3:05:30  lr: 0.000010  loss: 6.2461  loss_lm: 6.0544 (6.0924)  time: 2.5159  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 298/4722]  eta: 3:05:31  lr: 0.000010  loss: 5.9309  loss_lm: 6.1343 (6.1143)  time: 2.5161  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 298/4722]  eta: 3:05:30  lr: 0.000010  loss: 5.9093  loss_lm: 6.0406 (6.1272)  time: 2.5160  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 299/4722]  eta: 3:00:01  lr: 0.000010  loss: 5.8231  loss_lm: 6.1119 (6.1112)  time: 2.4422  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 299/4722]  eta: 3:00:01  lr: 0.000010  loss: 6.0309  loss_lm: 6.0851 (6.1230)  time: 2.4422  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 299/4722]  eta: 3:00:01  lr: 0.000010  loss: 6.2375  loss_lm: 6.1399 (6.2245)  time: 2.4422  data: 0.0000  max mem: 18962
Train: data epoch: [0]  [ 299/4722]  eta: 3:00:01  lr: 0.000010  loss: 5.7823  loss_lm: 6.0061 (6.1025)  time: 2.4421  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 299/4722]  eta: 3:00:01  lr: 0.000010  loss: 5.6397  loss_lm: 6.0178 (6.0600)  time: 2.4421  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [ 299/4722]  eta: 3:00:02  lr: 0.000010  loss: 6.1399  loss_lm: 6.1343 (6.1162)  time: 2.4423  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 300/4722]  eta: 3:22:10  lr: 0.000010  loss: 5.9500  loss_lm: 6.1399 (6.2062)  time: 2.7433  data: 0.0000  max mem: 18962
Train: data epoch: [0]  [ 300/4722]  eta: 3:22:10  lr: 0.000010  loss: 5.3388  loss_lm: 6.1119 (6.0597)  time: 2.7433  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 300/4722]  eta: 3:22:11  lr: 0.000010  loss: 6.2940  loss_lm: 6.1399 (6.1280)  time: 2.7434  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 300/4722]  eta: 3:22:10  lr: 0.000010  loss: 5.9178  loss_lm: 6.0061 (6.0902)  time: 2.7433  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 300/4722]  eta: 3:22:10  lr: 0.000010  loss: 6.7050  loss_lm: 6.0544 (6.1030)  time: 2.7432  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [ 300/4722]  eta: 3:22:10  lr: 0.000010  loss: 6.7412  loss_lm: 6.0873 (6.1642)  time: 2.7433  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 301/4722]  eta: 3:16:22  lr: 0.000010  loss: 6.1036  loss_lm: 6.0873 (6.1604)  time: 2.6650  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 301/4722]  eta: 3:16:22  lr: 0.000010  loss: 6.2230  loss_lm: 6.1119 (6.0699)  time: 2.6650  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 301/4722]  eta: 3:16:22  lr: 0.000010  loss: 5.6386  loss_lm: 6.1039 (6.1707)  time: 2.6650  data: 0.0000  max mem: 18962
Train: data epoch: [0]  [ 301/4722]  eta: 3:16:21  lr: 0.000010  loss: 5.7649  loss_lm: 5.9664 (6.0699)  time: 2.6650  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 301/4722]  eta: 3:16:21  lr: 0.000010  loss: 6.1597  loss_lm: 6.0544 (6.1066)  time: 2.6649  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [ 301/4722]  eta: 3:16:22  lr: 0.000010  loss: 5.5069  loss_lm: 6.1343 (6.0892)  time: 2.6651  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 302/4722]  eta: 3:11:10  lr: 0.000010  loss: 5.7574  loss_lm: 6.0873 (6.1367)  time: 2.5952  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 302/4722]  eta: 3:11:10  lr: 0.000010  loss: 5.8299  loss_lm: 6.1039 (6.1507)  time: 2.5952  data: 0.0000  max mem: 18962
Train: data epoch: [0]  [ 302/4722]  eta: 3:11:10  lr: 0.000010  loss: 6.5400  loss_lm: 6.1865 (6.0975)  time: 2.5952  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 302/4722]  eta: 3:11:10  lr: 0.000010  loss: 5.6698  loss_lm: 5.9664 (6.0463)  time: 2.5951  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 302/4722]  eta: 3:11:10  lr: 0.000010  loss: 6.1736  loss_lm: 6.1597 (6.1105)  time: 2.5951  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 302/4722]  eta: 3:11:11  lr: 0.000010  loss: 5.8157  loss_lm: 6.1343 (6.0731)  time: 2.5953  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 303/4722]  eta: 3:06:34  lr: 0.000010  loss: 5.6305  loss_lm: 6.0944 (6.1218)  time: 2.5333  data: 0.0000  max mem: 18962
Train: data epoch: [0]  [ 303/4722]  eta: 3:06:34  lr: 0.000010  loss: 6.0580  loss_lm: 6.0851 (6.1323)  time: 2.5333  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 303/4722]  eta: 3:06:34  lr: 0.000010  loss: 6.0513  loss_lm: 5.9664 (6.0466)  time: 2.5332  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 303/4722]  eta: 3:06:34  lr: 0.000010  loss: 5.4544  loss_lm: 6.0544 (6.0741)  time: 2.5332  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 303/4722]  eta: 3:06:34  lr: 0.000010  loss: 5.8213  loss_lm: 6.1119 (6.0822)  time: 2.5333  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 303/4722]  eta: 3:06:34  lr: 0.000010  loss: 6.1925  loss_lm: 6.1343 (6.0797)  time: 2.5333  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 304/4722]  eta: 3:02:26  lr: 0.000010  loss: 5.9954  loss_lm: 6.0944 (6.1151)  time: 2.4776  data: 0.0000  max mem: 18962
Train: data epoch: [0]  [ 304/4722]  eta: 3:02:26  lr: 0.000010  loss: 6.3524  loss_lm: 6.0873 (6.1439)  time: 2.4776  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 304/4722]  eta: 3:02:25  lr: 0.000010  loss: 5.7795  loss_lm: 6.1119 (6.0663)  time: 2.4776  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 304/4722]  eta: 3:02:25  lr: 0.000010  loss: 6.0411  loss_lm: 6.0061 (6.0463)  time: 2.4776  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 304/4722]  eta: 3:02:26  lr: 0.000010  loss: 6.3047  loss_lm: 6.1399 (6.0916)  time: 2.4777  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 304/4722]  eta: 3:02:25  lr: 0.000010  loss: 6.4405  loss_lm: 6.1597 (6.0934)  time: 2.4775  data: 0.0000  max mem: 19014

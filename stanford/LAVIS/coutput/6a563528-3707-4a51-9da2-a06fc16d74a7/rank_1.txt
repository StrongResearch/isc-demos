WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-12 23:42:56.314734] Starting job
[2023-11-12 23:42:56.314742] Starting job
[2023-11-12 23:42:56.314797] Starting job
[2023-11-12 23:42:56.314807] Starting job
[2023-11-12 23:42:56.314878] Starting job
[2023-11-12 23:42:56.314947] Starting job
| distributed init (rank 10, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 11, world 12): env://
| distributed init (rank 8, world 12): env://
| distributed init (rank 9, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   1/4722]  eta: 6:02:03  lr: 0.000010  loss: 10.4597  loss_lm: 10.4597 (10.4597)  time: 4.6015  data: 0.0000  max mem: 15461Train: data epoch: [0]  [   1/4722]  eta: 6:02:04  lr: 0.000010  loss: 10.4274  loss_lm: 10.4274 (10.4274)  time: 4.6016  data: 0.0000  max mem: 15459

Train: data epoch: [0]  [   1/4722]  eta: 6:02:06  lr: 0.000010  loss: 10.4788  loss_lm: 10.4788 (10.4788)  time: 4.6021  data: 0.0000  max mem: 15465
Train: data epoch: [0]  [   1/4722]  eta: 6:02:04  lr: 0.000010  loss: 10.4390  loss_lm: 10.4390 (10.4390)  time: 4.6016  data: 0.0000  max mem: 15461
Train: data epoch: [0]  [   1/4722]  eta: 6:02:06  lr: 0.000010  loss: 10.3325  loss_lm: 10.3325 (10.3325)  time: 4.6021  data: 0.0000  max mem: 15459Train: data epoch: [0]  [   1/4722]  eta: 6:02:04  lr: 0.000010  loss: 10.3660  loss_lm: 10.3660 (10.3660)  time: 4.6016  data: 0.0000  max mem: 15461

Train: data epoch: [0]  [   2/4722]  eta: 3:59:03  lr: 0.000010  loss: 9.9602  loss_lm: 9.9602 (10.1463)  time: 3.0389  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [   2/4722]  eta: 3:59:02  lr: 0.000010  loss: 9.9057  loss_lm: 9.9057 (10.1827)  time: 3.0386  data: 0.0000  max mem: 18857
Train: data epoch: [0]  [   2/4722]  eta: 3:59:02  lr: 0.000010  loss: 9.7747  loss_lm: 9.7747 (10.0704)  time: 3.0386  data: 0.0000  max mem: 18858
Train: data epoch: [0]  [   2/4722]  eta: 3:59:03  lr: 0.000010  loss: 9.9085  loss_lm: 9.9085 (10.1937)  time: 3.0388  data: 0.0000  max mem: 18858
Train: data epoch: [0]  [   2/4722]  eta: 3:59:02  lr: 0.000010  loss: 9.8662  loss_lm: 9.8662 (10.1526)  time: 3.0386  data: 0.0000  max mem: 18857
Train: data epoch: [0]  [   2/4722]  eta: 3:59:02  lr: 0.000010  loss: 9.8655  loss_lm: 9.8655 (10.1464)  time: 3.0386  data: 0.0000  max mem: 18867
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   3/4722]  eta: 3:17:38  lr: 0.000010  loss: 9.5976  loss_lm: 9.9602 (9.9634)  time: 2.5129  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [   3/4722]  eta: 3:17:37  lr: 0.000010  loss: 9.4722  loss_lm: 9.8655 (9.9217)  time: 2.5127  data: 0.0000  max mem: 18872
Train: data epoch: [0]  [   3/4722]  eta: 3:17:37  lr: 0.000010  loss: 9.4635  loss_lm: 9.7747 (9.8681)  time: 2.5127  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [   3/4722]  eta: 3:17:37  lr: 0.000010  loss: 9.5316  loss_lm: 9.8662 (9.9456)  time: 2.5127  data: 0.0000  max mem: 18864Train: data epoch: [0]  [   3/4722]  eta: 3:17:38  lr: 0.000010  loss: 9.3681  loss_lm: 9.9085 (9.9185)  time: 2.5129  data: 0.0000  max mem: 18858

Train: data epoch: [0]  [   3/4722]  eta: 3:17:37  lr: 0.000010  loss: 9.6146  loss_lm: 9.9057 (9.9934)  time: 2.5127  data: 0.0000  max mem: 18862
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   4/4722]  eta: 2:56:55  lr: 0.000010  loss: 9.3008  loss_lm: 9.5316 (9.7844)  time: 2.2500  data: 0.0000  max mem: 18865Train: data epoch: [0]  [   4/4722]  eta: 2:56:55  lr: 0.000010  loss: 9.4527  loss_lm: 9.4722 (9.8044)  time: 2.2500  data: 0.0000  max mem: 18872Train: data epoch: [0]  [   4/4722]  eta: 2:56:56  lr: 0.000010  loss: 9.6874  loss_lm: 9.6874 (9.8944)  time: 2.2501  data: 0.0000  max mem: 19007


Train: data epoch: [0]  [   4/4722]  eta: 2:56:55  lr: 0.000010  loss: 9.3180  loss_lm: 9.6146 (9.8245)  time: 2.2500  data: 0.0000  max mem: 18862
Train: data epoch: [0]  [   4/4722]  eta: 2:56:55  lr: 0.000010  loss: 9.2039  loss_lm: 9.4635 (9.7021)  time: 2.2500  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [   4/4722]  eta: 2:56:56  lr: 0.000010  loss: 9.3426  loss_lm: 9.3681 (9.7745)  time: 2.2501  data: 0.0000  max mem: 18859
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   5/4722]  eta: 3:58:54  lr: 0.000010  loss: 9.1134  loss_lm: 9.3681 (9.6423)  time: 3.0389  data: 0.0000  max mem: 18859
Train: data epoch: [0]  [   5/4722]  eta: 3:58:54  lr: 0.000010  loss: 8.9510  loss_lm: 9.6874 (9.7057)  time: 3.0390  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [   5/4722]  eta: 3:58:54  lr: 0.000010  loss: 9.3431  loss_lm: 9.6146 (9.7282)  time: 3.0388  data: 0.0000  max mem: 18862
Train: data epoch: [0]  [   5/4722]  eta: 3:58:54  lr: 0.000010  loss: 9.0994  loss_lm: 9.4722 (9.6634)  time: 3.0389  data: 0.0000  max mem: 18872
Train: data epoch: [0]  [   5/4722]  eta: 3:58:54  lr: 0.000010  loss: 9.3177  loss_lm: 9.4635 (9.6252)  time: 3.0389  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [   5/4722]  eta: 3:58:54  lr: 0.000010  loss: 9.1783  loss_lm: 9.5316 (9.6632)  time: 3.0389  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [   6/4722]  eta: 3:38:27  lr: 0.000010  loss: 9.1402  loss_lm: 9.3008 (9.5760)  time: 2.7793  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [   6/4722]  eta: 3:38:27  lr: 0.000010  loss: 9.3487  loss_lm: 9.3487 (9.5934)  time: 2.7794  data: 0.0000  max mem: 18859
Train: data epoch: [0]  [   6/4722]  eta: 3:38:27  lr: 0.000010  loss: 9.2979  loss_lm: 9.3177 (9.5706)  time: 2.7793  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [   6/4722]  eta: 3:38:26  lr: 0.000010  loss: 9.1222  loss_lm: 9.3431 (9.6272)  time: 2.7793  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [   6/4722]  eta: 3:38:27  lr: 0.000010  loss: 8.9965  loss_lm: 9.4527 (9.5523)  time: 2.7793  data: 0.0000  max mem: 18872Train: data epoch: [0]  [   6/4722]  eta: 3:38:27  lr: 0.000010  loss: 8.8845  loss_lm: 9.5976 (9.5689)  time: 2.7794  data: 0.0000  max mem: 19007

Train: data epoch: [0]  [   7/4722]  eta: 3:23:34  lr: 0.000010  loss: 8.8993  loss_lm: 9.3008 (9.4793)  time: 2.5906  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [   7/4722]  eta: 3:23:35  lr: 0.000010  loss: 9.0469  loss_lm: 9.3487 (9.5153)  time: 2.5907  data: 0.0000  max mem: 18859
Train: data epoch: [0]  [   7/4722]  eta: 3:23:34  lr: 0.000010  loss: 8.8834  loss_lm: 9.3177 (9.4725)  time: 2.5906  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [   7/4722]  eta: 3:23:35  lr: 0.000010  loss: 9.0148  loss_lm: 9.5976 (9.4897)  time: 2.5907  data: 0.0000  max mem: 19007Train: data epoch: [0]  [   7/4722]  eta: 3:23:34  lr: 0.000010  loss: 8.6923  loss_lm: 9.4527 (9.4294)  time: 2.5906  data: 0.0000  max mem: 18872

Train: data epoch: [0]  [   7/4722]  eta: 3:23:34  lr: 0.000010  loss: 9.0939  loss_lm: 9.3431 (9.5510)  time: 2.5906  data: 0.0000  max mem: 18934
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   8/4722]  eta: 3:12:26  lr: 0.000010  loss: 8.7514  loss_lm: 9.1783 (9.3884)  time: 2.4493  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [   8/4722]  eta: 3:12:26  lr: 0.000010  loss: 8.8279  loss_lm: 9.3426 (9.4294)  time: 2.4494  data: 0.0000  max mem: 18859
Train: data epoch: [0]  [   8/4722]  eta: 3:12:25  lr: 0.000010  loss: 8.8386  loss_lm: 9.0994 (9.3556)  time: 2.4493  data: 0.0000  max mem: 18872Train: data epoch: [0]  [   8/4722]  eta: 3:12:26  lr: 0.000010  loss: 9.0513  loss_lm: 9.0513 (9.4349)  time: 2.4494  data: 0.0000  max mem: 19007

Train: data epoch: [0]  [   8/4722]  eta: 3:12:25  lr: 0.000010  loss: 8.7468  loss_lm: 9.3180 (9.4505)  time: 2.4493  data: 0.0000  max mem: 18934
Train: data epoch: [0]  [   8/4722]  eta: 3:12:25  lr: 0.000010  loss: 8.9808  loss_lm: 9.2979 (9.4110)  time: 2.4493  data: 0.0000  max mem: 18908
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   9/4722]  eta: 3:03:47  lr: 0.000010  loss: 8.9408  loss_lm: 9.3180 (9.3939)  time: 2.3397  data: 0.0000  max mem: 18934
Train: data epoch: [0]  [   9/4722]  eta: 3:03:47  lr: 0.000010  loss: 9.0042  loss_lm: 9.0513 (9.3871)  time: 2.3398  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [   9/4722]  eta: 3:03:47  lr: 0.000010  loss: 9.0216  loss_lm: 9.0994 (9.3185)  time: 2.3397  data: 0.0000  max mem: 18872
Train: data epoch: [0]  [   9/4722]  eta: 3:03:47  lr: 0.000010  loss: 8.6588  loss_lm: 9.2979 (9.3274)  time: 2.3397  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [   9/4722]  eta: 3:03:47  lr: 0.000010  loss: 8.5788  loss_lm: 9.1783 (9.2984)  time: 2.3398  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [   9/4722]  eta: 3:03:47  lr: 0.000010  loss: 8.9085  loss_lm: 9.3426 (9.3715)  time: 2.3398  data: 0.0000  max mem: 18859
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  10/4722]  eta: 3:38:58  lr: 0.000010  loss: 8.9283  loss_lm: 9.1134 (9.3272)  time: 2.7883  data: 0.0000  max mem: 18859
Train: data epoch: [0]  [  10/4722]  eta: 3:38:57  lr: 0.000010  loss: 8.7799  loss_lm: 9.1222 (9.3325)  time: 2.7882  data: 0.0000  max mem: 18934
Train: data epoch: [0]  [  10/4722]  eta: 3:38:57  lr: 0.000010  loss: 9.0546  loss_lm: 9.2039 (9.3001)  time: 2.7882  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [  10/4722]  eta: 3:38:58  lr: 0.000010  loss: 8.5831  loss_lm: 9.0148 (9.3067)  time: 2.7883  data: 0.0000  max mem: 19007Train: data epoch: [0]  [  10/4722]  eta: 3:38:57  lr: 0.000010  loss: 9.2681  loss_lm: 9.0994 (9.3134)  time: 2.7882  data: 0.0000  max mem: 18874

Train: data epoch: [0]  [  10/4722]  eta: 3:38:58  lr: 0.000010  loss: 8.7692  loss_lm: 9.1402 (9.2455)  time: 2.7882  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  11/4722]  eta: 3:29:27  lr: 0.000010  loss: 8.7954  loss_lm: 9.1134 (9.2788)  time: 2.6677  data: 0.0000  max mem: 18860Train: data epoch: [0]  [  11/4722]  eta: 3:29:27  lr: 0.000010  loss: 9.0921  loss_lm: 9.1402 (9.2315)  time: 2.6676  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  11/4722]  eta: 3:29:27  lr: 0.000010  loss: 8.6822  loss_lm: 9.0994 (9.2560)  time: 2.6676  data: 0.0000  max mem: 18874
Train: data epoch: [0]  [  11/4722]  eta: 3:29:27  lr: 0.000010  loss: 9.0535  loss_lm: 9.0513 (9.2836)  time: 2.6677  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  11/4722]  eta: 3:29:27  lr: 0.000010  loss: 8.4385  loss_lm: 9.1222 (9.2512)  time: 2.6676  data: 0.0000  max mem: 18934Train: data epoch: [0]  [  11/4722]  eta: 3:29:27  lr: 0.000010  loss: 8.6201  loss_lm: 9.2039 (9.2383)  time: 2.6676  data: 0.0000  max mem: 18908

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  12/4722]  eta: 3:21:30  lr: 0.000010  loss: 8.7270  loss_lm: 9.0921 (9.1895)  time: 2.5670  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  12/4722]  eta: 3:21:30  lr: 0.000010  loss: 8.6563  loss_lm: 9.0469 (9.2270)  time: 2.5670  data: 0.0000  max mem: 18860
Train: data epoch: [0]  [  12/4722]  eta: 3:21:30  lr: 0.000010  loss: 8.7841  loss_lm: 9.0216 (9.2167)  time: 2.5670  data: 0.0000  max mem: 18874
Train: data epoch: [0]  [  12/4722]  eta: 3:21:30  lr: 0.000010  loss: 8.7700  loss_lm: 9.0148 (9.2408)  time: 2.5670  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  12/4722]  eta: 3:21:30  lr: 0.000010  loss: 8.9561  loss_lm: 9.0939 (9.2266)  time: 2.5670  data: 0.0000  max mem: 18934
Train: data epoch: [0]  [  12/4722]  eta: 3:21:30  lr: 0.000010  loss: 8.8831  loss_lm: 9.0546 (9.2087)  time: 2.5670  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [  13/4722]  eta: 3:14:47  lr: 0.000010  loss: 8.4882  loss_lm: 9.0921 (9.1356)  time: 2.4819  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  13/4722]  eta: 3:14:47  lr: 0.000010  loss: 8.7303  loss_lm: 9.0469 (9.1887)  time: 2.4820  data: 0.0000  max mem: 18860

Train: data epoch: [0]  [  13/4722]  eta: 3:14:47  lr: 0.000010  loss: 8.5267  loss_lm: 9.0148 (9.1859)  time: 2.4819  data: 0.0000  max mem: 19007Train: data epoch: [0]  [  13/4722]  eta: 3:14:47  lr: 0.000010  loss: 8.4507  loss_lm: 9.0216 (9.1578)  time: 2.4819  data: 0.0000  max mem: 18874

Train: data epoch: [0]  [  13/4722]  eta: 3:14:47  lr: 0.000010  loss: 8.6608  loss_lm: 9.0939 (9.1831)  time: 2.4819  data: 0.0000  max mem: 18934Train: data epoch: [0]  [  13/4722]  eta: 3:14:47  lr: 0.000010  loss: 8.7532  loss_lm: 9.0546 (9.1737)  time: 2.4819  data: 0.0000  max mem: 18908

Train: data epoch: [0]  [  14/4722]  eta: 3:09:00  lr: 0.000010  loss: 8.9714  loss_lm: 8.9714 (9.1238)  time: 2.4087  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  14/4722]  eta: 3:09:00  lr: 0.000010  loss: 8.5649  loss_lm: 8.9283 (9.1442)  time: 2.4088  data: 0.0000  max mem: 18860
Train: data epoch: [0]  [  14/4722]  eta: 3:09:00  lr: 0.000010  loss: 9.0341  loss_lm: 9.0148 (9.1751)  time: 2.4087  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  14/4722]  eta: 3:09:00  lr: 0.000010  loss: 8.5242  loss_lm: 8.9965 (9.1125)  time: 2.4087  data: 0.0000  max mem: 18874
Train: data epoch: [0]  [  14/4722]  eta: 3:09:00  lr: 0.000010  loss: 8.7904  loss_lm: 8.9808 (9.1463)  time: 2.4087  data: 0.0000  max mem: 18908Train: data epoch: [0]  [  14/4722]  eta: 3:09:00  lr: 0.000010  loss: 8.8934  loss_lm: 8.9561 (9.1624)  time: 2.4087  data: 0.0000  max mem: 18934

Train: data epoch: [0]  [  15/4722]  eta: 3:31:50  lr: 0.000010  loss: 8.4293  loss_lm: 8.9283 (9.0965)  time: 2.7004  data: 0.0000  max mem: 18860
Train: data epoch: [0]  [  15/4722]  eta: 3:31:50  lr: 0.000010  loss: 8.6138  loss_lm: 8.9808 (9.1108)  time: 2.7003  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [  15/4722]  eta: 3:31:50  lr: 0.000010  loss: 8.5847  loss_lm: 8.9561 (9.1239)  time: 2.7003  data: 0.0000  max mem: 18934
Train: data epoch: [0]  [  15/4722]  eta: 3:31:50  lr: 0.000010  loss: 8.4144  loss_lm: 8.9965 (9.0660)  time: 2.7003  data: 0.0000  max mem: 18874Train: data epoch: [0]  [  15/4722]  eta: 3:31:50  lr: 0.000010  loss: 8.4396  loss_lm: 9.0148 (9.1260)  time: 2.7003  data: 0.0000  max mem: 19007

Train: data epoch: [0]  [  15/4722]  eta: 3:31:50  lr: 0.000010  loss: 8.6582  loss_lm: 8.9714 (9.0928)  time: 2.7003  data: 0.0000  max mem: 18907
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  16/4722]  eta: 3:25:42  lr: 0.000010  loss: 9.1738  loss_lm: 8.9283 (9.1014)  time: 2.6228  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  16/4722]  eta: 3:25:42  lr: 0.000010  loss: 9.0084  loss_lm: 8.9714 (9.0875)  time: 2.6227  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  16/4722]  eta: 3:25:42  lr: 0.000010  loss: 8.6135  loss_lm: 8.8386 (9.0377)  time: 2.6227  data: 0.0000  max mem: 18874
Train: data epoch: [0]  [  16/4722]  eta: 3:25:42  lr: 0.000010  loss: 8.4999  loss_lm: 9.0042 (9.0869)  time: 2.6227  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  16/4722]  eta: 3:25:42  lr: 0.000010  loss: 8.6930  loss_lm: 8.9408 (9.0969)  time: 2.6227  data: 0.0000  max mem: 18934Train: data epoch: [0]  [  16/4722]  eta: 3:25:42  lr: 0.000010  loss: 8.6089  loss_lm: 8.8834 (9.0794)  time: 2.6227  data: 0.0000  max mem: 18908

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  17/4722]  eta: 3:20:19  lr: 0.000010  loss: 8.4062  loss_lm: 8.9283 (9.0605)  time: 2.5546  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  17/4722]  eta: 3:20:19  lr: 0.000010  loss: 8.7046  loss_lm: 8.9714 (9.0650)  time: 2.5545  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  17/4722]  eta: 3:20:18  lr: 0.000010  loss: 8.5312  loss_lm: 8.9408 (9.0637)  time: 2.5545  data: 0.0000  max mem: 19011
Train: data epoch: [0]  [  17/4722]  eta: 3:20:18  lr: 0.000010  loss: 8.7235  loss_lm: 8.8386 (9.0192)  time: 2.5545  data: 0.0000  max mem: 18874Train: data epoch: [0]  [  17/4722]  eta: 3:20:19  lr: 0.000010  loss: 8.2927  loss_lm: 9.0042 (9.0402)  time: 2.5545  data: 0.0000  max mem: 19007

Train: data epoch: [0]  [  17/4722]  eta: 3:20:18  lr: 0.000010  loss: 8.4573  loss_lm: 8.8834 (9.0428)  time: 2.5545  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [  18/4722]  eta: 3:15:29  lr: 0.000010  loss: 8.3649  loss_lm: 8.9085 (9.0218)  time: 2.4936  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  18/4722]  eta: 3:15:29  lr: 0.000010  loss: 8.5641  loss_lm: 8.8993 (9.0372)  time: 2.4935  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  18/4722]  eta: 3:15:29  lr: 0.000010  loss: 8.5657  loss_lm: 8.9510 (9.0138)  time: 2.4935  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  18/4722]  eta: 3:15:29  lr: 0.000010  loss: 8.6710  loss_lm: 8.8934 (9.0419)  time: 2.4935  data: 0.0000  max mem: 19011Train: data epoch: [0]  [  18/4722]  eta: 3:15:29  lr: 0.000010  loss: 8.7879  loss_lm: 8.8831 (9.0287)  time: 2.4935  data: 0.0000  max mem: 18908

Train: data epoch: [0]  [  18/4722]  eta: 3:15:29  lr: 0.000010  loss: 8.2618  loss_lm: 8.7841 (8.9771)  time: 2.4935  data: 0.0000  max mem: 18874
Train: data epoch: [0]  [  19/4722]  eta: 3:11:10  lr: 0.000010  loss: 8.7571  loss_lm: 8.8993 (9.0224)  time: 2.4389  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  19/4722]  eta: 3:11:10  lr: 0.000010  loss: 8.7634  loss_lm: 8.7841 (8.9659)  time: 2.4389  data: 0.0000  max mem: 19009
Train: data epoch: [0]  [  19/4722]  eta: 3:11:10  lr: 0.000010  loss: 8.4556  loss_lm: 8.9085 (8.9920)  time: 2.4390  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  19/4722]  eta: 3:11:10  lr: 0.000010  loss: 8.7132  loss_lm: 8.9510 (8.9980)  time: 2.4389  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  19/4722]  eta: 3:11:10  lr: 0.000010  loss: 8.7892  loss_lm: 8.8934 (9.0286)  time: 2.4389  data: 0.0000  max mem: 19011Train: data epoch: [0]  [  19/4722]  eta: 3:11:10  lr: 0.000010  loss: 8.6026  loss_lm: 8.8831 (9.0062)  time: 2.4389  data: 0.0000  max mem: 18908

Train: data epoch: [0]  [  20/4722]  eta: 3:28:17  lr: 0.000010  loss: 8.8607  loss_lm: 8.8607 (8.9990)  time: 2.6580  data: 0.0000  max mem: 18908Train: data epoch: [0]  [  20/4722]  eta: 3:28:18  lr: 0.000010  loss: 8.3327  loss_lm: 8.8279 (8.9591)  time: 2.6581  data: 0.0000  max mem: 18861

Train: data epoch: [0]  [  20/4722]  eta: 3:28:17  lr: 0.000010  loss: 8.6283  loss_lm: 8.7892 (9.0085)  time: 2.6580  data: 0.0000  max mem: 19011
Train: data epoch: [0]  [  20/4722]  eta: 3:28:17  lr: 0.000010  loss: 8.3279  loss_lm: 8.8845 (8.9645)  time: 2.6580  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  20/4722]  eta: 3:28:17  lr: 0.000010  loss: 8.1487  loss_lm: 8.7634 (8.9250)  time: 2.6580  data: 0.0000  max mem: 19009
Train: data epoch: [0]  [  20/4722]  eta: 3:28:17  lr: 0.000010  loss: 8.7579  loss_lm: 8.7692 (9.0092)  time: 2.6580  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  21/4722]  eta: 3:23:47  lr: 0.000010  loss: 8.4372  loss_lm: 8.7579 (8.9820)  time: 2.5011  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  21/4722]  eta: 3:23:48  lr: 0.000010  loss: 8.8376  loss_lm: 8.8279 (8.9533)  time: 2.5011  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  21/4722]  eta: 3:23:47  lr: 0.000010  loss: 8.5751  loss_lm: 8.7700 (8.9460)  time: 2.5010  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  21/4722]  eta: 3:23:47  lr: 0.000010  loss: 8.6148  loss_lm: 8.7799 (8.9898)  time: 2.5011  data: 0.0000  max mem: 19011Train: data epoch: [0]  [  21/4722]  eta: 3:23:47  lr: 0.000010  loss: 8.4797  loss_lm: 8.7904 (8.9742)  time: 2.5011  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [  21/4722]  eta: 3:23:47  lr: 0.000010  loss: 8.3517  loss_lm: 8.7235 (8.8977)  time: 2.5011  data: 0.0000  max mem: 19009

Train: data epoch: [0]  [  22/4722]  eta: 3:19:46  lr: 0.000010  loss: 8.2620  loss_lm: 8.7571 (8.9492)  time: 2.5015  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  22/4722]  eta: 3:19:46  lr: 0.000010  loss: 8.4032  loss_lm: 8.6923 (8.8753)  time: 2.5015  data: 0.0000  max mem: 19009
Train: data epoch: [0]  [  22/4722]  eta: 3:19:46  lr: 0.000010  loss: 8.5788  loss_lm: 8.7132 (8.9293)  time: 2.5015  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  22/4722]  eta: 3:19:46  lr: 0.000010  loss: 8.6861  loss_lm: 8.7954 (8.9411)  time: 2.5016  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  22/4722]  eta: 3:19:46  lr: 0.000010  loss: 8.6148  loss_lm: 8.7468 (8.9728)  time: 2.5015  data: 0.0000  max mem: 19011Train: data epoch: [0]  [  22/4722]  eta: 3:19:46  lr: 0.000010  loss: 8.3007  loss_lm: 8.7879 (8.9436)  time: 2.5015  data: 0.0000  max mem: 18908

Train: data epoch: [0]  [  23/4722]  eta: 3:16:01  lr: 0.000010  loss: 8.5609  loss_lm: 8.7514 (8.9323)  time: 2.5014  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  23/4722]  eta: 3:16:00  lr: 0.000010  loss: 8.7427  loss_lm: 8.6923 (8.8695)  time: 2.5014  data: 0.0000  max mem: 19009
Train: data epoch: [0]  [  23/4722]  eta: 3:16:00  lr: 0.000010  loss: 8.5116  loss_lm: 8.7532 (8.9248)  time: 2.5014  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [  23/4722]  eta: 3:16:01  lr: 0.000010  loss: 8.7553  loss_lm: 8.7132 (8.9217)  time: 2.5014  data: 0.0000  max mem: 19007Train: data epoch: [0]  [  23/4722]  eta: 3:16:00  lr: 0.000010  loss: 8.5493  loss_lm: 8.6930 (8.9543)  time: 2.5014  data: 0.0000  max mem: 19011

Train: data epoch: [0]  [  23/4722]  eta: 3:16:01  lr: 0.000010  loss: 8.5882  loss_lm: 8.7303 (8.9258)  time: 2.5014  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  24/4722]  eta: 3:12:33  lr: 0.000010  loss: 8.4656  loss_lm: 8.7270 (8.9129)  time: 2.5011  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  24/4722]  eta: 3:12:33  lr: 0.000010  loss: 8.6914  loss_lm: 8.6914 (8.8621)  time: 2.5011  data: 0.0000  max mem: 19009
Train: data epoch: [0]  [  24/4722]  eta: 3:12:33  lr: 0.000010  loss: 8.3280  loss_lm: 8.6861 (8.9009)  time: 2.5011  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  24/4722]  eta: 3:12:33  lr: 0.000010  loss: 8.6927  loss_lm: 8.6927 (8.9122)  time: 2.5011  data: 0.0000  max mem: 19007Train: data epoch: [0]  [  24/4722]  eta: 3:12:33  lr: 0.000010  loss: 8.2318  loss_lm: 8.6588 (8.8960)  time: 2.5011  data: 0.0000  max mem: 18908Train: data epoch: [0]  [  24/4722]  eta: 3:12:33  lr: 0.000010  loss: 8.5464  loss_lm: 8.6710 (8.9373)  time: 2.5011  data: 0.0000  max mem: 19011


Train: data epoch: [0]  [  25/4722]  eta: 3:25:54  lr: 0.000010  loss: 8.6913  loss_lm: 8.6710 (8.9275)  time: 2.5282  data: 0.0000  max mem: 19011
Train: data epoch: [0]  [  25/4722]  eta: 3:25:54  lr: 0.000010  loss: 8.4474  loss_lm: 8.6563 (8.8827)  time: 2.5282  data: 0.0000  max mem: 18861Train: data epoch: [0]  [  25/4722]  eta: 3:25:54  lr: 0.000010  loss: 8.6049  loss_lm: 8.6201 (8.8843)  time: 2.5282  data: 0.0000  max mem: 18908

Train: data epoch: [0]  [  25/4722]  eta: 3:25:54  lr: 0.000010  loss: 8.1536  loss_lm: 8.5831 (8.8818)  time: 2.5281  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  25/4722]  eta: 3:25:54  lr: 0.000010  loss: 8.6915  loss_lm: 8.6914 (8.8552)  time: 2.5282  data: 0.0000  max mem: 19009
Train: data epoch: [0]  [  25/4722]  eta: 3:25:54  lr: 0.000010  loss: 8.4824  loss_lm: 8.7046 (8.8957)  time: 2.5282  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  26/4722]  eta: 3:22:19  lr: 0.000010  loss: 8.6217  loss_lm: 8.6582 (8.8851)  time: 2.5269  data: 0.0000  max mem: 18907Train: data epoch: [0]  [  26/4722]  eta: 3:22:19  lr: 0.000010  loss: 8.4500  loss_lm: 8.6822 (8.8397)  time: 2.5269  data: 0.0000  max mem: 19009

Train: data epoch: [0]  [  26/4722]  eta: 3:22:19  lr: 0.000010  loss: 8.6293  loss_lm: 8.5831 (8.8721)  time: 2.5269  data: 0.0000  max mem: 19007Train: data epoch: [0]  [  26/4722]  eta: 3:22:19  lr: 0.000010  loss: 8.6597  loss_lm: 8.6608 (8.9172)  time: 2.5269  data: 0.0000  max mem: 19011Train: data epoch: [0]  [  26/4722]  eta: 3:22:19  lr: 0.000010  loss: 8.1945  loss_lm: 8.6138 (8.8578)  time: 2.5269  data: 0.0000  max mem: 18908


Train: data epoch: [0]  [  26/4722]  eta: 3:22:20  lr: 0.000010  loss: 8.5364  loss_lm: 8.5882 (8.8694)  time: 2.5269  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  27/4722]  eta: 3:19:01  lr: 0.000010  loss: 8.7492  loss_lm: 8.6582 (8.8801)  time: 2.5268  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  27/4722]  eta: 3:19:00  lr: 0.000010  loss: 8.5029  loss_lm: 8.6135 (8.8272)  time: 2.5268  data: 0.0000  max mem: 19009
Train: data epoch: [0]  [  27/4722]  eta: 3:19:01  lr: 0.000010  loss: 8.4577  loss_lm: 8.5649 (8.8542)  time: 2.5268  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [  27/4722]  eta: 3:19:00  lr: 0.000010  loss: 8.5528  loss_lm: 8.5788 (8.8603)  time: 2.5267  data: 0.0000  max mem: 19007
Train: data epoch: [0]  [  27/4722]  eta: 3:19:00  lr: 0.000010  loss: 8.3976  loss_lm: 8.6597 (8.8980)  time: 2.5268  data: 0.0000  max mem: 19011
Train: data epoch: [0]  [  27/4722]  eta: 3:19:00  lr: 0.000010  loss: 8.2503  loss_lm: 8.6089 (8.8353)  time: 2.5268  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [  28/4722]  eta: 3:15:56  lr: 0.000010  loss: 8.4555  loss_lm: 8.5242 (8.8139)  time: 2.5266  data: 0.0000  max mem: 19009
Train: data epoch: [0]  [  28/4722]  eta: 3:15:56  lr: 0.000010  loss: 8.4468  loss_lm: 8.6217 (8.8646)  time: 2.5266  data: 0.0000  max mem: 18907
Train: data epoch: [0]  [  28/4722]  eta: 3:15:56  lr: 0.000010  loss: 8.4746  loss_lm: 8.6283 (8.8828)  time: 2.5266  data: 0.0000  max mem: 19011Train: data epoch: [0]  [  28/4722]  eta: 3:15:56  lr: 0.000010  loss: 8.5276  loss_lm: 8.5751 (8.8484)  time: 2.5266  data: 0.0000  max mem: 19007

Train: data epoch: [0]  [  28/4722]  eta: 3:15:56  lr: 0.000010  loss: 8.1592  loss_lm: 8.6049 (8.8111)  time: 2.5266  data: 0.0000  max mem: 18908
Train: data epoch: [0]  [  28/4722]  eta: 3:15:56  lr: 0.000010  loss: 8.5934  loss_lm: 8.5649 (8.8449)  time: 2.5267  data: 0.0000  max mem: 18861
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-12 23:49:10.493196] Starting job
[2023-11-12 23:49:10.493228] Starting job
[2023-11-12 23:49:10.493296] Starting job
[2023-11-12 23:49:10.493366] Starting job
[2023-11-12 23:49:10.501255] Starting job
[2023-11-12 23:49:10.503594] Starting job
| distributed init (rank 10, world 12): env://
| distributed init (rank 8, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 11, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 9, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  26/4722]  eta: 5:37:36  lr: 0.000010  loss: 8.1611  loss_lm: 8.1611 (8.1611)  time: 4.3136  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  26/4722]  eta: 5:37:43  lr: 0.000010  loss: 8.6209  loss_lm: 8.6209 (8.6209)  time: 4.3150  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  26/4722]  eta: 5:37:36  lr: 0.000010  loss: 8.6457  loss_lm: 8.6457 (8.6457)  time: 4.3136  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  26/4722]  eta: 5:37:41  lr: 0.000010  loss: 8.5448  loss_lm: 8.5448 (8.5448)  time: 4.3146  data: 0.0000  max mem: 18868Train: data epoch: [0]  [  26/4722]  eta: 5:37:36  lr: 0.000010  loss: 8.6385  loss_lm: 8.6385 (8.6385)  time: 4.3137  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  26/4722]  eta: 5:37:42  lr: 0.000010  loss: 8.4594  loss_lm: 8.4594 (8.4594)  time: 4.3149  data: 0.0000  max mem: 18863
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  27/4722]  eta: 3:47:17  lr: 0.000010  loss: 8.7157  loss_lm: 8.6209 (8.6683)  time: 2.9046  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  27/4722]  eta: 3:47:13  lr: 0.000010  loss: 8.2431  loss_lm: 8.1611 (8.2021)  time: 2.9039  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  27/4722]  eta: 3:47:16  lr: 0.000010  loss: 8.4776  loss_lm: 8.4776 (8.5112)  time: 2.9044  data: 0.0000  max mem: 18868Train: data epoch: [0]  [  27/4722]  eta: 3:47:13  lr: 0.000010  loss: 8.3932  loss_lm: 8.3932 (8.5158)  time: 2.9039  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  27/4722]  eta: 3:47:13  lr: 0.000010  loss: 8.5454  loss_lm: 8.5454 (8.5956)  time: 2.9039  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  27/4722]  eta: 3:47:16  lr: 0.000010  loss: 8.5424  loss_lm: 8.4594 (8.5009)  time: 2.9045  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  28/4722]  eta: 3:10:01  lr: 0.000010  loss: 8.4614  loss_lm: 8.6209 (8.5993)  time: 2.4290  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  28/4722]  eta: 3:10:01  lr: 0.000010  loss: 8.4609  loss_lm: 8.4609 (8.4876)  time: 2.4289  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  28/4722]  eta: 3:10:00  lr: 0.000010  loss: 8.5962  loss_lm: 8.5448 (8.5395)  time: 2.4288  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  28/4722]  eta: 3:09:59  lr: 0.000010  loss: 8.5163  loss_lm: 8.5454 (8.5692)  time: 2.4285  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  28/4722]  eta: 3:09:59  lr: 0.000010  loss: 8.1777  loss_lm: 8.1777 (8.1940)  time: 2.4285  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  28/4722]  eta: 3:09:59  lr: 0.000010  loss: 8.5041  loss_lm: 8.5041 (8.5119)  time: 2.4285  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  29/4722]  eta: 2:51:20  lr: 0.000010  loss: 8.2480  loss_lm: 8.4614 (8.5115)  time: 2.1906  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  29/4722]  eta: 2:51:20  lr: 0.000010  loss: 7.9392  loss_lm: 8.4594 (8.3505)  time: 2.1906  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  29/4722]  eta: 2:51:18  lr: 0.000010  loss: 8.3458  loss_lm: 8.3932 (8.4704)  time: 2.1903  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  29/4722]  eta: 2:51:20  lr: 0.000010  loss: 8.6583  loss_lm: 8.5448 (8.5692)  time: 2.1905  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  29/4722]  eta: 2:51:18  lr: 0.000010  loss: 8.2273  loss_lm: 8.5163 (8.4837)  time: 2.1903  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  29/4722]  eta: 2:51:19  lr: 0.000010  loss: 8.4151  loss_lm: 8.1777 (8.2493)  time: 2.1903  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  30/4722]  eta: 4:07:35  lr: 0.000010  loss: 8.0662  loss_lm: 8.3932 (8.3896)  time: 3.1662  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  30/4722]  eta: 4:07:35  lr: 0.000010  loss: 8.2855  loss_lm: 8.5163 (8.4440)  time: 3.1662  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  30/4722]  eta: 4:07:36  lr: 0.000010  loss: 8.2553  loss_lm: 8.5448 (8.5064)  time: 3.1664  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  30/4722]  eta: 4:07:35  lr: 0.000010  loss: 8.3435  loss_lm: 8.2431 (8.2681)  time: 3.1662  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  30/4722]  eta: 4:07:36  lr: 0.000010  loss: 8.5127  loss_lm: 8.4609 (8.3829)  time: 3.1664  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  30/4722]  eta: 4:07:37  lr: 0.000010  loss: 8.5117  loss_lm: 8.5117 (8.5115)  time: 3.1665  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  31/4722]  eta: 3:45:38  lr: 0.000010  loss: 8.2986  loss_lm: 8.4594 (8.3689)  time: 2.8860  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  31/4722]  eta: 3:45:38  lr: 0.000010  loss: 8.7064  loss_lm: 8.5448 (8.5397)  time: 2.8860  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [  31/4722]  eta: 3:45:38  lr: 0.000010  loss: 8.4649  loss_lm: 8.4649 (8.5038)  time: 2.8861  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  31/4722]  eta: 3:45:37  lr: 0.000010  loss: 8.4468  loss_lm: 8.2431 (8.2979)  time: 2.8858  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  31/4722]  eta: 3:45:37  lr: 0.000010  loss: 8.2704  loss_lm: 8.2855 (8.4151)  time: 2.8858  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  31/4722]  eta: 3:45:37  lr: 0.000010  loss: 8.3702  loss_lm: 8.3702 (8.3863)  time: 2.8859  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  32/4722]  eta: 3:29:54  lr: 0.000010  loss: 8.7185  loss_lm: 8.4609 (8.4188)  time: 2.6853  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  32/4722]  eta: 3:29:53  lr: 0.000010  loss: 8.2230  loss_lm: 8.2855 (8.3877)  time: 2.6851  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  32/4722]  eta: 3:29:54  lr: 0.000010  loss: 8.3195  loss_lm: 8.4649 (8.4774)  time: 2.6853  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  32/4722]  eta: 3:29:53  lr: 0.000010  loss: 8.0848  loss_lm: 8.2431 (8.2675)  time: 2.6851  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  32/4722]  eta: 3:29:53  lr: 0.000010  loss: 8.3664  loss_lm: 8.3702 (8.3835)  time: 2.6852  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  32/4722]  eta: 3:29:54  lr: 0.000010  loss: 8.2921  loss_lm: 8.5448 (8.5044)  time: 2.6853  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  33/4722]  eta: 3:18:17  lr: 0.000010  loss: 8.2555  loss_lm: 8.4594 (8.3984)  time: 2.5372  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  33/4722]  eta: 3:18:17  lr: 0.000010  loss: 8.5868  loss_lm: 8.4649 (8.4911)  time: 2.5372  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [  33/4722]  eta: 3:18:16  lr: 0.000010  loss: 8.3961  loss_lm: 8.2431 (8.2835)  time: 2.5371  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  33/4722]  eta: 3:18:16  lr: 0.000010  loss: 8.6896  loss_lm: 8.3702 (8.4217)  time: 2.5371  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  33/4722]  eta: 3:18:16  lr: 0.000010  loss: 8.1290  loss_lm: 8.4776 (8.4575)  time: 2.5372  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  33/4722]  eta: 3:18:16  lr: 0.000010  loss: 7.9429  loss_lm: 8.2704 (8.3321)  time: 2.5371  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  34/4722]  eta: 3:09:07  lr: 0.000010  loss: 8.5190  loss_lm: 8.5117 (8.4942)  time: 2.4204  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  34/4722]  eta: 3:09:06  lr: 0.000010  loss: 8.0586  loss_lm: 8.2704 (8.3017)  time: 2.4203  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  34/4722]  eta: 3:09:06  lr: 0.000010  loss: 8.5463  loss_lm: 8.3932 (8.4356)  time: 2.4203  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  34/4722]  eta: 3:09:06  lr: 0.000010  loss: 8.0829  loss_lm: 8.4776 (8.4158)  time: 2.4204  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  34/4722]  eta: 3:09:06  lr: 0.000010  loss: 8.4118  loss_lm: 8.4594 (8.3999)  time: 2.4204  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  34/4722]  eta: 3:09:06  lr: 0.000010  loss: 8.3892  loss_lm: 8.3435 (8.2953)  time: 2.4203  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  35/4722]  eta: 3:42:09  lr: 0.000010  loss: 8.4801  loss_lm: 8.3435 (8.3138)  time: 2.8440  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  35/4722]  eta: 3:42:09  lr: 0.000010  loss: 8.2769  loss_lm: 8.3702 (8.4197)  time: 2.8440  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  35/4722]  eta: 3:42:10  lr: 0.000010  loss: 8.7792  loss_lm: 8.4776 (8.4522)  time: 2.8441  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  35/4722]  eta: 3:42:09  lr: 0.000010  loss: 8.1830  loss_lm: 8.2273 (8.2898)  time: 2.8440  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  35/4722]  eta: 3:42:10  lr: 0.000010  loss: 7.9826  loss_lm: 8.4118 (8.3582)  time: 2.8441  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  35/4722]  eta: 3:42:10  lr: 0.000010  loss: 8.1916  loss_lm: 8.4649 (8.4639)  time: 2.8441  data: 0.0000  max mem: 18867
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  36/4722]  eta: 3:32:26  lr: 0.000010  loss: 8.4362  loss_lm: 8.4649 (8.4614)  time: 2.7201  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  36/4722]  eta: 3:32:25  lr: 0.000010  loss: 7.9685  loss_lm: 8.2273 (8.2606)  time: 2.7200  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  36/4722]  eta: 3:32:26  lr: 0.000010  loss: 8.5065  loss_lm: 8.4594 (8.3716)  time: 2.7201  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  36/4722]  eta: 3:32:26  lr: 0.000010  loss: 8.0097  loss_lm: 8.3702 (8.3824)  time: 2.7200  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  36/4722]  eta: 3:32:26  lr: 0.000010  loss: 8.1546  loss_lm: 8.3435 (8.2993)  time: 2.7200  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  36/4722]  eta: 3:32:26  lr: 0.000010  loss: 8.3655  loss_lm: 8.4776 (8.4443)  time: 2.7201  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  37/4722]  eta: 3:24:20  lr: 0.000010  loss: 8.0423  loss_lm: 8.2230 (8.2424)  time: 2.6170  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  37/4722]  eta: 3:24:21  lr: 0.000010  loss: 8.6321  loss_lm: 8.4649 (8.4756)  time: 2.6171  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [  37/4722]  eta: 3:24:20  lr: 0.000010  loss: 8.3675  loss_lm: 8.3675 (8.4379)  time: 2.6171  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  37/4722]  eta: 3:24:20  lr: 0.000010  loss: 7.9557  loss_lm: 8.2431 (8.2707)  time: 2.6170  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  37/4722]  eta: 3:24:20  lr: 0.000010  loss: 8.0419  loss_lm: 8.3664 (8.3541)  time: 2.6170  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  37/4722]  eta: 3:24:21  lr: 0.000010  loss: 7.9918  loss_lm: 8.4118 (8.3400)  time: 2.6171  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  38/4722]  eta: 3:17:28  lr: 0.000010  loss: 8.4875  loss_lm: 8.3435 (8.2873)  time: 2.5297  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  38/4722]  eta: 3:17:29  lr: 0.000010  loss: 8.2362  loss_lm: 8.4649 (8.4572)  time: 2.5298  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [  38/4722]  eta: 3:17:28  lr: 0.000010  loss: 8.0321  loss_lm: 8.2230 (8.2262)  time: 2.5297  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  38/4722]  eta: 3:17:28  lr: 0.000010  loss: 8.0332  loss_lm: 8.3664 (8.3294)  time: 2.5297  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  38/4722]  eta: 3:17:29  lr: 0.000010  loss: 8.1560  loss_lm: 8.3675 (8.4162)  time: 2.5297  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  38/4722]  eta: 3:17:29  lr: 0.000010  loss: 8.3051  loss_lm: 8.4118 (8.3373)  time: 2.5298  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  39/4722]  eta: 3:11:43  lr: 0.000010  loss: 8.0543  loss_lm: 8.4614 (8.4284)  time: 2.4565  data: 0.0000  max mem: 18867Train: data epoch: [0]  [  39/4722]  eta: 3:11:43  lr: 0.000010  loss: 8.1571  loss_lm: 8.3655 (8.3977)  time: 2.4564  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [  39/4722]  eta: 3:11:43  lr: 0.000010  loss: 8.2283  loss_lm: 8.2230 (8.2264)  time: 2.4564  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  39/4722]  eta: 3:11:43  lr: 0.000010  loss: 8.2535  loss_lm: 8.3051 (8.3313)  time: 2.4565  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  39/4722]  eta: 3:11:43  lr: 0.000010  loss: 8.0036  loss_lm: 8.3458 (8.3061)  time: 2.4564  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  39/4722]  eta: 3:11:43  lr: 0.000010  loss: 8.5922  loss_lm: 8.3435 (8.3091)  time: 2.4564  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  40/4722]  eta: 3:34:48  lr: 0.000010  loss: 8.2718  loss_lm: 8.3655 (8.3893)  time: 2.7528  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  40/4722]  eta: 3:34:48  lr: 0.000010  loss: 8.3706  loss_lm: 8.3706 (8.3339)  time: 2.7528  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  40/4722]  eta: 3:34:48  lr: 0.000010  loss: 7.9300  loss_lm: 8.4614 (8.3952)  time: 2.7528  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  40/4722]  eta: 3:34:48  lr: 0.000010  loss: 7.9159  loss_lm: 8.3458 (8.2801)  time: 2.7527  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  40/4722]  eta: 3:34:48  lr: 0.000010  loss: 8.0002  loss_lm: 8.2230 (8.2113)  time: 2.7527  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  40/4722]  eta: 3:34:48  lr: 0.000010  loss: 8.1990  loss_lm: 8.3435 (8.3018)  time: 2.7527  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  41/4722]  eta: 3:28:34  lr: 0.000010  loss: 8.3029  loss_lm: 8.4362 (8.3894)  time: 2.6735  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  41/4722]  eta: 3:28:34  lr: 0.000010  loss: 8.2555  loss_lm: 8.2769 (8.2786)  time: 2.6734  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  41/4722]  eta: 3:28:34  lr: 0.000010  loss: 8.4432  loss_lm: 8.2230 (8.2258)  time: 2.6734  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  41/4722]  eta: 3:28:34  lr: 0.000010  loss: 7.9525  loss_lm: 8.2431 (8.2799)  time: 2.6734  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  41/4722]  eta: 3:28:34  lr: 0.000010  loss: 8.4222  loss_lm: 8.3655 (8.3914)  time: 2.6735  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  41/4722]  eta: 3:28:34  lr: 0.000010  loss: 8.4407  loss_lm: 8.3706 (8.3406)  time: 2.6735  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  42/4722]  eta: 3:23:03  lr: 0.000010  loss: 7.7887  loss_lm: 8.4362 (8.3541)  time: 2.6034  data: 0.0000  max mem: 18867Train: data epoch: [0]  [  42/4722]  eta: 3:23:03  lr: 0.000010  loss: 8.2789  loss_lm: 8.2789 (8.2786)  time: 2.6033  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  42/4722]  eta: 3:23:03  lr: 0.000010  loss: 8.2536  loss_lm: 8.2273 (8.2274)  time: 2.6033  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  42/4722]  eta: 3:23:03  lr: 0.000010  loss: 8.2421  loss_lm: 8.2431 (8.2777)  time: 2.6033  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  42/4722]  eta: 3:23:03  lr: 0.000010  loss: 8.3022  loss_lm: 8.3655 (8.3861)  time: 2.6034  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  42/4722]  eta: 3:23:03  lr: 0.000010  loss: 8.0113  loss_lm: 8.3706 (8.3212)  time: 2.6034  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  43/4722]  eta: 3:18:08  lr: 0.000010  loss: 7.9429  loss_lm: 8.3195 (8.3313)  time: 2.5408  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  43/4722]  eta: 3:18:08  lr: 0.000010  loss: 7.7060  loss_lm: 8.2769 (8.2468)  time: 2.5407  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  43/4722]  eta: 3:18:08  lr: 0.000010  loss: 8.0851  loss_lm: 8.3022 (8.3694)  time: 2.5408  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  43/4722]  eta: 3:18:07  lr: 0.000010  loss: 8.1078  loss_lm: 8.2421 (8.2683)  time: 2.5407  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  43/4722]  eta: 3:18:08  lr: 0.000010  loss: 8.3293  loss_lm: 8.2273 (8.2331)  time: 2.5407  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  43/4722]  eta: 3:18:08  lr: 0.000010  loss: 8.4043  loss_lm: 8.3706 (8.3259)  time: 2.5408  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  44/4722]  eta: 3:13:44  lr: 0.000010  loss: 7.8314  loss_lm: 8.3195 (8.3050)  time: 2.4849  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  44/4722]  eta: 3:13:43  lr: 0.000010  loss: 7.9881  loss_lm: 8.3022 (8.3493)  time: 2.4848  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  44/4722]  eta: 3:13:43  lr: 0.000010  loss: 8.3363  loss_lm: 8.2789 (8.2515)  time: 2.4848  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  44/4722]  eta: 3:13:43  lr: 0.000010  loss: 8.4678  loss_lm: 8.4043 (8.3333)  time: 2.4848  data: 0.0000  max mem: 18976Train: data epoch: [0]  [  44/4722]  eta: 3:13:43  lr: 0.000010  loss: 7.9839  loss_lm: 8.2273 (8.2200)  time: 2.4848  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  44/4722]  eta: 3:13:43  lr: 0.000010  loss: 8.1749  loss_lm: 8.2421 (8.2634)  time: 2.4848  data: 0.0000  max mem: 18865


Train: data epoch: [0]  [  45/4722]  eta: 3:30:32  lr: 0.000010  loss: 8.3218  loss_lm: 8.3195 (8.3058)  time: 2.7010  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  45/4722]  eta: 3:30:32  lr: 0.000010  loss: 8.0680  loss_lm: 8.2230 (8.2124)  time: 2.7010  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  45/4722]  eta: 3:30:32  lr: 0.000010  loss: 7.9559  loss_lm: 8.1990 (8.2480)  time: 2.7009  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  45/4722]  eta: 3:30:32  lr: 0.000010  loss: 8.3941  loss_lm: 8.3022 (8.3516)  time: 2.7010  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  45/4722]  eta: 3:30:32  lr: 0.000010  loss: 7.9289  loss_lm: 8.3706 (8.3131)  time: 2.7010  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  45/4722]  eta: 3:30:32  lr: 0.000010  loss: 8.5362  loss_lm: 8.2789 (8.2657)  time: 2.7010  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  46/4722]  eta: 3:25:58  lr: 0.000010  loss: 7.8913  loss_lm: 8.2921 (8.3297)  time: 2.5593  data: 0.0000  max mem: 18868Train: data epoch: [0]  [  46/4722]  eta: 3:25:58  lr: 0.000010  loss: 8.0931  loss_lm: 8.3029 (8.2957)  time: 2.5593  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [  46/4722]  eta: 3:25:57  lr: 0.000010  loss: 8.0700  loss_lm: 8.2769 (8.2564)  time: 2.5593  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  46/4722]  eta: 3:25:58  lr: 0.000010  loss: 8.1681  loss_lm: 8.1830 (8.2103)  time: 2.5593  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  46/4722]  eta: 3:25:57  lr: 0.000010  loss: 7.8755  loss_lm: 8.1990 (8.2303)  time: 2.5593  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  46/4722]  eta: 3:25:58  lr: 0.000010  loss: 7.8176  loss_lm: 8.3051 (8.2895)  time: 2.5593  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  47/4722]  eta: 3:21:48  lr: 0.000010  loss: 7.8426  loss_lm: 8.2480 (8.2751)  time: 2.5585  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  47/4722]  eta: 3:21:47  lr: 0.000010  loss: 7.9243  loss_lm: 8.2555 (8.2413)  time: 2.5585  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  47/4722]  eta: 3:21:47  lr: 0.000010  loss: 7.9043  loss_lm: 8.1681 (8.1964)  time: 2.5585  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  47/4722]  eta: 3:21:47  lr: 0.000010  loss: 7.8630  loss_lm: 8.2718 (8.3084)  time: 2.5585  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  47/4722]  eta: 3:21:47  lr: 0.000010  loss: 8.1170  loss_lm: 8.1777 (8.2251)  time: 2.5585  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  47/4722]  eta: 3:21:47  lr: 0.000010  loss: 8.0692  loss_lm: 8.2986 (8.2795)  time: 2.5585  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  48/4722]  eta: 3:18:01  lr: 0.000010  loss: 8.5112  loss_lm: 8.2480 (8.2853)  time: 2.5590  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  48/4722]  eta: 3:18:00  lr: 0.000010  loss: 8.0080  loss_lm: 8.0680 (8.1882)  time: 2.5589  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  48/4722]  eta: 3:18:00  lr: 0.000010  loss: 7.8701  loss_lm: 8.0700 (8.2252)  time: 2.5589  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  48/4722]  eta: 3:18:01  lr: 0.000010  loss: 8.4904  loss_lm: 8.2718 (8.3164)  time: 2.5589  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  48/4722]  eta: 3:18:01  lr: 0.000010  loss: 7.8354  loss_lm: 8.2555 (8.2602)  time: 2.5589  data: 0.0000  max mem: 18976Train: data epoch: [0]  [  48/4722]  eta: 3:18:00  lr: 0.000010  loss: 7.6496  loss_lm: 8.1749 (8.2001)  time: 2.5589  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  49/4722]  eta: 3:14:32  lr: 0.000010  loss: 7.9645  loss_lm: 8.2362 (8.2720)  time: 2.5593  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  49/4722]  eta: 3:14:31  lr: 0.000010  loss: 8.1845  loss_lm: 8.0700 (8.2235)  time: 2.5592  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  49/4722]  eta: 3:14:31  lr: 0.000010  loss: 7.9077  loss_lm: 8.0586 (8.1765)  time: 2.5592  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  49/4722]  eta: 3:14:32  lr: 0.000010  loss: 8.0031  loss_lm: 8.2553 (8.3033)  time: 2.5592  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  49/4722]  eta: 3:14:31  lr: 0.000010  loss: 7.8672  loss_lm: 8.1546 (8.1862)  time: 2.5592  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  49/4722]  eta: 3:14:32  lr: 0.000010  loss: 8.1957  loss_lm: 8.2555 (8.2575)  time: 2.5592  data: 0.0000  max mem: 18976
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-12 23:53:55.976003] Starting job[2023-11-12 23:53:55.976004] Starting job
[2023-11-12 23:53:55.976006] Starting job[2023-11-12 23:53:55.976009] Starting job


[2023-11-12 23:53:55.976020] Starting job
[2023-11-12 23:53:55.976169] Starting job
| distributed init (rank 6, world 12): env://
| distributed init (rank 8, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 9, world 12): env://
| distributed init (rank 10, world 12): env://
| distributed init (rank 11, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  46/4722]  eta: 5:35:57  lr: 0.000010  loss: 7.8473  loss_lm: 7.8473 (7.8473)  time: 4.3107  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  46/4722]  eta: 5:36:01  lr: 0.000010  loss: 8.0923  loss_lm: 8.0923 (8.0923)  time: 4.3117  data: 0.0000  max mem: 18868Train: data epoch: [0]  [  46/4722]  eta: 5:35:56  lr: 0.000010  loss: 8.1456  loss_lm: 8.1456 (8.1456)  time: 4.3106  data: 0.0000  max mem: 18906


Train: data epoch: [0]  [  46/4722]  eta: 5:36:00  lr: 0.000010  loss: 8.0686  loss_lm: 8.0686 (8.0686)  time: 4.3114  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  46/4722]  eta: 5:35:51  lr: 0.000010  loss: 7.8794  loss_lm: 7.8794 (7.8794)  time: 4.3096  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [  46/4722]  eta: 5:35:58  lr: 0.000010  loss: 7.8178  loss_lm: 7.8178 (7.8178)  time: 4.3110  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  47/4722]  eta: 3:46:16  lr: 0.000010  loss: 7.8866  loss_lm: 7.8794 (7.8830)  time: 2.9040  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  47/4722]  eta: 3:46:18  lr: 0.000010  loss: 7.9131  loss_lm: 7.9131 (8.0293)  time: 2.9046  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  47/4722]  eta: 3:46:19  lr: 0.000010  loss: 8.1359  loss_lm: 7.8473 (7.9916)  time: 2.9046  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  47/4722]  eta: 3:46:20  lr: 0.000010  loss: 7.9468  loss_lm: 7.9468 (8.0077)  time: 2.9050  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  47/4722]  eta: 3:46:19  lr: 0.000010  loss: 8.1163  loss_lm: 7.8178 (7.9670)  time: 2.9047  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  47/4722]  eta: 3:46:21  lr: 0.000010  loss: 7.8192  loss_lm: 7.8192 (7.9557)  time: 2.9051  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  48/4722]  eta: 3:10:12  lr: 0.000010  loss: 8.4676  loss_lm: 7.8866 (8.0779)  time: 2.4416  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  48/4722]  eta: 3:10:13  lr: 0.000010  loss: 8.0337  loss_lm: 8.0337 (8.0308)  time: 2.4419  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  48/4722]  eta: 3:10:13  lr: 0.000010  loss: 7.6534  loss_lm: 7.8473 (7.8788)  time: 2.4420  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  48/4722]  eta: 3:10:14  lr: 0.000010  loss: 7.8885  loss_lm: 7.9468 (7.9680)  time: 2.4422  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  48/4722]  eta: 3:10:14  lr: 0.000010  loss: 7.8269  loss_lm: 7.8269 (7.9203)  time: 2.4420  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  48/4722]  eta: 3:10:15  lr: 0.000010  loss: 8.4820  loss_lm: 8.0923 (8.1312)  time: 2.4423  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  49/4722]  eta: 2:52:03  lr: 0.000010  loss: 8.0315  loss_lm: 7.8866 (8.0663)  time: 2.2091  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  49/4722]  eta: 2:52:04  lr: 0.000010  loss: 7.9159  loss_lm: 7.9159 (8.0021)  time: 2.2093  data: 0.0000  max mem: 18906Train: data epoch: [0]  [  49/4722]  eta: 2:52:04  lr: 0.000010  loss: 7.8703  loss_lm: 7.8473 (7.8767)  time: 2.2094  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  49/4722]  eta: 2:52:05  lr: 0.000010  loss: 8.1789  loss_lm: 7.9468 (8.0207)  time: 2.2095  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  49/4722]  eta: 2:52:04  lr: 0.000010  loss: 8.1772  loss_lm: 7.8269 (7.9845)  time: 2.2094  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  49/4722]  eta: 2:52:05  lr: 0.000010  loss: 7.9504  loss_lm: 7.9504 (8.0860)  time: 2.2095  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  50/4722]  eta: 4:04:09  lr: 0.000010  loss: 7.6710  loss_lm: 7.8866 (7.9872)  time: 3.1356  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  50/4722]  eta: 4:04:11  lr: 0.000010  loss: 7.8700  loss_lm: 7.9468 (7.9906)  time: 3.1360  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  50/4722]  eta: 4:04:10  lr: 0.000010  loss: 7.8400  loss_lm: 7.8400 (7.9556)  time: 3.1359  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  50/4722]  eta: 4:04:10  lr: 0.000010  loss: 8.5063  loss_lm: 7.8703 (8.0026)  time: 3.1358  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  50/4722]  eta: 4:04:11  lr: 0.000010  loss: 8.0439  loss_lm: 8.0439 (8.0776)  time: 3.1360  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  50/4722]  eta: 4:04:10  lr: 0.000010  loss: 7.5576  loss_lm: 7.9159 (7.9132)  time: 3.1358  data: 0.0000  max mem: 18906
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  51/4722]  eta: 3:43:47  lr: 0.000010  loss: 7.9811  loss_lm: 7.9468 (7.9890)  time: 2.8747  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  51/4722]  eta: 3:43:46  lr: 0.000010  loss: 8.0445  loss_lm: 7.9159 (7.9350)  time: 2.8745  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  51/4722]  eta: 3:43:47  lr: 0.000010  loss: 7.9933  loss_lm: 7.8400 (7.9619)  time: 2.8746  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  51/4722]  eta: 3:43:46  lr: 0.000010  loss: 8.2419  loss_lm: 7.8703 (8.0425)  time: 2.8745  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  51/4722]  eta: 3:43:46  lr: 0.000010  loss: 8.2028  loss_lm: 7.8866 (8.0231)  time: 2.8744  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  51/4722]  eta: 3:43:47  lr: 0.000010  loss: 8.0318  loss_lm: 8.0318 (8.0699)  time: 2.8746  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  52/4722]  eta: 3:29:08  lr: 0.000010  loss: 7.6822  loss_lm: 7.9468 (7.9452)  time: 2.6870  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  52/4722]  eta: 3:29:07  lr: 0.000010  loss: 7.8963  loss_lm: 7.9159 (7.9295)  time: 2.6868  data: 0.0000  max mem: 18906Train: data epoch: [0]  [  52/4722]  eta: 3:29:07  lr: 0.000010  loss: 8.0972  loss_lm: 8.0972 (8.0503)  time: 2.6869  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [  52/4722]  eta: 3:29:07  lr: 0.000010  loss: 7.8284  loss_lm: 7.8866 (7.9953)  time: 2.6867  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  52/4722]  eta: 3:29:07  lr: 0.000010  loss: 7.6750  loss_lm: 7.8400 (7.9209)  time: 2.6869  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  52/4722]  eta: 3:29:08  lr: 0.000010  loss: 7.8583  loss_lm: 8.0318 (8.0397)  time: 2.6869  data: 0.0000  max mem: 18868

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  53/4722]  eta: 3:17:34  lr: 0.000010  loss: 8.0310  loss_lm: 7.9468 (7.9559)  time: 2.5389  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  53/4722]  eta: 3:17:33  lr: 0.000010  loss: 8.0474  loss_lm: 7.8866 (8.0018)  time: 2.5387  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  53/4722]  eta: 3:17:33  lr: 0.000010  loss: 7.4257  loss_lm: 7.8269 (7.8590)  time: 2.5388  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  53/4722]  eta: 3:17:33  lr: 0.000010  loss: 8.1079  loss_lm: 8.0972 (8.0575)  time: 2.5388  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  53/4722]  eta: 3:17:33  lr: 0.000010  loss: 7.9902  loss_lm: 7.9159 (7.9371)  time: 2.5388  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  53/4722]  eta: 3:17:34  lr: 0.000010  loss: 7.6104  loss_lm: 7.9504 (7.9860)  time: 2.5389  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  54/4722]  eta: 3:08:25  lr: 0.000010  loss: 7.9395  loss_lm: 7.9395 (7.9374)  time: 2.4220  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  54/4722]  eta: 3:08:26  lr: 0.000010  loss: 7.9049  loss_lm: 7.9468 (7.9502)  time: 2.4221  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  54/4722]  eta: 3:08:26  lr: 0.000010  loss: 8.0960  loss_lm: 7.8400 (7.8854)  time: 2.4221  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  54/4722]  eta: 3:08:26  lr: 0.000010  loss: 8.2775  loss_lm: 8.1079 (8.0820)  time: 2.4220  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  54/4722]  eta: 3:08:26  lr: 0.000010  loss: 8.0003  loss_lm: 8.0003 (7.9876)  time: 2.4221  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  54/4722]  eta: 3:08:25  lr: 0.000010  loss: 7.9956  loss_lm: 7.9956 (8.0011)  time: 2.4219  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  55/4722]  eta: 3:43:23  lr: 0.000010  loss: 7.9390  loss_lm: 7.9390 (7.9491)  time: 2.8720  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  55/4722]  eta: 3:43:23  lr: 0.000010  loss: 7.6280  loss_lm: 7.8269 (7.8596)  time: 2.8719  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  55/4722]  eta: 3:43:22  lr: 0.000010  loss: 7.7716  loss_lm: 8.0972 (8.0509)  time: 2.8719  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [  55/4722]  eta: 3:43:22  lr: 0.000010  loss: 7.9387  loss_lm: 7.9387 (7.9949)  time: 2.8718  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  55/4722]  eta: 3:43:23  lr: 0.000010  loss: 7.9107  loss_lm: 7.9504 (7.9799)  time: 2.8719  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  55/4722]  eta: 3:43:22  lr: 0.000010  loss: 7.6743  loss_lm: 7.9159 (7.9111)  time: 2.8719  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  56/4722]  eta: 3:33:33  lr: 0.000010  loss: 8.1548  loss_lm: 7.9468 (7.9678)  time: 2.7460  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  56/4722]  eta: 3:33:32  lr: 0.000010  loss: 7.9361  loss_lm: 7.9387 (7.9895)  time: 2.7458  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  56/4722]  eta: 3:33:32  lr: 0.000010  loss: 7.6716  loss_lm: 7.9504 (7.9519)  time: 2.7460  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  56/4722]  eta: 3:33:32  lr: 0.000010  loss: 8.1190  loss_lm: 7.9395 (7.9300)  time: 2.7459  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  56/4722]  eta: 3:33:32  lr: 0.000010  loss: 8.0157  loss_lm: 8.0972 (8.0477)  time: 2.7459  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  56/4722]  eta: 3:33:32  lr: 0.000010  loss: 7.8101  loss_lm: 7.8269 (7.8551)  time: 2.7460  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  57/4722]  eta: 3:25:19  lr: 0.000010  loss: 7.9724  loss_lm: 7.9468 (7.9682)  time: 2.6408  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  57/4722]  eta: 3:25:18  lr: 0.000010  loss: 7.6895  loss_lm: 7.9107 (7.9300)  time: 2.6407  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  57/4722]  eta: 3:25:18  lr: 0.000010  loss: 7.6549  loss_lm: 7.9361 (7.9617)  time: 2.6406  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  57/4722]  eta: 3:25:18  lr: 0.000010  loss: 7.7100  loss_lm: 7.8178 (7.8430)  time: 2.6407  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  57/4722]  eta: 3:25:18  lr: 0.000010  loss: 8.3627  loss_lm: 8.0972 (8.0740)  time: 2.6407  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  57/4722]  eta: 3:25:18  lr: 0.000010  loss: 7.5297  loss_lm: 7.9159 (7.8966)  time: 2.6407  data: 0.0000  max mem: 18906
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  58/4722]  eta: 3:18:25  lr: 0.000010  loss: 7.6399  loss_lm: 7.9468 (7.9429)  time: 2.5527  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  58/4722]  eta: 3:18:25  lr: 0.000010  loss: 7.7308  loss_lm: 7.9107 (7.9147)  time: 2.5526  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  58/4722]  eta: 3:18:24  lr: 0.000010  loss: 7.9537  loss_lm: 7.9387 (7.9610)  time: 2.5525  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  58/4722]  eta: 3:18:24  lr: 0.000010  loss: 7.9261  loss_lm: 7.9261 (7.8989)  time: 2.5525  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  58/4722]  eta: 3:18:25  lr: 0.000010  loss: 7.9453  loss_lm: 8.0972 (8.0641)  time: 2.5526  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  58/4722]  eta: 3:18:25  lr: 0.000010  loss: 8.1496  loss_lm: 7.8269 (7.8666)  time: 2.5526  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  59/4722]  eta: 3:12:38  lr: 0.000010  loss: 8.3491  loss_lm: 7.8269 (7.9011)  time: 2.4787  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  59/4722]  eta: 3:12:38  lr: 0.000010  loss: 8.3025  loss_lm: 7.9468 (7.9686)  time: 2.4788  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  59/4722]  eta: 3:12:38  lr: 0.000010  loss: 7.8468  loss_lm: 8.0157 (8.0486)  time: 2.4787  data: 0.0000  max mem: 19014Train: data epoch: [0]  [  59/4722]  eta: 3:12:38  lr: 0.000010  loss: 7.6249  loss_lm: 7.9159 (7.8793)  time: 2.4787  data: 0.0000  max mem: 18906

Train: data epoch: [0]  [  59/4722]  eta: 3:12:38  lr: 0.000010  loss: 8.3663  loss_lm: 7.9107 (7.9470)  time: 2.4788  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  59/4722]  eta: 3:12:38  lr: 0.000010  loss: 7.7780  loss_lm: 7.9361 (7.9480)  time: 2.4787  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  60/4722]  eta: 3:35:25  lr: 0.000010  loss: 8.0620  loss_lm: 7.9261 (7.8915)  time: 2.7725  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  60/4722]  eta: 3:35:26  lr: 0.000010  loss: 7.7349  loss_lm: 7.9468 (7.9530)  time: 2.7727  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  60/4722]  eta: 3:35:25  lr: 0.000010  loss: 7.5828  loss_lm: 7.9361 (7.9236)  time: 2.7725  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  60/4722]  eta: 3:35:25  lr: 0.000010  loss: 7.5539  loss_lm: 8.0157 (8.0156)  time: 2.7725  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  60/4722]  eta: 3:35:25  lr: 0.000010  loss: 7.9306  loss_lm: 7.9306 (7.9459)  time: 2.7726  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  60/4722]  eta: 3:35:25  lr: 0.000010  loss: 7.6841  loss_lm: 7.8269 (7.8866)  time: 2.7726  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  61/4722]  eta: 3:29:06  lr: 0.000010  loss: 7.6719  loss_lm: 7.9107 (7.9288)  time: 2.6919  data: 0.0000  max mem: 18868Train: data epoch: [0]  [  61/4722]  eta: 3:29:06  lr: 0.000010  loss: 7.9259  loss_lm: 7.9259 (7.9238)  time: 2.6918  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [  61/4722]  eta: 3:29:07  lr: 0.000010  loss: 8.0112  loss_lm: 7.9468 (7.9567)  time: 2.6919  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  61/4722]  eta: 3:29:06  lr: 0.000010  loss: 7.9948  loss_lm: 7.9261 (7.8979)  time: 2.6918  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  61/4722]  eta: 3:29:06  lr: 0.000010  loss: 7.5633  loss_lm: 7.9453 (7.9873)  time: 2.6918  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  61/4722]  eta: 3:29:06  lr: 0.000010  loss: 7.8280  loss_lm: 7.8269 (7.8829)  time: 2.6918  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  62/4722]  eta: 3:23:32  lr: 0.000010  loss: 8.1157  loss_lm: 7.9395 (7.9108)  time: 2.6207  data: 0.0000  max mem: 18906Train: data epoch: [0]  [  62/4722]  eta: 3:23:32  lr: 0.000010  loss: 7.6330  loss_lm: 7.9107 (7.9114)  time: 2.6207  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [  62/4722]  eta: 3:23:33  lr: 0.000010  loss: 7.7665  loss_lm: 7.9468 (7.9455)  time: 2.6208  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  62/4722]  eta: 3:23:32  lr: 0.000010  loss: 7.4146  loss_lm: 7.9259 (7.8938)  time: 2.6207  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  62/4722]  eta: 3:23:32  lr: 0.000010  loss: 7.9220  loss_lm: 7.9453 (7.9835)  time: 2.6207  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  62/4722]  eta: 3:23:32  lr: 0.000010  loss: 7.9351  loss_lm: 7.8280 (7.8860)  time: 2.6207  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  63/4722]  eta: 3:18:34  lr: 0.000010  loss: 8.0857  loss_lm: 7.9468 (7.9533)  time: 2.5573  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  63/4722]  eta: 3:18:34  lr: 0.000010  loss: 7.7571  loss_lm: 7.8583 (7.9028)  time: 2.5572  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  63/4722]  eta: 3:18:33  lr: 0.000010  loss: 7.5363  loss_lm: 7.8866 (7.8740)  time: 2.5571  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  63/4722]  eta: 3:18:33  lr: 0.000010  loss: 7.7598  loss_lm: 7.9261 (7.9024)  time: 2.5572  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  63/4722]  eta: 3:18:33  lr: 0.000010  loss: 8.1061  loss_lm: 7.9453 (7.9903)  time: 2.5572  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  63/4722]  eta: 3:18:33  lr: 0.000010  loss: 7.6137  loss_lm: 7.8269 (7.8709)  time: 2.5572  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  64/4722]  eta: 3:14:07  lr: 0.000010  loss: 7.5761  loss_lm: 7.8583 (7.8856)  time: 2.5006  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  64/4722]  eta: 3:14:07  lr: 0.000010  loss: 7.6560  loss_lm: 7.8866 (7.8625)  time: 2.5005  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  64/4722]  eta: 3:14:08  lr: 0.000010  loss: 7.8248  loss_lm: 7.9468 (7.9465)  time: 2.5007  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  64/4722]  eta: 3:14:07  lr: 0.000010  loss: 7.6820  loss_lm: 7.9453 (7.9741)  time: 2.5006  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  64/4722]  eta: 3:14:07  lr: 0.000010  loss: 7.7817  loss_lm: 7.9261 (7.8960)  time: 2.5006  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  64/4722]  eta: 3:14:07  lr: 0.000010  loss: 7.6619  loss_lm: 7.8269 (7.8599)  time: 2.5006  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  65/4722]  eta: 3:30:29  lr: 0.000010  loss: 7.6092  loss_lm: 7.9159 (7.8817)  time: 2.7120  data: 0.0000  max mem: 18906Train: data epoch: [0]  [  65/4722]  eta: 3:30:29  lr: 0.000010  loss: 7.8698  loss_lm: 7.9220 (7.9688)  time: 2.7120  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [  65/4722]  eta: 3:30:30  lr: 0.000010  loss: 7.8639  loss_lm: 7.9390 (7.9424)  time: 2.7121  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  65/4722]  eta: 3:30:30  lr: 0.000010  loss: 7.6579  loss_lm: 7.8192 (7.8742)  time: 2.7121  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  65/4722]  eta: 3:30:29  lr: 0.000010  loss: 8.0552  loss_lm: 7.8866 (7.8721)  time: 2.7120  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  65/4722]  eta: 3:30:29  lr: 0.000010  loss: 7.5973  loss_lm: 7.8178 (7.8468)  time: 2.7120  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  66/4722]  eta: 3:26:15  lr: 0.000010  loss: 7.7564  loss_lm: 7.7571 (7.8686)  time: 2.5754  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  66/4722]  eta: 3:26:15  lr: 0.000010  loss: 7.6640  loss_lm: 7.9131 (7.8713)  time: 2.5754  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  66/4722]  eta: 3:26:16  lr: 0.000010  loss: 8.0179  loss_lm: 7.9390 (7.9460)  time: 2.5754  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  66/4722]  eta: 3:26:15  lr: 0.000010  loss: 7.5359  loss_lm: 7.9220 (7.9482)  time: 2.5754  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  66/4722]  eta: 3:26:15  lr: 0.000010  loss: 7.6102  loss_lm: 7.8101 (7.8355)  time: 2.5753  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  66/4722]  eta: 3:26:15  lr: 0.000010  loss: 7.5065  loss_lm: 7.8866 (7.8547)  time: 2.5754  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  67/4722]  eta: 3:22:04  lr: 0.000010  loss: 7.9241  loss_lm: 7.7571 (7.8711)  time: 2.5746  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  67/4722]  eta: 3:22:04  lr: 0.000010  loss: 7.4837  loss_lm: 7.8703 (7.9271)  time: 2.5746  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  67/4722]  eta: 3:22:04  lr: 0.000010  loss: 8.1823  loss_lm: 7.9159 (7.8854)  time: 2.5746  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  67/4722]  eta: 3:22:04  lr: 0.000010  loss: 7.5667  loss_lm: 7.7100 (7.8233)  time: 2.5746  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  67/4722]  eta: 3:22:04  lr: 0.000010  loss: 7.6539  loss_lm: 7.9049 (7.9327)  time: 2.5747  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  67/4722]  eta: 3:22:04  lr: 0.000010  loss: 8.2404  loss_lm: 7.9259 (7.8722)  time: 2.5746  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  68/4722]  eta: 3:18:16  lr: 0.000010  loss: 7.4923  loss_lm: 7.8963 (7.8684)  time: 2.5733  data: 0.0000  max mem: 18906Train: data epoch: [0]  [  68/4722]  eta: 3:18:16  lr: 0.000010  loss: 7.7033  loss_lm: 7.8703 (7.9174)  time: 2.5733  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [  68/4722]  eta: 3:18:16  lr: 0.000010  loss: 7.4426  loss_lm: 7.6841 (7.8067)  time: 2.5732  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  68/4722]  eta: 3:18:16  lr: 0.000010  loss: 7.3916  loss_lm: 7.7564 (7.8503)  time: 2.5733  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  68/4722]  eta: 3:18:16  lr: 0.000010  loss: 7.2576  loss_lm: 7.9049 (7.9033)  time: 2.5733  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  68/4722]  eta: 3:18:16  lr: 0.000010  loss: 7.5747  loss_lm: 7.8284 (7.8593)  time: 2.5733  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  69/4722]  eta: 3:14:45  lr: 0.000010  loss: 7.9701  loss_lm: 7.6841 (7.8135)  time: 2.5719  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  69/4722]  eta: 3:14:45  lr: 0.000010  loss: 7.5932  loss_lm: 7.7817 (7.8569)  time: 2.5719  data: 0.0000  max mem: 18906
Train: data epoch: [0]  [  69/4722]  eta: 3:14:45  lr: 0.000010  loss: 7.6246  loss_lm: 7.8698 (7.9052)  time: 2.5719  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  69/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.4539  loss_lm: 7.8700 (7.8846)  time: 2.5720  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [  69/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.7047  loss_lm: 7.7308 (7.8442)  time: 2.5719  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  69/4722]  eta: 3:14:45  lr: 0.000010  loss: 7.5613  loss_lm: 7.7780 (7.8469)  time: 2.5719  data: 0.0000  max mem: 18867
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:00:09.061617] Starting job
[2023-11-13 00:00:09.061810] Starting job
[2023-11-13 00:00:09.061871] Starting job
[2023-11-13 00:00:09.061885] Starting job
[2023-11-13 00:00:09.111677] Starting job
[2023-11-13 00:00:09.121162] Starting job
| distributed init (rank 11, world 12): env://
| distributed init (rank 10, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 8, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 9, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  66/4722]  eta: 5:37:51  lr: 0.000010  loss: 7.5167  loss_lm: 7.5167 (7.5167)  time: 4.3539  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  66/4722]  eta: 5:37:54  lr: 0.000010  loss: 7.4908  loss_lm: 7.4908 (7.4908)  time: 4.3545  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  66/4722]  eta: 5:38:01  lr: 0.000010  loss: 7.7380  loss_lm: 7.7380 (7.7380)  time: 4.3560  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  66/4722]  eta: 5:38:00  lr: 0.000010  loss: 7.6744  loss_lm: 7.6744 (7.6744)  time: 4.3558  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  66/4722]  eta: 5:37:50  lr: 0.000010  loss: 8.0273  loss_lm: 8.0273 (8.0273)  time: 4.3535  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  66/4722]  eta: 5:37:54  lr: 0.000010  loss: 7.6177  loss_lm: 7.6177 (7.6177)  time: 4.3545  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [  67/4722]  eta: 3:46:58  lr: 0.000010  loss: 7.8831  loss_lm: 7.7380 (7.8106)  time: 2.9257  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  67/4722]  eta: 3:46:53  lr: 0.000010  loss: 7.6228  loss_lm: 7.6228 (7.8250)  time: 2.9244  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  67/4722]  eta: 3:46:58  lr: 0.000010  loss: 8.1935  loss_lm: 7.6744 (7.9339)  time: 2.9256  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  67/4722]  eta: 3:46:54  lr: 0.000010  loss: 7.5263  loss_lm: 7.5263 (7.5720)  time: 2.9247  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  67/4722]  eta: 3:46:55  lr: 0.000010  loss: 8.2205  loss_lm: 7.4908 (7.8556)  time: 2.9249  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  67/4722]  eta: 3:46:54  lr: 0.000010  loss: 7.4937  loss_lm: 7.4937 (7.5052)  time: 2.9247  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  68/4722]  eta: 3:09:56  lr: 0.000010  loss: 7.2860  loss_lm: 7.6228 (7.6454)  time: 2.4487  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  68/4722]  eta: 3:09:56  lr: 0.000010  loss: 7.7203  loss_lm: 7.5167 (7.5769)  time: 2.4487  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  68/4722]  eta: 3:09:56  lr: 0.000010  loss: 7.4331  loss_lm: 7.5263 (7.5257)  time: 2.4488  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  68/4722]  eta: 3:09:57  lr: 0.000010  loss: 7.5880  loss_lm: 7.5880 (7.7664)  time: 2.4489  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  68/4722]  eta: 3:09:59  lr: 0.000010  loss: 7.4863  loss_lm: 7.6744 (7.7847)  time: 2.4494  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  68/4722]  eta: 3:09:59  lr: 0.000010  loss: 7.3780  loss_lm: 7.7380 (7.6664)  time: 2.4495  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  69/4722]  eta: 2:51:08  lr: 0.000010  loss: 7.4579  loss_lm: 7.4579 (7.5985)  time: 2.2070  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  69/4722]  eta: 2:51:09  lr: 0.000010  loss: 7.5495  loss_lm: 7.5495 (7.7122)  time: 2.2071  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  69/4722]  eta: 2:51:11  lr: 0.000010  loss: 7.5602  loss_lm: 7.5602 (7.7286)  time: 2.2075  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  69/4722]  eta: 2:51:09  lr: 0.000010  loss: 7.9393  loss_lm: 7.5263 (7.6291)  time: 2.2070  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  69/4722]  eta: 2:51:09  lr: 0.000010  loss: 7.6624  loss_lm: 7.5167 (7.5983)  time: 2.2070  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  69/4722]  eta: 2:51:11  lr: 0.000010  loss: 7.7110  loss_lm: 7.7110 (7.6775)  time: 2.2075  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  70/4722]  eta: 4:07:32  lr: 0.000010  loss: 7.8041  loss_lm: 7.6744 (7.7437)  time: 3.1927  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  70/4722]  eta: 4:07:30  lr: 0.000010  loss: 7.8796  loss_lm: 7.5880 (7.7457)  time: 3.1924  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  70/4722]  eta: 4:07:30  lr: 0.000010  loss: 8.0100  loss_lm: 7.6624 (7.6806)  time: 3.1923  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  70/4722]  eta: 4:07:32  lr: 0.000010  loss: 8.1174  loss_lm: 7.7380 (7.7655)  time: 3.1927  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  70/4722]  eta: 4:07:30  lr: 0.000010  loss: 7.2819  loss_lm: 7.4579 (7.5352)  time: 3.1923  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  70/4722]  eta: 4:07:30  lr: 0.000010  loss: 8.1761  loss_lm: 7.6177 (7.7385)  time: 3.1923  data: 0.0000  max mem: 18910
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  71/4722]  eta: 3:45:19  lr: 0.000010  loss: 7.3915  loss_lm: 7.5602 (7.6850)  time: 2.9069  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  71/4722]  eta: 3:45:18  lr: 0.000010  loss: 7.4369  loss_lm: 7.5263 (7.6882)  time: 2.9065  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  71/4722]  eta: 3:45:19  lr: 0.000010  loss: 8.3068  loss_lm: 7.7380 (7.8557)  time: 2.9069  data: 0.0000  max mem: 18865


Train: data epoch: [0]  [  71/4722]  eta: 3:45:18  lr: 0.000010  loss: 7.3366  loss_lm: 7.5495 (7.6775)  time: 2.9066  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  71/4722]  eta: 3:45:18  lr: 0.000010  loss: 7.3936  loss_lm: 7.5167 (7.6328)  time: 2.9065  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  71/4722]  eta: 3:45:18  lr: 0.000010  loss: 7.6996  loss_lm: 7.4579 (7.5626)  time: 2.9065  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  72/4722]  eta: 3:29:26  lr: 0.000010  loss: 7.4787  loss_lm: 7.5263 (7.6583)  time: 2.7026  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  72/4722]  eta: 3:29:27  lr: 0.000010  loss: 7.4642  loss_lm: 7.5495 (7.6470)  time: 2.7026  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  72/4722]  eta: 3:29:26  lr: 0.000010  loss: 7.7519  loss_lm: 7.6624 (7.6498)  time: 2.7025  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  72/4722]  eta: 3:29:28  lr: 0.000010  loss: 7.6720  loss_lm: 7.6720 (7.6831)  time: 2.7028  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  72/4722]  eta: 3:29:28  lr: 0.000010  loss: 7.8535  loss_lm: 7.8535 (7.8554)  time: 2.7028  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  72/4722]  eta: 3:29:26  lr: 0.000010  loss: 7.9144  loss_lm: 7.6228 (7.6128)  time: 2.7025  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  73/4722]  eta: 3:17:31  lr: 0.000010  loss: 8.0602  loss_lm: 7.5263 (7.7085)  time: 2.5494  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  73/4722]  eta: 3:17:32  lr: 0.000010  loss: 7.7387  loss_lm: 7.5495 (7.6585)  time: 2.5494  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  73/4722]  eta: 3:17:31  lr: 0.000010  loss: 7.6629  loss_lm: 7.6624 (7.6514)  time: 2.5493  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  73/4722]  eta: 3:17:32  lr: 0.000010  loss: 7.5088  loss_lm: 7.7380 (7.8121)  time: 2.5496  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  73/4722]  eta: 3:17:33  lr: 0.000010  loss: 7.3958  loss_lm: 7.5602 (7.6472)  time: 2.5496  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  73/4722]  eta: 3:17:31  lr: 0.000010  loss: 7.6664  loss_lm: 7.6228 (7.6195)  time: 2.5493  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  74/4722]  eta: 3:08:15  lr: 0.000010  loss: 7.8758  loss_lm: 7.6629 (7.6764)  time: 2.4302  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  74/4722]  eta: 3:08:16  lr: 0.000010  loss: 7.8909  loss_lm: 7.8535 (7.8208)  time: 2.4305  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  74/4722]  eta: 3:08:16  lr: 0.000010  loss: 7.2343  loss_lm: 7.5602 (7.6013)  time: 2.4305  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  74/4722]  eta: 3:08:15  lr: 0.000010  loss: 7.6206  loss_lm: 7.6177 (7.6988)  time: 2.4303  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [  74/4722]  eta: 3:08:15  lr: 0.000010  loss: 8.1569  loss_lm: 7.6664 (7.6792)  time: 2.4302  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  74/4722]  eta: 3:08:16  lr: 0.000010  loss: 7.8927  loss_lm: 7.5880 (7.6845)  time: 2.4303  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  75/4722]  eta: 3:40:56  lr: 0.000010  loss: 7.5966  loss_lm: 7.5966 (7.6886)  time: 2.8526  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  75/4722]  eta: 3:40:57  lr: 0.000010  loss: 7.6120  loss_lm: 7.5602 (7.6024)  time: 2.8528  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  75/4722]  eta: 3:40:56  lr: 0.000010  loss: 7.7397  loss_lm: 7.6664 (7.6853)  time: 2.8526  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  75/4722]  eta: 3:40:57  lr: 0.000010  loss: 7.5414  loss_lm: 7.7380 (7.7929)  time: 2.8528  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  75/4722]  eta: 3:40:56  lr: 0.000010  loss: 8.0503  loss_lm: 7.5880 (7.7211)  time: 2.8527  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  75/4722]  eta: 3:40:56  lr: 0.000010  loss: 7.2886  loss_lm: 7.6624 (7.6376)  time: 2.8526  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  76/4722]  eta: 3:31:35  lr: 0.000010  loss: 7.8092  loss_lm: 7.6629 (7.6532)  time: 2.7326  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  76/4722]  eta: 3:31:36  lr: 0.000010  loss: 7.5492  loss_lm: 7.5966 (7.6759)  time: 2.7327  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  76/4722]  eta: 3:31:36  lr: 0.000010  loss: 7.4251  loss_lm: 7.7380 (7.7594)  time: 2.7328  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  76/4722]  eta: 3:31:36  lr: 0.000010  loss: 7.2942  loss_lm: 7.5602 (7.5744)  time: 2.7328  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  76/4722]  eta: 3:31:36  lr: 0.000010  loss: 7.9653  loss_lm: 7.7387 (7.7433)  time: 2.7327  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  76/4722]  eta: 3:31:35  lr: 0.000010  loss: 7.9327  loss_lm: 7.6996 (7.7078)  time: 2.7326  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  77/4722]  eta: 3:23:27  lr: 0.000010  loss: 7.4035  loss_lm: 7.6624 (7.6324)  time: 2.6280  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  77/4722]  eta: 3:23:27  lr: 0.000010  loss: 7.8706  loss_lm: 7.5966 (7.6921)  time: 2.6281  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  77/4722]  eta: 3:23:27  lr: 0.000010  loss: 7.6188  loss_lm: 7.5602 (7.5781)  time: 2.6282  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  77/4722]  eta: 3:23:27  lr: 0.000010  loss: 8.1657  loss_lm: 7.7380 (7.7933)  time: 2.6282  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  77/4722]  eta: 3:23:27  lr: 0.000010  loss: 7.5498  loss_lm: 7.5880 (7.7272)  time: 2.6280  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  77/4722]  eta: 3:23:26  lr: 0.000010  loss: 7.8184  loss_lm: 7.6996 (7.7170)  time: 2.6280  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  78/4722]  eta: 3:16:31  lr: 0.000010  loss: 7.4348  loss_lm: 7.5966 (7.6723)  time: 2.5391  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  78/4722]  eta: 3:16:31  lr: 0.000010  loss: 7.5624  loss_lm: 7.6624 (7.6270)  time: 2.5391  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  78/4722]  eta: 3:16:32  lr: 0.000010  loss: 7.1463  loss_lm: 7.7380 (7.7435)  time: 2.5392  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  78/4722]  eta: 3:16:31  lr: 0.000010  loss: 8.0821  loss_lm: 7.7387 (7.7545)  time: 2.5391  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  78/4722]  eta: 3:16:32  lr: 0.000010  loss: 7.7648  loss_lm: 7.6120 (7.5924)  time: 2.5392  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  78/4722]  eta: 3:16:31  lr: 0.000010  loss: 7.2569  loss_lm: 7.6996 (7.6816)  time: 2.5390  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  79/4722]  eta: 3:10:37  lr: 0.000010  loss: 7.4189  loss_lm: 7.5624 (7.6121)  time: 2.4635  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  79/4722]  eta: 3:10:38  lr: 0.000010  loss: 7.2001  loss_lm: 7.5492 (7.6386)  time: 2.4635  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  79/4722]  eta: 3:10:38  lr: 0.000010  loss: 7.6745  loss_lm: 7.7110 (7.7386)  time: 2.4636  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  79/4722]  eta: 3:10:37  lr: 0.000010  loss: 7.8015  loss_lm: 7.6996 (7.6902)  time: 2.4634  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  79/4722]  eta: 3:10:38  lr: 0.000010  loss: 7.4067  loss_lm: 7.5602 (7.5792)  time: 2.4636  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  79/4722]  eta: 3:10:37  lr: 0.000010  loss: 7.1434  loss_lm: 7.5880 (7.7108)  time: 2.4635  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  80/4722]  eta: 3:33:49  lr: 0.000010  loss: 7.4069  loss_lm: 7.5602 (7.5677)  time: 2.7638  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  80/4722]  eta: 3:33:48  lr: 0.000010  loss: 7.2625  loss_lm: 7.6996 (7.6616)  time: 2.7636  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  80/4722]  eta: 3:33:49  lr: 0.000010  loss: 7.4499  loss_lm: 7.5880 (7.6934)  time: 2.7637  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  80/4722]  eta: 3:33:49  lr: 0.000010  loss: 7.7127  loss_lm: 7.5966 (7.6435)  time: 2.7637  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [  80/4722]  eta: 3:33:49  lr: 0.000010  loss: 7.5545  loss_lm: 7.7110 (7.7263)  time: 2.7638  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  80/4722]  eta: 3:33:48  lr: 0.000010  loss: 7.7219  loss_lm: 7.6624 (7.6195)  time: 2.7637  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  81/4722]  eta: 3:27:36  lr: 0.000010  loss: 7.2360  loss_lm: 7.5624 (7.5955)  time: 2.6841  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  81/4722]  eta: 3:27:36  lr: 0.000010  loss: 7.9616  loss_lm: 7.5966 (7.6634)  time: 2.6841  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  81/4722]  eta: 3:27:37  lr: 0.000010  loss: 7.3017  loss_lm: 7.6745 (7.6998)  time: 2.6842  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  81/4722]  eta: 3:27:37  lr: 0.000010  loss: 7.3230  loss_lm: 7.4863 (7.5524)  time: 2.6842  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  81/4722]  eta: 3:27:36  lr: 0.000010  loss: 7.3354  loss_lm: 7.5498 (7.6710)  time: 2.6841  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  81/4722]  eta: 3:27:36  lr: 0.000010  loss: 7.5912  loss_lm: 7.6664 (7.6572)  time: 2.6840  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  82/4722]  eta: 3:22:05  lr: 0.000010  loss: 7.3685  loss_lm: 7.5624 (7.5821)  time: 2.6132  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  82/4722]  eta: 3:22:05  lr: 0.000010  loss: 7.2717  loss_lm: 7.5966 (7.6404)  time: 2.6133  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  82/4722]  eta: 3:22:05  lr: 0.000010  loss: 7.1288  loss_lm: 7.6745 (7.6662)  time: 2.6133  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  82/4722]  eta: 3:22:05  lr: 0.000010  loss: 7.6427  loss_lm: 7.5880 (7.6694)  time: 2.6132  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  82/4722]  eta: 3:22:05  lr: 0.000010  loss: 7.2076  loss_lm: 7.4863 (7.5321)  time: 2.6133  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  82/4722]  eta: 3:22:05  lr: 0.000010  loss: 7.9947  loss_lm: 7.6996 (7.6771)  time: 2.6132  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  83/4722]  eta: 3:17:09  lr: 0.000010  loss: 7.8115  loss_lm: 7.6745 (7.6743)  time: 2.5500  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  83/4722]  eta: 3:17:09  lr: 0.000010  loss: 7.4786  loss_lm: 7.5167 (7.5764)  time: 2.5499  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  83/4722]  eta: 3:17:09  lr: 0.000010  loss: 7.4840  loss_lm: 7.5492 (7.6317)  time: 2.5500  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  83/4722]  eta: 3:17:09  lr: 0.000010  loss: 7.9279  loss_lm: 7.4863 (7.5541)  time: 2.5500  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  83/4722]  eta: 3:17:09  lr: 0.000010  loss: 7.8167  loss_lm: 7.5880 (7.6776)  time: 2.5499  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  83/4722]  eta: 3:17:08  lr: 0.000010  loss: 7.3649  loss_lm: 7.6664 (7.6597)  time: 2.5499  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  84/4722]  eta: 3:12:43  lr: 0.000010  loss: 7.6029  loss_lm: 7.5624 (7.5778)  time: 2.4933  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  84/4722]  eta: 3:12:43  lr: 0.000010  loss: 7.5518  loss_lm: 7.5518 (7.6275)  time: 2.4933  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  84/4722]  eta: 3:12:43  lr: 0.000010  loss: 7.7348  loss_lm: 7.6427 (7.6806)  time: 2.4933  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  84/4722]  eta: 3:12:44  lr: 0.000010  loss: 7.3407  loss_lm: 7.6745 (7.6567)  time: 2.4934  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  84/4722]  eta: 3:12:44  lr: 0.000010  loss: 7.3598  loss_lm: 7.4863 (7.5439)  time: 2.4934  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  84/4722]  eta: 3:12:43  lr: 0.000010  loss: 7.0908  loss_lm: 7.6664 (7.6298)  time: 2.4932  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  85/4722]  eta: 3:30:02  lr: 0.000010  loss: 7.3342  loss_lm: 7.5492 (7.6128)  time: 2.7179  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  85/4722]  eta: 3:30:02  lr: 0.000010  loss: 7.4535  loss_lm: 7.5880 (7.6692)  time: 2.7178  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  85/4722]  eta: 3:30:03  lr: 0.000010  loss: 7.3188  loss_lm: 7.4069 (7.5326)  time: 2.7179  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  85/4722]  eta: 3:30:03  lr: 0.000010  loss: 7.5536  loss_lm: 7.5545 (7.6516)  time: 2.7179  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  85/4722]  eta: 3:30:02  lr: 0.000010  loss: 7.7079  loss_lm: 7.6664 (7.6337)  time: 2.7178  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  85/4722]  eta: 3:30:02  lr: 0.000010  loss: 7.5602  loss_lm: 7.5602 (7.5769)  time: 2.7178  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  86/4722]  eta: 3:25:27  lr: 0.000010  loss: 8.0657  loss_lm: 7.5624 (7.6002)  time: 2.5743  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  86/4722]  eta: 3:25:27  lr: 0.000010  loss: 7.1055  loss_lm: 7.5263 (7.5887)  time: 2.5743  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  86/4722]  eta: 3:25:27  lr: 0.000010  loss: 7.5190  loss_lm: 7.5536 (7.6453)  time: 2.5743  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  86/4722]  eta: 3:25:27  lr: 0.000010  loss: 7.6956  loss_lm: 7.4069 (7.5404)  time: 2.5743  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  86/4722]  eta: 3:25:27  lr: 0.000010  loss: 7.3600  loss_lm: 7.5880 (7.6545)  time: 2.5743  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  86/4722]  eta: 3:25:27  lr: 0.000010  loss: 7.7304  loss_lm: 7.6664 (7.6383)  time: 2.5743  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  87/4722]  eta: 3:21:15  lr: 0.000010  loss: 7.2284  loss_lm: 7.5624 (7.5833)  time: 2.5734  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  87/4722]  eta: 3:21:15  lr: 0.000010  loss: 7.5588  loss_lm: 7.5492 (7.5873)  time: 2.5734  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  87/4722]  eta: 3:21:15  lr: 0.000010  loss: 7.5540  loss_lm: 7.5536 (7.6411)  time: 2.5733  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  87/4722]  eta: 3:21:15  lr: 0.000010  loss: 7.8270  loss_lm: 7.4069 (7.5534)  time: 2.5733  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  87/4722]  eta: 3:21:15  lr: 0.000010  loss: 7.4158  loss_lm: 7.5498 (7.6436)  time: 2.5733  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  87/4722]  eta: 3:21:15  lr: 0.000010  loss: 7.6187  loss_lm: 7.6664 (7.6374)  time: 2.5733  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  88/4722]  eta: 3:17:25  lr: 0.000010  loss: 7.6811  loss_lm: 7.5624 (7.5875)  time: 2.5724  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  88/4722]  eta: 3:17:25  lr: 0.000010  loss: 7.0254  loss_lm: 7.5492 (7.5629)  time: 2.5724  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  88/4722]  eta: 3:17:25  lr: 0.000010  loss: 7.5132  loss_lm: 7.5495 (7.6380)  time: 2.5724  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  88/4722]  eta: 3:17:25  lr: 0.000010  loss: 7.5269  loss_lm: 7.5536 (7.6361)  time: 2.5723  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  88/4722]  eta: 3:17:25  lr: 0.000010  loss: 7.6306  loss_lm: 7.4069 (7.5568)  time: 2.5724  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  88/4722]  eta: 3:17:25  lr: 0.000010  loss: 7.4193  loss_lm: 7.6664 (7.6279)  time: 2.5723  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  89/4722]  eta: 3:13:54  lr: 0.000010  loss: 7.3837  loss_lm: 7.5602 (7.5790)  time: 2.5721  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  89/4722]  eta: 3:13:54  lr: 0.000010  loss: 7.3124  loss_lm: 7.4840 (7.5524)  time: 2.5721  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  89/4722]  eta: 3:13:54  lr: 0.000010  loss: 7.4921  loss_lm: 7.5414 (7.6301)  time: 2.5720  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  89/4722]  eta: 3:13:54  lr: 0.000010  loss: 7.4816  loss_lm: 7.4069 (7.5536)  time: 2.5720  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  89/4722]  eta: 3:13:54  lr: 0.000010  loss: 7.8793  loss_lm: 7.5498 (7.6480)  time: 2.5720  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  89/4722]  eta: 3:13:54  lr: 0.000010  loss: 7.6647  loss_lm: 7.6664 (7.6295)  time: 2.5720  data: 0.0000  max mem: 18865
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:03:26.550600] Starting job[2023-11-13 00:03:26.550601] Starting job

[2023-11-13 00:03:26.550707] Starting job
[2023-11-13 00:03:26.565114] Starting job
[2023-11-13 00:03:26.566888] Starting job
| distributed init (rank 8, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 10, world 12): env://
| distributed init (rank 9, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:03:30.616350] Starting job
| distributed init (rank 11, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  86/4722]  eta: 5:08:35  lr: 0.000010  loss: 7.6938  loss_lm: 7.6938 (7.6938)  time: 3.9938  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [  86/4722]  eta: 5:08:38  lr: 0.000010  loss: 8.0537  loss_lm: 8.0537 (8.0537)  time: 3.9946  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  86/4722]  eta: 5:08:39  lr: 0.000010  loss: 7.1085  loss_lm: 7.1085 (7.1085)  time: 3.9947  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  86/4722]  eta: 5:08:31  lr: 0.000010  loss: 7.7244  loss_lm: 7.7244 (7.7244)  time: 3.9931  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  86/4722]  eta: 5:08:39  lr: 0.000010  loss: 7.5384  loss_lm: 7.5384 (7.5384)  time: 3.9947  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  86/4722]  eta: 5:08:32  lr: 0.000010  loss: 7.3511  loss_lm: 7.3511 (7.3511)  time: 3.9932  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  87/4722]  eta: 3:31:43  lr: 0.000010  loss: 7.8587  loss_lm: 7.6938 (7.7763)  time: 2.7407  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  87/4722]  eta: 3:31:41  lr: 0.000010  loss: 7.6260  loss_lm: 7.6260 (7.6752)  time: 2.7403  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  87/4722]  eta: 3:31:45  lr: 0.000010  loss: 7.5067  loss_lm: 7.1085 (7.3076)  time: 2.7411  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  87/4722]  eta: 3:31:44  lr: 0.000010  loss: 7.2017  loss_lm: 7.2017 (7.6277)  time: 2.7411  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  87/4722]  eta: 3:31:45  lr: 0.000010  loss: 7.5678  loss_lm: 7.5384 (7.5531)  time: 2.7411  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  87/4722]  eta: 3:31:41  lr: 0.000010  loss: 7.4208  loss_lm: 7.3511 (7.3860)  time: 2.7403  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  88/4722]  eta: 2:59:05  lr: 0.000010  loss: 7.4366  loss_lm: 7.6260 (7.5956)  time: 2.3188  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  88/4722]  eta: 2:59:06  lr: 0.000010  loss: 7.6509  loss_lm: 7.6938 (7.7345)  time: 2.3191  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  88/4722]  eta: 2:59:07  lr: 0.000010  loss: 7.6921  loss_lm: 7.6921 (7.6492)  time: 2.3193  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  88/4722]  eta: 2:59:07  lr: 0.000010  loss: 7.0328  loss_lm: 7.1085 (7.2160)  time: 2.3194  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  88/4722]  eta: 2:59:07  lr: 0.000010  loss: 7.5272  loss_lm: 7.5384 (7.5445)  time: 2.3193  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  88/4722]  eta: 2:59:05  lr: 0.000010  loss: 7.5184  loss_lm: 7.4208 (7.4301)  time: 2.3188  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  89/4722]  eta: 2:42:44  lr: 0.000010  loss: 7.6888  loss_lm: 7.6260 (7.6189)  time: 2.1076  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  89/4722]  eta: 2:42:45  lr: 0.000010  loss: 7.5008  loss_lm: 7.6509 (7.6760)  time: 2.1078  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  89/4722]  eta: 2:42:46  lr: 0.000010  loss: 7.3619  loss_lm: 7.3619 (7.5774)  time: 2.1080  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  89/4722]  eta: 2:42:46  lr: 0.000010  loss: 7.3018  loss_lm: 7.1085 (7.2374)  time: 2.1080  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  89/4722]  eta: 2:42:46  lr: 0.000010  loss: 7.4712  loss_lm: 7.5272 (7.5262)  time: 2.1080  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  89/4722]  eta: 2:42:44  lr: 0.000010  loss: 7.8810  loss_lm: 7.4208 (7.5429)  time: 2.1076  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  90/4722]  eta: 4:06:08  lr: 0.000010  loss: 7.2615  loss_lm: 7.5272 (7.4732)  time: 3.1883  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  90/4722]  eta: 4:06:06  lr: 0.000010  loss: 7.3648  loss_lm: 7.4208 (7.5072)  time: 3.1880  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  90/4722]  eta: 4:06:08  lr: 0.000010  loss: 7.1858  loss_lm: 7.3619 (7.4990)  time: 3.1883  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  90/4722]  eta: 4:06:07  lr: 0.000010  loss: 7.5886  loss_lm: 7.6509 (7.6586)  time: 3.1882  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  90/4722]  eta: 4:06:08  lr: 0.000010  loss: 7.9397  loss_lm: 7.3018 (7.3779)  time: 3.1883  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  90/4722]  eta: 4:06:07  lr: 0.000010  loss: 7.3739  loss_lm: 7.6260 (7.5699)  time: 3.1881  data: 0.0000  max mem: 18866

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  91/4722]  eta: 3:44:10  lr: 0.000010  loss: 7.4456  loss_lm: 7.5886 (7.6231)  time: 2.9046  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  91/4722]  eta: 3:44:10  lr: 0.000010  loss: 7.1935  loss_lm: 7.4366 (7.5072)  time: 2.9044  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  91/4722]  eta: 3:44:10  lr: 0.000010  loss: 7.3206  loss_lm: 7.3648 (7.4761)  time: 2.9044  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  91/4722]  eta: 3:44:11  lr: 0.000010  loss: 7.2502  loss_lm: 7.2502 (7.3566)  time: 2.9046  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  91/4722]  eta: 3:44:11  lr: 0.000010  loss: 7.3590  loss_lm: 7.3590 (7.4757)  time: 2.9047  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  91/4722]  eta: 3:44:11  lr: 0.000010  loss: 7.3197  loss_lm: 7.4712 (7.4476)  time: 2.9047  data: 0.0000  max mem: 18865

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  92/4722]  eta: 3:28:23  lr: 0.000010  loss: 7.3459  loss_lm: 7.5886 (7.5835)  time: 2.7005  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  92/4722]  eta: 3:28:23  lr: 0.000010  loss: 7.6438  loss_lm: 7.3018 (7.3976)  time: 2.7006  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  92/4722]  eta: 3:28:23  lr: 0.000010  loss: 7.3291  loss_lm: 7.3590 (7.4547)  time: 2.7006  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  92/4722]  eta: 3:28:23  lr: 0.000010  loss: 7.4548  loss_lm: 7.4712 (7.4487)  time: 2.7006  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  92/4722]  eta: 3:28:22  lr: 0.000010  loss: 7.6579  loss_lm: 7.6260 (7.5287)  time: 2.7004  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  92/4722]  eta: 3:28:22  lr: 0.000010  loss: 7.4998  loss_lm: 7.4208 (7.4795)  time: 2.7004  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  93/4722]  eta: 3:16:33  lr: 0.000010  loss: 7.4812  loss_lm: 7.3590 (7.4581)  time: 2.5477  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  93/4722]  eta: 3:16:33  lr: 0.000010  loss: 7.4764  loss_lm: 7.5008 (7.5701)  time: 2.5477  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  93/4722]  eta: 3:16:32  lr: 0.000010  loss: 7.4655  loss_lm: 7.4208 (7.4778)  time: 2.5475  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  93/4722]  eta: 3:16:33  lr: 0.000010  loss: 7.3445  loss_lm: 7.3018 (7.3910)  time: 2.5477  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  93/4722]  eta: 3:16:33  lr: 0.000010  loss: 7.3830  loss_lm: 7.4548 (7.4404)  time: 2.5477  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  93/4722]  eta: 3:16:32  lr: 0.000010  loss: 7.2688  loss_lm: 7.4366 (7.4962)  time: 2.5476  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  94/4722]  eta: 3:07:18  lr: 0.000010  loss: 7.2320  loss_lm: 7.3018 (7.3733)  time: 2.4284  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  94/4722]  eta: 3:07:18  lr: 0.000010  loss: 7.3946  loss_lm: 7.5008 (7.5506)  time: 2.4284  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  94/4722]  eta: 3:07:18  lr: 0.000010  loss: 7.6892  loss_lm: 7.4655 (7.5013)  time: 2.4283  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  94/4722]  eta: 3:07:18  lr: 0.000010  loss: 7.1471  loss_lm: 7.3590 (7.4235)  time: 2.4284  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  94/4722]  eta: 3:07:18  lr: 0.000010  loss: 7.1120  loss_lm: 7.4548 (7.4039)  time: 2.4284  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  94/4722]  eta: 3:07:18  lr: 0.000010  loss: 7.6640  loss_lm: 7.6260 (7.5149)  time: 2.4283  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  95/4722]  eta: 3:38:32  lr: 0.000010  loss: 7.3701  loss_lm: 7.4208 (7.4881)  time: 2.8339  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  95/4722]  eta: 3:38:32  lr: 0.000010  loss: 7.5123  loss_lm: 7.4548 (7.4148)  time: 2.8340  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  95/4722]  eta: 3:38:32  lr: 0.000010  loss: 7.2914  loss_lm: 7.4366 (7.4925)  time: 2.8339  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  95/4722]  eta: 3:38:32  lr: 0.000010  loss: 7.1951  loss_lm: 7.3291 (7.4007)  time: 2.8340  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  95/4722]  eta: 3:38:32  lr: 0.000010  loss: 7.8000  loss_lm: 7.5008 (7.5755)  time: 2.8340  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [  95/4722]  eta: 3:38:32  lr: 0.000010  loss: 7.0362  loss_lm: 7.2502 (7.3396)  time: 2.8340  data: 0.0000  max mem: 18910
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  96/4722]  eta: 3:28:58  lr: 0.000010  loss: 7.5364  loss_lm: 7.5364 (7.5720)  time: 2.7104  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [  96/4722]  eta: 3:28:57  lr: 0.000010  loss: 7.4906  loss_lm: 7.4655 (7.4884)  time: 2.7103  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  96/4722]  eta: 3:28:58  lr: 0.000010  loss: 7.7255  loss_lm: 7.3018 (7.3747)  time: 2.7104  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  96/4722]  eta: 3:28:58  lr: 0.000010  loss: 6.9399  loss_lm: 7.4548 (7.3716)  time: 2.7104  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  96/4722]  eta: 3:28:57  lr: 0.000010  loss: 7.3187  loss_lm: 7.4366 (7.4767)  time: 2.7103  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  96/4722]  eta: 3:28:58  lr: 0.000010  loss: 7.7933  loss_lm: 7.3590 (7.4364)  time: 2.7104  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  97/4722]  eta: 3:20:59  lr: 0.000010  loss: 7.4455  loss_lm: 7.3018 (7.3806)  time: 2.6074  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  97/4722]  eta: 3:20:59  lr: 0.000010  loss: 7.7252  loss_lm: 7.5364 (7.5847)  time: 2.6074  data: 0.0000  max mem: 18961Train: data epoch: [0]  [  97/4722]  eta: 3:20:59  lr: 0.000010  loss: 8.0616  loss_lm: 7.3590 (7.4885)  time: 2.6075  data: 0.0000  max mem: 18888

Train: data epoch: [0]  [  97/4722]  eta: 3:20:58  lr: 0.000010  loss: 7.2477  loss_lm: 7.4208 (7.4683)  time: 2.6074  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  97/4722]  eta: 3:20:59  lr: 0.000010  loss: 7.7073  loss_lm: 7.4548 (7.3996)  time: 2.6075  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  97/4722]  eta: 3:20:58  lr: 0.000010  loss: 7.5935  loss_lm: 7.4366 (7.4864)  time: 2.6073  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  98/4722]  eta: 3:14:13  lr: 0.000010  loss: 7.3672  loss_lm: 7.5364 (7.5680)  time: 2.5203  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [  98/4722]  eta: 3:14:14  lr: 0.000010  loss: 6.9303  loss_lm: 7.3590 (7.4455)  time: 2.5203  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  98/4722]  eta: 3:14:13  lr: 0.000010  loss: 7.6498  loss_lm: 7.4712 (7.4188)  time: 2.5203  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  98/4722]  eta: 3:14:13  lr: 0.000010  loss: 7.6028  loss_lm: 7.3445 (7.3977)  time: 2.5203  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  98/4722]  eta: 3:14:13  lr: 0.000010  loss: 7.1537  loss_lm: 7.4208 (7.4441)  time: 2.5202  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  98/4722]  eta: 3:14:13  lr: 0.000010  loss: 7.7537  loss_lm: 7.5935 (7.5070)  time: 2.5202  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  99/4722]  eta: 3:08:27  lr: 0.000010  loss: 7.3632  loss_lm: 7.4366 (7.4967)  time: 2.4459  data: 0.0000  max mem: 18876Train: data epoch: [0]  [  99/4722]  eta: 3:08:27  lr: 0.000010  loss: 7.0532  loss_lm: 7.5008 (7.5312)  time: 2.4460  data: 0.0000  max mem: 18961

Train: data epoch: [0]  [  99/4722]  eta: 3:08:27  lr: 0.000010  loss: 7.4615  loss_lm: 7.3445 (7.4022)  time: 2.4460  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  99/4722]  eta: 3:08:27  lr: 0.000010  loss: 7.3922  loss_lm: 7.3590 (7.4417)  time: 2.4460  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  99/4722]  eta: 3:08:27  lr: 0.000010  loss: 7.0504  loss_lm: 7.4548 (7.3925)  time: 2.4460  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  99/4722]  eta: 3:08:27  lr: 0.000010  loss: 7.3362  loss_lm: 7.3701 (7.4364)  time: 2.4459  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:05  lr: 0.000010  loss: 7.6072  loss_lm: 7.4208 (7.4478)  time: 2.7402  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 100/4722]  eta: 3:31:05  lr: 0.000010  loss: 7.0416  loss_lm: 7.5008 (7.4986)  time: 2.7403  data: 0.0000  max mem: 18961

Train: data epoch: [0]  [ 100/4722]  eta: 3:31:05  lr: 0.000010  loss: 7.1796  loss_lm: 7.4548 (7.3783)  time: 2.7403  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:05  lr: 0.000010  loss: 7.5132  loss_lm: 7.5132 (7.4978)  time: 2.7402  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:05  lr: 0.000010  loss: 7.4271  loss_lm: 7.3619 (7.4407)  time: 2.7403  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:05  lr: 0.000010  loss: 7.5825  loss_lm: 7.4455 (7.4143)  time: 2.7403  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 101/4722]  eta: 3:24:58  lr: 0.000010  loss: 7.2662  loss_lm: 7.4764 (7.4841)  time: 2.6614  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [ 101/4722]  eta: 3:24:58  lr: 0.000010  loss: 7.3698  loss_lm: 7.3830 (7.3778)  time: 2.6615  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 101/4722]  eta: 3:24:58  lr: 0.000010  loss: 7.5558  loss_lm: 7.4455 (7.4231)  time: 2.6614  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 101/4722]  eta: 3:24:58  lr: 0.000010  loss: 7.2726  loss_lm: 7.3590 (7.4302)  time: 2.6615  data: 0.0000  max mem: 18888Train: data epoch: [0]  [ 101/4722]  eta: 3:24:58  lr: 0.000010  loss: 7.6155  loss_lm: 7.5132 (7.5052)  time: 2.6614  data: 0.0000  max mem: 18876

Train: data epoch: [0]  [ 101/4722]  eta: 3:24:58  lr: 0.000010  loss: 7.6291  loss_lm: 7.4208 (7.4591)  time: 2.6614  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 102/4722]  eta: 3:19:32  lr: 0.000010  loss: 7.3510  loss_lm: 7.5132 (7.4961)  time: 2.5915  data: 0.0000  max mem: 18876Train: data epoch: [0]  [ 102/4722]  eta: 3:19:33  lr: 0.000010  loss: 7.4855  loss_lm: 7.4548 (7.3841)  time: 2.5916  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 102/4722]  eta: 3:19:33  lr: 0.000010  loss: 7.1799  loss_lm: 7.4455 (7.4088)  time: 2.5916  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 102/4722]  eta: 3:19:33  lr: 0.000010  loss: 7.7794  loss_lm: 7.5008 (7.5014)  time: 2.5916  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [ 102/4722]  eta: 3:19:33  lr: 0.000010  loss: 7.4460  loss_lm: 7.3619 (7.4312)  time: 2.5916  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 102/4722]  eta: 3:19:32  lr: 0.000010  loss: 7.5149  loss_lm: 7.4655 (7.4624)  time: 2.5915  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.4238  loss_lm: 7.4366 (7.4921)  time: 2.5297  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.4218  loss_lm: 7.4218 (7.4602)  time: 2.5297  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.4430  loss_lm: 7.4430 (7.4107)  time: 2.5297  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.4784  loss_lm: 7.4784 (7.5002)  time: 2.5298  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.2943  loss_lm: 7.3830 (7.3791)  time: 2.5298  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.8629  loss_lm: 7.3619 (7.4551)  time: 2.5298  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:25  lr: 0.000010  loss: 7.1475  loss_lm: 7.4430 (7.3968)  time: 2.4741  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:25  lr: 0.000010  loss: 7.6197  loss_lm: 7.4655 (7.4686)  time: 2.4741  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 104/4722]  eta: 3:10:25  lr: 0.000010  loss: 7.5694  loss_lm: 7.5132 (7.4962)  time: 2.4741  data: 0.0000  max mem: 18876

Train: data epoch: [0]  [ 104/4722]  eta: 3:10:25  lr: 0.000010  loss: 7.3044  loss_lm: 7.4784 (7.4899)  time: 2.4742  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:25  lr: 0.000010  loss: 6.8475  loss_lm: 7.3619 (7.4232)  time: 2.4742  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:25  lr: 0.000010  loss: 7.3986  loss_lm: 7.3986 (7.3802)  time: 2.4742  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 7.4861  loss_lm: 7.4655 (7.4694)  time: 2.6884  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 7.4271  loss_lm: 7.4366 (7.4927)  time: 2.6884  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 7.0343  loss_lm: 7.4764 (7.4671)  time: 2.6885  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 7.1417  loss_lm: 7.3830 (7.3682)  time: 2.6885  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 7.2054  loss_lm: 7.3590 (7.4123)  time: 2.6885  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 7.0537  loss_lm: 7.3445 (7.3797)  time: 2.6885  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 106/4722]  eta: 3:22:39  lr: 0.000010  loss: 7.4611  loss_lm: 7.4430 (7.3836)  time: 2.5661  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 106/4722]  eta: 3:22:39  lr: 0.000010  loss: 7.3461  loss_lm: 7.4271 (7.4857)  time: 2.5662  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [ 106/4722]  eta: 3:22:39  lr: 0.000010  loss: 7.4920  loss_lm: 7.3590 (7.4161)  time: 2.5662  data: 0.0000  max mem: 18888Train: data epoch: [0]  [ 106/4722]  eta: 3:22:39  lr: 0.000010  loss: 7.2631  loss_lm: 7.4655 (7.4596)  time: 2.5662  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 106/4722]  eta: 3:22:39  lr: 0.000010  loss: 7.3893  loss_lm: 7.4456 (7.4634)  time: 2.5662  data: 0.0000  max mem: 18961


Train: data epoch: [0]  [ 106/4722]  eta: 3:22:39  lr: 0.000010  loss: 7.2509  loss_lm: 7.3698 (7.3626)  time: 2.5662  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:49  lr: 0.000010  loss: 7.2394  loss_lm: 7.3445 (7.3770)  time: 2.5692  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:48  lr: 0.000010  loss: 7.6928  loss_lm: 7.4271 (7.4951)  time: 2.5693  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:48  lr: 0.000010  loss: 6.8780  loss_lm: 7.4655 (7.4332)  time: 2.5693  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:49  lr: 0.000010  loss: 7.2847  loss_lm: 7.3946 (7.4553)  time: 2.5693  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:49  lr: 0.000010  loss: 7.5494  loss_lm: 7.3619 (7.4221)  time: 2.5692  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:49  lr: 0.000010  loss: 7.5849  loss_lm: 7.3698 (7.3727)  time: 2.5692  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 108/4722]  eta: 3:15:12  lr: 0.000010  loss: 7.3644  loss_lm: 7.3644 (7.3765)  time: 2.5714  data: 0.0000  max mem: 18910Train: data epoch: [0]  [ 108/4722]  eta: 3:15:12  lr: 0.000010  loss: 7.4106  loss_lm: 7.3946 (7.4533)  time: 2.5714  data: 0.0000  max mem: 18961

Train: data epoch: [0]  [ 108/4722]  eta: 3:15:12  lr: 0.000010  loss: 7.3336  loss_lm: 7.4238 (7.4881)  time: 2.5714  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [ 108/4722]  eta: 3:15:12  lr: 0.000010  loss: 7.2198  loss_lm: 7.4218 (7.4239)  time: 2.5714  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 108/4722]  eta: 3:15:12  lr: 0.000010  loss: 7.2576  loss_lm: 7.3197 (7.3677)  time: 2.5714  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 108/4722]  eta: 3:15:12  lr: 0.000010  loss: 6.8556  loss_lm: 7.3590 (7.3975)  time: 2.5714  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 109/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.6478  loss_lm: 7.4238 (7.4948)  time: 2.5741  data: 0.0000  max mem: 18876Train: data epoch: [0]  [ 109/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.3505  loss_lm: 7.3644 (7.3754)  time: 2.5740  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [ 109/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.4668  loss_lm: 7.3590 (7.4004)  time: 2.5740  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [ 109/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.5388  loss_lm: 7.3946 (7.4569)  time: 2.5741  data: 0.0000  max mem: 18961
Train: data epoch: [0]  [ 109/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.4656  loss_lm: 7.3197 (7.3718)  time: 2.5740  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 109/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.2727  loss_lm: 7.3701 (7.4176)  time: 2.5741  data: 0.0000  max mem: 18864
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:10:19.843171] Starting job[2023-11-13 00:10:19.843177] Starting job[2023-11-13 00:10:19.843172] Starting job


[2023-11-13 00:10:19.843258] Starting job
[2023-11-13 00:10:19.843286] Starting job
[2023-11-13 00:10:19.845990] Starting job
| distributed init (rank 10, world 12): env://
| distributed init (rank 8, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 9, world 12): env://
| distributed init (rank 11, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 106/4722]  eta: 4:55:24  lr: 0.000010  loss: 7.2632  loss_lm: 7.2632 (7.2632)  time: 3.8398  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 106/4722]  eta: 4:55:13  lr: 0.000010  loss: 7.4037  loss_lm: 7.4037 (7.4037)  time: 3.8375  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 106/4722]  eta: 4:55:22  lr: 0.000010  loss: 7.3569  loss_lm: 7.3569 (7.3569)  time: 3.8394  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 106/4722]  eta: 4:55:35  lr: 0.000010  loss: 7.2188  loss_lm: 7.2188 (7.2188)  time: 3.8421  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 106/4722]  eta: 4:55:20  lr: 0.000010  loss: 7.5026  loss_lm: 7.5026 (7.5026)  time: 3.8389  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 106/4722]  eta: 4:55:23  lr: 0.000010  loss: 7.4454  loss_lm: 7.4454 (7.4454)  time: 3.8396  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:22  lr: 0.000010  loss: 6.8816  loss_lm: 6.8816 (7.0724)  time: 2.6572  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:17  lr: 0.000010  loss: 7.2171  loss_lm: 7.2171 (7.3104)  time: 2.6560  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:28  lr: 0.000010  loss: 7.5665  loss_lm: 7.2188 (7.3927)  time: 2.6584  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:22  lr: 0.000010  loss: 7.6756  loss_lm: 7.3569 (7.5162)  time: 2.6570  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:20  lr: 0.000010  loss: 7.5295  loss_lm: 7.5026 (7.5161)  time: 2.6567  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:21  lr: 0.000010  loss: 7.2406  loss_lm: 7.2406 (7.3430)  time: 2.6570  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 108/4722]  eta: 2:54:29  lr: 0.000010  loss: 7.1758  loss_lm: 7.1758 (7.1069)  time: 2.2690  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 108/4722]  eta: 2:54:25  lr: 0.000010  loss: 7.3979  loss_lm: 7.3979 (7.3396)  time: 2.2683  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 108/4722]  eta: 2:54:32  lr: 0.000010  loss: 7.2372  loss_lm: 7.2372 (7.3408)  time: 2.2698  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 108/4722]  eta: 2:54:28  lr: 0.000010  loss: 7.3522  loss_lm: 7.3569 (7.4615)  time: 2.2689  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 108/4722]  eta: 2:54:27  lr: 0.000010  loss: 6.8649  loss_lm: 7.5026 (7.2990)  time: 2.2687  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 108/4722]  eta: 2:54:28  lr: 0.000010  loss: 7.3595  loss_lm: 7.3595 (7.3485)  time: 2.2689  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:07  lr: 0.000010  loss: 7.3104  loss_lm: 7.1758 (7.1577)  time: 2.0697  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:04  lr: 0.000010  loss: 7.5181  loss_lm: 7.3979 (7.3842)  time: 2.0691  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:06  lr: 0.000010  loss: 7.6530  loss_lm: 7.3569 (7.5094)  time: 2.0696  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:10  lr: 0.000010  loss: 7.4898  loss_lm: 7.2372 (7.3781)  time: 2.0702  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:06  lr: 0.000010  loss: 7.4640  loss_lm: 7.4640 (7.3402)  time: 2.0694  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:06  lr: 0.000010  loss: 7.3690  loss_lm: 7.3595 (7.3536)  time: 2.0695  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 110/4722]  eta: 3:52:17  lr: 0.000010  loss: 7.2584  loss_lm: 7.3569 (7.4592)  time: 3.0220  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 110/4722]  eta: 3:52:15  lr: 0.000010  loss: 7.2369  loss_lm: 7.3979 (7.3547)  time: 3.0216  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 110/4722]  eta: 3:52:19  lr: 0.000010  loss: 7.1080  loss_lm: 7.2372 (7.3241)  time: 3.0225  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 110/4722]  eta: 3:52:16  lr: 0.000010  loss: 7.4082  loss_lm: 7.4640 (7.3538)  time: 3.0219  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 110/4722]  eta: 3:52:17  lr: 0.000010  loss: 7.3063  loss_lm: 7.3595 (7.3442)  time: 3.0220  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 110/4722]  eta: 3:52:18  lr: 0.000010  loss: 7.2770  loss_lm: 7.2632 (7.1816)  time: 3.0221  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 111/4722]  eta: 3:32:33  lr: 0.000010  loss: 6.9543  loss_lm: 7.1758 (7.1437)  time: 2.7659  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 111/4722]  eta: 3:32:31  lr: 0.000010  loss: 7.6560  loss_lm: 7.3979 (7.4049)  time: 2.7655  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 111/4722]  eta: 3:32:33  lr: 0.000010  loss: 6.9487  loss_lm: 7.3522 (7.3741)  time: 2.7659  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 111/4722]  eta: 3:32:35  lr: 0.000010  loss: 7.5531  loss_lm: 7.2372 (7.3622)  time: 2.7663  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 111/4722]  eta: 3:32:32  lr: 0.000010  loss: 7.4604  loss_lm: 7.4604 (7.3716)  time: 2.7657  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 111/4722]  eta: 3:32:32  lr: 0.000010  loss: 6.6611  loss_lm: 7.3063 (7.2303)  time: 2.7658  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:24  lr: 0.000010  loss: 7.3525  loss_lm: 7.2632 (7.1735)  time: 2.5822  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:22  lr: 0.000010  loss: 7.3228  loss_lm: 7.3979 (7.3932)  time: 2.5819  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:23  lr: 0.000010  loss: 7.4888  loss_lm: 7.3569 (7.3905)  time: 2.5822  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:25  lr: 0.000010  loss: 7.4724  loss_lm: 7.4724 (7.3780)  time: 2.5826  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:23  lr: 0.000010  loss: 7.6186  loss_lm: 7.3595 (7.2858)  time: 2.5821  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 112/4722]  eta: 3:18:23  lr: 0.000010  loss: 7.2796  loss_lm: 7.4604 (7.3584)  time: 2.5821  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 113/4722]  eta: 3:07:41  lr: 0.000010  loss: 7.0564  loss_lm: 7.3522 (7.3487)  time: 2.4433  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 113/4722]  eta: 3:07:40  lr: 0.000010  loss: 7.2114  loss_lm: 7.3228 (7.3705)  time: 2.4431  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 113/4722]  eta: 3:07:40  lr: 0.000010  loss: 7.1157  loss_lm: 7.4082 (7.3281)  time: 2.4432  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 113/4722]  eta: 3:07:40  lr: 0.000010  loss: 7.1336  loss_lm: 7.3063 (7.2668)  time: 2.4432  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [ 113/4722]  eta: 3:07:42  lr: 0.000010  loss: 7.1925  loss_lm: 7.2372 (7.3548)  time: 2.4436  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 113/4722]  eta: 3:07:41  lr: 0.000010  loss: 7.1699  loss_lm: 7.1758 (7.1731)  time: 2.4433  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 114/4722]  eta: 2:59:21  lr: 0.000010  loss: 6.9273  loss_lm: 7.3522 (7.3019)  time: 2.3353  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 114/4722]  eta: 2:59:20  lr: 0.000010  loss: 6.7380  loss_lm: 7.3228 (7.3002)  time: 2.3351  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 114/4722]  eta: 2:59:20  lr: 0.000010  loss: 7.1565  loss_lm: 7.4082 (7.3090)  time: 2.3352  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 114/4722]  eta: 2:59:21  lr: 0.000010  loss: 6.9704  loss_lm: 7.1758 (7.1506)  time: 2.3353  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 114/4722]  eta: 2:59:22  lr: 0.000010  loss: 7.1002  loss_lm: 7.2372 (7.3265)  time: 2.3356  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 114/4722]  eta: 2:59:20  lr: 0.000010  loss: 7.3314  loss_lm: 7.3314 (7.2739)  time: 2.3352  data: 0.0000  max mem: 18924
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:04  lr: 0.000010  loss: 7.3863  loss_lm: 7.3522 (7.3104)  time: 2.7751  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:04  lr: 0.000010  loss: 7.1366  loss_lm: 7.1699 (7.1492)  time: 2.7751  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:04  lr: 0.000010  loss: 6.9769  loss_lm: 7.3063 (7.2442)  time: 2.7750  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:05  lr: 0.000010  loss: 7.0802  loss_lm: 7.2188 (7.3019)  time: 2.7753  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:04  lr: 0.000010  loss: 7.0966  loss_lm: 7.2796 (7.2878)  time: 2.7749  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:03  lr: 0.000010  loss: 7.0111  loss_lm: 7.2369 (7.2713)  time: 2.7749  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 116/4722]  eta: 3:23:58  lr: 0.000010  loss: 7.0341  loss_lm: 7.1699 (7.1387)  time: 2.6572  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 116/4722]  eta: 3:23:59  lr: 0.000010  loss: 7.2668  loss_lm: 7.2372 (7.2987)  time: 2.6574  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 116/4722]  eta: 3:23:58  lr: 0.000010  loss: 7.2848  loss_lm: 7.3522 (7.3080)  time: 2.6572  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 116/4722]  eta: 3:23:58  lr: 0.000010  loss: 7.1806  loss_lm: 7.3063 (7.2385)  time: 2.6571  data: 0.0000  max mem: 18924

Train: data epoch: [0]  [ 116/4722]  eta: 3:23:58  lr: 0.000010  loss: 6.8410  loss_lm: 7.2369 (7.2322)  time: 2.6570  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 116/4722]  eta: 3:23:58  lr: 0.000010  loss: 7.0153  loss_lm: 7.2796 (7.2630)  time: 2.6571  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:24  lr: 0.000010  loss: 7.0437  loss_lm: 7.1366 (7.1308)  time: 2.5590  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:23  lr: 0.000010  loss: 7.2191  loss_lm: 7.2191 (7.2594)  time: 2.5589  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:24  lr: 0.000010  loss: 7.7188  loss_lm: 7.3522 (7.3423)  time: 2.5590  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:23  lr: 0.000010  loss: 7.6924  loss_lm: 7.3063 (7.2763)  time: 2.5589  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:24  lr: 0.000010  loss: 7.3661  loss_lm: 7.2372 (7.3043)  time: 2.5592  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:23  lr: 0.000010  loss: 7.3771  loss_lm: 7.2369 (7.2442)  time: 2.5588  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:57  lr: 0.000010  loss: 7.0769  loss_lm: 7.1366 (7.1266)  time: 2.4756  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:57  lr: 0.000010  loss: 7.4787  loss_lm: 7.3569 (7.3528)  time: 2.4756  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:57  lr: 0.000010  loss: 7.2790  loss_lm: 7.3063 (7.2765)  time: 2.4756  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:57  lr: 0.000010  loss: 7.2453  loss_lm: 7.2453 (7.2583)  time: 2.4755  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:58  lr: 0.000010  loss: 7.2733  loss_lm: 7.2668 (7.3019)  time: 2.4758  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:57  lr: 0.000010  loss: 7.1659  loss_lm: 7.2369 (7.2382)  time: 2.4755  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:27  lr: 0.000010  loss: 6.9913  loss_lm: 7.0769 (7.1170)  time: 2.4044  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:28  lr: 0.000010  loss: 7.1605  loss_lm: 7.2372 (7.2918)  time: 2.4046  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:27  lr: 0.000010  loss: 7.3774  loss_lm: 7.3569 (7.3545)  time: 2.4044  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 119/4722]  eta: 3:04:27  lr: 0.000010  loss: 7.6710  loss_lm: 7.3063 (7.3047)  time: 2.4044  data: 0.0000  max mem: 18924

Train: data epoch: [0]  [ 119/4722]  eta: 3:04:27  lr: 0.000010  loss: 7.0903  loss_lm: 7.2191 (7.2463)  time: 2.4043  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:26  lr: 0.000010  loss: 7.1976  loss_lm: 7.2171 (7.2353)  time: 2.4043  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:26  lr: 0.000010  loss: 7.2027  loss_lm: 7.3063 (7.2979)  time: 2.7045  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:26  lr: 0.000010  loss: 7.1621  loss_lm: 7.3569 (7.3417)  time: 2.7046  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:26  lr: 0.000010  loss: 7.1824  loss_lm: 7.2191 (7.2420)  time: 2.7045  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 120/4722]  eta: 3:27:27  lr: 0.000010  loss: 7.1494  loss_lm: 7.2372 (7.2823)  time: 2.7047  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 120/4722]  eta: 3:27:26  lr: 0.000010  loss: 7.0528  loss_lm: 7.0769 (7.1127)  time: 2.7046  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:25  lr: 0.000010  loss: 7.7107  loss_lm: 7.2369 (7.2670)  time: 2.7044  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 121/4722]  eta: 3:21:29  lr: 0.000010  loss: 7.5217  loss_lm: 7.0769 (7.1383)  time: 2.6276  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 121/4722]  eta: 3:21:29  lr: 0.000010  loss: 7.2600  loss_lm: 7.3522 (7.3366)  time: 2.6276  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 121/4722]  eta: 3:21:30  lr: 0.000010  loss: 7.1010  loss_lm: 7.2188 (7.2710)  time: 2.6277  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 121/4722]  eta: 3:21:29  lr: 0.000010  loss: 7.0161  loss_lm: 7.2790 (7.2803)  time: 2.6275  data: 0.0000  max mem: 18924

Train: data epoch: [0]  [ 121/4722]  eta: 3:21:29  lr: 0.000010  loss: 7.2427  loss_lm: 7.2191 (7.2421)  time: 2.6275  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 121/4722]  eta: 3:21:28  lr: 0.000010  loss: 7.2410  loss_lm: 7.2369 (7.2654)  time: 2.6274  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 122/4722]  eta: 3:16:13  lr: 0.000010  loss: 7.6110  loss_lm: 7.1366 (7.1661)  time: 2.5594  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 122/4722]  eta: 3:16:13  lr: 0.000010  loss: 7.1688  loss_lm: 7.3522 (7.3267)  time: 2.5594  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 122/4722]  eta: 3:16:12  lr: 0.000010  loss: 7.0416  loss_lm: 7.2191 (7.2303)  time: 2.5593  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 122/4722]  eta: 3:16:12  lr: 0.000010  loss: 7.3317  loss_lm: 7.2410 (7.2693)  time: 2.5592  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 122/4722]  eta: 3:16:13  lr: 0.000010  loss: 6.9694  loss_lm: 7.2790 (7.2620)  time: 2.5594  data: 0.0000  max mem: 18924Train: data epoch: [0]  [ 122/4722]  eta: 3:16:13  lr: 0.000010  loss: 7.1898  loss_lm: 7.2188 (7.2662)  time: 2.5595  data: 0.0000  max mem: 18866

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 123/4722]  eta: 3:11:31  lr: 0.000010  loss: 7.3852  loss_lm: 7.1366 (7.1782)  time: 2.4987  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 123/4722]  eta: 3:11:31  lr: 0.000010  loss: 7.5109  loss_lm: 7.3522 (7.3369)  time: 2.4987  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 123/4722]  eta: 3:11:31  lr: 0.000010  loss: 6.8502  loss_lm: 7.2406 (7.2391)  time: 2.4987  data: 0.0000  max mem: 18924

Train: data epoch: [0]  [ 123/4722]  eta: 3:11:30  lr: 0.000010  loss: 6.9330  loss_lm: 7.2369 (7.2506)  time: 2.4985  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 123/4722]  eta: 3:11:31  lr: 0.000010  loss: 6.6376  loss_lm: 7.1925 (7.2313)  time: 2.4988  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 123/4722]  eta: 3:11:31  lr: 0.000010  loss: 6.8277  loss_lm: 7.1824 (7.2079)  time: 2.4986  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:19  lr: 0.000010  loss: 7.1905  loss_lm: 7.1699 (7.1789)  time: 2.4445  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:19  lr: 0.000010  loss: 7.5743  loss_lm: 7.3569 (7.3494)  time: 2.4445  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:19  lr: 0.000010  loss: 6.8636  loss_lm: 7.2406 (7.2193)  time: 2.4444  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:18  lr: 0.000010  loss: 7.2260  loss_lm: 7.2369 (7.2493)  time: 2.4443  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 124/4722]  eta: 3:07:20  lr: 0.000010  loss: 7.1832  loss_lm: 7.1925 (7.2288)  time: 2.4445  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 124/4722]  eta: 3:07:19  lr: 0.000010  loss: 7.1172  loss_lm: 7.1824 (7.2031)  time: 2.4444  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:04  lr: 0.000010  loss: 7.7280  loss_lm: 7.1699 (7.2063)  time: 2.6505  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:04  lr: 0.000010  loss: 6.9528  loss_lm: 7.3522 (7.3296)  time: 2.6505  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:04  lr: 0.000010  loss: 7.3876  loss_lm: 7.2406 (7.2277)  time: 2.6504  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:03  lr: 0.000010  loss: 6.5017  loss_lm: 7.2260 (7.2119)  time: 2.6503  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:04  lr: 0.000010  loss: 7.3371  loss_lm: 7.1925 (7.2342)  time: 2.6506  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:03  lr: 0.000010  loss: 7.0444  loss_lm: 7.1565 (7.1952)  time: 2.6504  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 126/4722]  eta: 3:18:43  lr: 0.000010  loss: 7.0963  loss_lm: 7.1366 (7.2011)  time: 2.5320  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 126/4722]  eta: 3:18:43  lr: 0.000010  loss: 7.2717  loss_lm: 7.2848 (7.3268)  time: 2.5320  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 126/4722]  eta: 3:18:43  lr: 0.000010  loss: 7.5131  loss_lm: 7.2406 (7.2413)  time: 2.5320  data: 0.0000  max mem: 18924

Train: data epoch: [0]  [ 126/4722]  eta: 3:18:42  lr: 0.000010  loss: 7.4967  loss_lm: 7.1565 (7.2096)  time: 2.5319  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 126/4722]  eta: 3:18:42  lr: 0.000010  loss: 6.6696  loss_lm: 7.2171 (7.1861)  time: 2.5320  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 126/4722]  eta: 3:18:43  lr: 0.000010  loss: 6.7435  loss_lm: 7.1898 (7.2108)  time: 2.5319  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 127/4722]  eta: 3:14:46  lr: 0.000010  loss: 6.9662  loss_lm: 7.1366 (7.1904)  time: 2.5319  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 127/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.0789  loss_lm: 7.2717 (7.3156)  time: 2.5319  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 127/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.1524  loss_lm: 7.2027 (7.2373)  time: 2.5319  data: 0.0000  max mem: 18924Train: data epoch: [0]  [ 127/4722]  eta: 3:14:45  lr: 0.000010  loss: 6.6470  loss_lm: 7.2114 (7.1616)  time: 2.5319  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 127/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.2955  loss_lm: 7.1898 (7.2147)  time: 2.5319  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 127/4722]  eta: 3:14:45  lr: 0.000010  loss: 6.8665  loss_lm: 7.1172 (7.1940)  time: 2.5318  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.8900  loss_lm: 7.1366 (7.2208)  time: 2.5311  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.0318  loss_lm: 7.1806 (7.2284)  time: 2.5311  data: 0.0000  max mem: 18924
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.6785  loss_lm: 7.2717 (7.3314)  time: 2.5311  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.3508  loss_lm: 7.2114 (7.1698)  time: 2.5310  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.2908  loss_lm: 7.1565 (7.1982)  time: 2.5310  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.3469  loss_lm: 7.1898 (7.2204)  time: 2.5310  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 129/4722]  eta: 3:07:51  lr: 0.000010  loss: 7.0845  loss_lm: 7.0963 (7.2152)  time: 2.5309  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 129/4722]  eta: 3:07:51  lr: 0.000010  loss: 7.3751  loss_lm: 7.1806 (7.2345)  time: 2.5309  data: 0.0000  max mem: 18924

Train: data epoch: [0]  [ 129/4722]  eta: 3:07:51  lr: 0.000010  loss: 7.2302  loss_lm: 7.2600 (7.3271)  time: 2.5309  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 129/4722]  eta: 3:07:50  lr: 0.000010  loss: 7.0712  loss_lm: 7.1976 (7.1657)  time: 2.5308  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 129/4722]  eta: 3:07:50  lr: 0.000010  loss: 6.9707  loss_lm: 7.1172 (7.1887)  time: 2.5308  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 129/4722]  eta: 3:07:51  lr: 0.000010  loss: 6.9821  loss_lm: 7.1832 (7.2105)  time: 2.5308  data: 0.0000  max mem: 18866
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:15:04.541178] Starting job
[2023-11-13 00:15:04.541189] Starting job
[2023-11-13 00:15:04.541275] Starting job
[2023-11-13 00:15:04.554828] Starting job
[2023-11-13 00:15:04.554896] Starting job
| distributed init (rank 10, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 11, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 8, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:15:09.308865] Starting job
| distributed init (rank 9, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 126/4722]  eta: 5:18:48  lr: 0.000010  loss: 6.6647  loss_lm: 6.6647 (6.6647)  time: 4.1620  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 126/4722]  eta: 5:19:01  lr: 0.000010  loss: 6.7096  loss_lm: 6.7096 (6.7096)  time: 4.1649  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 126/4722]  eta: 5:18:57  lr: 0.000010  loss: 7.5088  loss_lm: 7.5088 (7.5088)  time: 4.1639  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 126/4722]  eta: 5:18:59  lr: 0.000010  loss: 7.1036  loss_lm: 7.1036 (7.1036)  time: 4.1644  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 126/4722]  eta: 5:18:50  lr: 0.000010  loss: 7.5177  loss_lm: 7.5177 (7.5177)  time: 4.1624  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 126/4722]  eta: 5:18:57  lr: 0.000010  loss: 7.2743  loss_lm: 7.2743 (7.2743)  time: 4.1639  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:16  lr: 0.000010  loss: 6.6595  loss_lm: 6.6595 (6.6621)  time: 2.8240  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:22  lr: 0.000010  loss: 7.3334  loss_lm: 6.7096 (7.0215)  time: 2.8254  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:20  lr: 0.000010  loss: 6.8649  loss_lm: 6.8649 (7.1869)  time: 2.8249  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:21  lr: 0.000010  loss: 6.9793  loss_lm: 6.9793 (7.0414)  time: 2.8252  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:20  lr: 0.000010  loss: 7.0807  loss_lm: 7.0807 (7.1775)  time: 2.8248  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:17  lr: 0.000010  loss: 7.1772  loss_lm: 7.1772 (7.3474)  time: 2.8242  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 128/4722]  eta: 3:01:49  lr: 0.000010  loss: 7.3317  loss_lm: 6.6647 (6.8853)  time: 2.3748  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 128/4722]  eta: 3:01:52  lr: 0.000010  loss: 7.2812  loss_lm: 7.2812 (7.2183)  time: 2.3754  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 128/4722]  eta: 3:01:53  lr: 0.000010  loss: 7.3716  loss_lm: 7.3334 (7.1382)  time: 2.3757  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 128/4722]  eta: 3:01:52  lr: 0.000010  loss: 7.6366  loss_lm: 7.2743 (7.3306)  time: 2.3753  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 128/4722]  eta: 3:01:53  lr: 0.000010  loss: 7.8955  loss_lm: 7.1036 (7.3261)  time: 2.3755  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 128/4722]  eta: 3:01:50  lr: 0.000010  loss: 7.0507  loss_lm: 7.1772 (7.2485)  time: 2.3748  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:36  lr: 0.000010  loss: 7.1033  loss_lm: 6.6647 (6.9398)  time: 2.1503  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:39  lr: 0.000010  loss: 6.9914  loss_lm: 6.9914 (7.1015)  time: 2.1510  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:38  lr: 0.000010  loss: 6.9366  loss_lm: 6.9366 (7.1479)  time: 2.1508  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:38  lr: 0.000010  loss: 7.2820  loss_lm: 7.2743 (7.3184)  time: 2.1507  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:38  lr: 0.000010  loss: 7.1424  loss_lm: 7.1036 (7.2802)  time: 2.1509  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:36  lr: 0.000010  loss: 7.3541  loss_lm: 7.1772 (7.2749)  time: 2.1504  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:18  lr: 0.000010  loss: 6.6100  loss_lm: 6.6647 (6.8739)  time: 3.1269  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:21  lr: 0.000010  loss: 7.1443  loss_lm: 7.1443 (7.1101)  time: 3.1274  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:20  lr: 0.000010  loss: 6.5895  loss_lm: 7.1036 (7.1421)  time: 3.1273  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:19  lr: 0.000010  loss: 7.2973  loss_lm: 7.2820 (7.3142)  time: 3.1271  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:20  lr: 0.000010  loss: 7.1156  loss_lm: 7.1156 (7.1414)  time: 3.1272  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:18  lr: 0.000010  loss: 7.3584  loss_lm: 7.3541 (7.2916)  time: 3.1269  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:21  lr: 0.000010  loss: 7.6897  loss_lm: 7.1443 (7.2067)  time: 2.8538  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 131/4722]  eta: 3:38:19  lr: 0.000010  loss: 6.9378  loss_lm: 6.6647 (6.8845)  time: 2.8533  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 131/4722]  eta: 3:38:20  lr: 0.000010  loss: 6.8767  loss_lm: 6.9366 (7.0973)  time: 2.8536  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:19  lr: 0.000010  loss: 7.2106  loss_lm: 7.2106 (7.2781)  time: 2.8533  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:21  lr: 0.000010  loss: 6.5240  loss_lm: 6.9793 (7.0391)  time: 2.8536  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:20  lr: 0.000010  loss: 7.4307  loss_lm: 7.2820 (7.3336)  time: 2.8535  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 132/4722]  eta: 3:23:15  lr: 0.000010  loss: 7.2601  loss_lm: 7.1156 (7.1206)  time: 2.6570  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 132/4722]  eta: 3:23:14  lr: 0.000010  loss: 7.2585  loss_lm: 6.9378 (6.9379)  time: 2.6568  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 132/4722]  eta: 3:23:16  lr: 0.000010  loss: 6.8888  loss_lm: 7.1443 (7.1613)  time: 2.6571  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 132/4722]  eta: 3:23:14  lr: 0.000010  loss: 6.6923  loss_lm: 7.2106 (7.1944)  time: 2.6567  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 132/4722]  eta: 3:23:15  lr: 0.000010  loss: 7.2611  loss_lm: 7.1036 (7.0708)  time: 2.6570  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 132/4722]  eta: 3:23:15  lr: 0.000010  loss: 7.3673  loss_lm: 7.2973 (7.3384)  time: 2.6569  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 133/4722]  eta: 3:11:54  lr: 0.000010  loss: 7.2586  loss_lm: 7.2820 (7.3285)  time: 2.5092  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 133/4722]  eta: 3:11:55  lr: 0.000010  loss: 6.9199  loss_lm: 6.9793 (7.0519)  time: 2.5093  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 133/4722]  eta: 3:11:54  lr: 0.000010  loss: 7.2334  loss_lm: 7.1156 (7.1347)  time: 2.5092  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 133/4722]  eta: 3:11:54  lr: 0.000010  loss: 7.0795  loss_lm: 6.9378 (6.9556)  time: 2.5091  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 133/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.1049  loss_lm: 7.1049 (7.1542)  time: 2.5094  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 133/4722]  eta: 3:11:53  lr: 0.000010  loss: 7.3473  loss_lm: 7.2106 (7.2135)  time: 2.5090  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 134/4722]  eta: 3:03:06  lr: 0.000010  loss: 6.8056  loss_lm: 7.2820 (7.2704)  time: 2.3947  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 134/4722]  eta: 3:03:07  lr: 0.000010  loss: 6.6425  loss_lm: 7.1049 (7.0974)  time: 2.3948  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 134/4722]  eta: 3:03:06  lr: 0.000010  loss: 7.2621  loss_lm: 7.2334 (7.1488)  time: 2.3947  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 134/4722]  eta: 3:03:06  lr: 0.000010  loss: 6.9406  loss_lm: 7.2106 (7.1832)  time: 2.3945  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 134/4722]  eta: 3:03:07  lr: 0.000010  loss: 7.4218  loss_lm: 7.1036 (7.0930)  time: 2.3947  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 134/4722]  eta: 3:03:06  lr: 0.000010  loss: 6.4934  loss_lm: 6.9378 (6.9043)  time: 2.3945  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:50  lr: 0.000010  loss: 7.3697  loss_lm: 7.1049 (7.1246)  time: 2.8103  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:49  lr: 0.000010  loss: 7.0591  loss_lm: 7.2743 (7.2492)  time: 2.8101  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:50  lr: 0.000010  loss: 6.9504  loss_lm: 7.1156 (7.1290)  time: 2.8101  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:50  lr: 0.000010  loss: 7.0134  loss_lm: 7.0134 (7.0851)  time: 2.8102  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:49  lr: 0.000010  loss: 6.5779  loss_lm: 7.1772 (7.1227)  time: 2.8100  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:49  lr: 0.000010  loss: 7.2186  loss_lm: 6.9378 (6.9357)  time: 2.8100  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:32  lr: 0.000010  loss: 6.6363  loss_lm: 6.9378 (6.9085)  time: 2.6892  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:33  lr: 0.000010  loss: 6.7182  loss_lm: 7.2743 (7.2010)  time: 2.6893  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:33  lr: 0.000010  loss: 7.1091  loss_lm: 7.1036 (7.0872)  time: 2.6894  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:33  lr: 0.000010  loss: 6.8234  loss_lm: 7.1049 (7.0972)  time: 2.6895  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:33  lr: 0.000010  loss: 7.0591  loss_lm: 7.1156 (7.1226)  time: 2.6894  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:32  lr: 0.000010  loss: 6.7862  loss_lm: 7.1772 (7.0921)  time: 2.6892  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 137/4722]  eta: 3:17:46  lr: 0.000010  loss: 7.3345  loss_lm: 7.2743 (7.2121)  time: 2.5880  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 137/4722]  eta: 3:17:45  lr: 0.000010  loss: 7.4417  loss_lm: 6.9378 (6.9529)  time: 2.5879  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 137/4722]  eta: 3:17:46  lr: 0.000010  loss: 6.7927  loss_lm: 7.0591 (7.0951)  time: 2.5881  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 137/4722]  eta: 3:17:46  lr: 0.000010  loss: 7.5014  loss_lm: 7.1036 (7.1217)  time: 2.5881  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 137/4722]  eta: 3:17:45  lr: 0.000010  loss: 6.8843  loss_lm: 7.0507 (7.0748)  time: 2.5879  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 137/4722]  eta: 3:17:46  lr: 0.000010  loss: 7.3876  loss_lm: 7.1049 (7.1214)  time: 2.5882  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:11  lr: 0.000010  loss: 6.7986  loss_lm: 7.1049 (7.0966)  time: 2.5024  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:10  lr: 0.000010  loss: 6.9946  loss_lm: 7.0591 (7.0874)  time: 2.5023  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:09  lr: 0.000010  loss: 7.1373  loss_lm: 7.0795 (6.9671)  time: 2.5022  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.1514  loss_lm: 7.2743 (7.2074)  time: 2.5023  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:09  lr: 0.000010  loss: 6.7462  loss_lm: 7.0507 (7.0495)  time: 2.5022  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.2372  loss_lm: 7.1091 (7.1306)  time: 2.5023  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 139/4722]  eta: 3:05:32  lr: 0.000010  loss: 7.0047  loss_lm: 7.0047 (6.9698)  time: 2.4290  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 139/4722]  eta: 3:05:32  lr: 0.000010  loss: 7.2593  loss_lm: 7.2593 (7.2111)  time: 2.4291  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [ 139/4722]  eta: 3:05:33  lr: 0.000010  loss: 6.9668  loss_lm: 6.9914 (7.0873)  time: 2.4292  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 139/4722]  eta: 3:05:32  lr: 0.000010  loss: 6.9006  loss_lm: 6.9946 (7.0741)  time: 2.4291  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 139/4722]  eta: 3:05:32  lr: 0.000010  loss: 7.0853  loss_lm: 7.1036 (7.1274)  time: 2.4291  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 139/4722]  eta: 3:05:32  lr: 0.000010  loss: 6.9339  loss_lm: 6.9406 (7.0412)  time: 2.4290  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:15  lr: 0.000010  loss: 6.8457  loss_lm: 7.0047 (6.9615)  time: 2.7664  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:16  lr: 0.000010  loss: 6.8353  loss_lm: 6.9914 (7.0705)  time: 2.7666  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:16  lr: 0.000010  loss: 7.2485  loss_lm: 7.1091 (7.1355)  time: 2.7666  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:16  lr: 0.000010  loss: 7.2968  loss_lm: 7.0591 (7.0889)  time: 2.7666  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:15  lr: 0.000010  loss: 6.8935  loss_lm: 6.9406 (7.0314)  time: 2.7664  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:16  lr: 0.000010  loss: 7.2206  loss_lm: 7.2593 (7.2117)  time: 2.7666  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 141/4722]  eta: 3:25:03  lr: 0.000010  loss: 7.1626  loss_lm: 7.0047 (6.9741)  time: 2.6857  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 141/4722]  eta: 3:25:03  lr: 0.000010  loss: 7.0674  loss_lm: 7.0591 (7.0876)  time: 2.6858  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 141/4722]  eta: 3:25:03  lr: 0.000010  loss: 7.2489  loss_lm: 7.2586 (7.2141)  time: 2.6858  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 141/4722]  eta: 3:25:03  lr: 0.000010  loss: 6.9292  loss_lm: 7.1036 (7.1226)  time: 2.6858  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 141/4722]  eta: 3:25:03  lr: 0.000010  loss: 6.9819  loss_lm: 6.9819 (7.0650)  time: 2.6859  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 141/4722]  eta: 3:25:02  lr: 0.000010  loss: 7.2547  loss_lm: 6.9406 (7.0453)  time: 2.6857  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 142/4722]  eta: 3:19:35  lr: 0.000010  loss: 6.4477  loss_lm: 6.9819 (7.0287)  time: 2.6147  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 142/4722]  eta: 3:19:34  lr: 0.000010  loss: 6.8505  loss_lm: 7.0047 (6.9668)  time: 2.6145  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 142/4722]  eta: 3:19:34  lr: 0.000010  loss: 7.2600  loss_lm: 7.0674 (7.0977)  time: 2.6146  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 142/4722]  eta: 3:19:34  lr: 0.000010  loss: 7.2991  loss_lm: 7.2593 (7.2191)  time: 2.6146  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 142/4722]  eta: 3:19:34  lr: 0.000010  loss: 7.2314  loss_lm: 7.0507 (7.0563)  time: 2.6145  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 142/4722]  eta: 3:19:35  lr: 0.000010  loss: 6.6840  loss_lm: 7.1036 (7.0968)  time: 2.6146  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 143/4722]  eta: 3:14:41  lr: 0.000010  loss: 6.8756  loss_lm: 6.9378 (6.9617)  time: 2.5511  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 143/4722]  eta: 3:14:41  lr: 0.000010  loss: 6.5994  loss_lm: 7.0591 (7.0700)  time: 2.5512  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 143/4722]  eta: 3:14:41  lr: 0.000010  loss: 6.8370  loss_lm: 7.2586 (7.1978)  time: 2.5511  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 143/4722]  eta: 3:14:41  lr: 0.000010  loss: 7.3439  loss_lm: 7.1036 (7.1105)  time: 2.5512  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 143/4722]  eta: 3:14:42  lr: 0.000010  loss: 6.8769  loss_lm: 6.9668 (7.0202)  time: 2.5512  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 143/4722]  eta: 3:14:41  lr: 0.000010  loss: 6.9935  loss_lm: 6.9935 (7.0528)  time: 2.5510  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 144/4722]  eta: 3:10:19  lr: 0.000010  loss: 7.1371  loss_lm: 7.0047 (6.9710)  time: 2.4944  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 144/4722]  eta: 3:10:20  lr: 0.000010  loss: 6.7047  loss_lm: 6.9668 (7.0036)  time: 2.4945  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 144/4722]  eta: 3:10:19  lr: 0.000010  loss: 7.0998  loss_lm: 7.2586 (7.1927)  time: 2.4945  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 144/4722]  eta: 3:10:19  lr: 0.000010  loss: 6.9046  loss_lm: 7.0591 (7.0613)  time: 2.4945  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 144/4722]  eta: 3:10:19  lr: 0.000010  loss: 7.0467  loss_lm: 7.0467 (7.0525)  time: 2.4944  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 144/4722]  eta: 3:10:19  lr: 0.000010  loss: 6.9700  loss_lm: 7.1036 (7.1031)  time: 2.4945  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:40  lr: 0.000010  loss: 6.9984  loss_lm: 6.9984 (6.9723)  time: 2.6962  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:40  lr: 0.000010  loss: 7.1997  loss_lm: 7.0591 (7.0682)  time: 2.6962  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:40  lr: 0.000010  loss: 7.0587  loss_lm: 7.0853 (7.1009)  time: 2.6963  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 145/4722]  eta: 3:25:41  lr: 0.000010  loss: 7.0689  loss_lm: 6.9668 (7.0069)  time: 2.6963  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 145/4722]  eta: 3:25:40  lr: 0.000010  loss: 7.0878  loss_lm: 7.0467 (7.0542)  time: 2.6961  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:40  lr: 0.000010  loss: 7.1547  loss_lm: 7.2489 (7.1908)  time: 2.6962  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 6.7358  loss_lm: 6.9984 (6.9611)  time: 2.5620  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 7.1937  loss_lm: 7.2206 (7.1909)  time: 2.5619  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 6.5976  loss_lm: 6.9668 (6.9874)  time: 2.5620  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 6.5230  loss_lm: 6.9946 (7.0423)  time: 2.5620  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 6.7564  loss_lm: 6.9935 (7.0401)  time: 2.5619  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 6.7938  loss_lm: 7.0587 (7.0863)  time: 2.5620  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 147/4722]  eta: 3:17:09  lr: 0.000010  loss: 6.5587  loss_lm: 7.2206 (7.1622)  time: 2.5619  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 147/4722]  eta: 3:17:09  lr: 0.000010  loss: 6.7211  loss_lm: 6.9984 (6.9502)  time: 2.5619  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 147/4722]  eta: 3:17:10  lr: 0.000010  loss: 6.8739  loss_lm: 6.8888 (6.9822)  time: 2.5619  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 147/4722]  eta: 3:17:10  lr: 0.000010  loss: 6.9367  loss_lm: 6.9946 (7.0375)  time: 2.5619  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 147/4722]  eta: 3:17:09  lr: 0.000010  loss: 7.0986  loss_lm: 6.9935 (7.0427)  time: 2.5618  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 147/4722]  eta: 3:17:10  lr: 0.000010  loss: 6.7403  loss_lm: 7.0587 (7.0705)  time: 2.5619  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 148/4722]  eta: 3:13:26  lr: 0.000010  loss: 7.4718  loss_lm: 6.9984 (6.9729)  time: 2.5620  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 148/4722]  eta: 3:13:27  lr: 0.000010  loss: 6.2663  loss_lm: 7.1937 (7.1232)  time: 2.5620  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 148/4722]  eta: 3:13:27  lr: 0.000010  loss: 7.4177  loss_lm: 6.9946 (7.0540)  time: 2.5620  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 148/4722]  eta: 3:13:27  lr: 0.000010  loss: 6.6429  loss_lm: 7.0134 (7.0519)  time: 2.5620  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 148/4722]  eta: 3:13:27  lr: 0.000010  loss: 7.2103  loss_lm: 6.8888 (6.9921)  time: 2.5620  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 148/4722]  eta: 3:13:26  lr: 0.000010  loss: 7.2083  loss_lm: 6.9935 (7.0499)  time: 2.5619  data: 0.0000  max mem: 18865
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:19:48.362062] Starting job
[2023-11-13 00:19:48.362087] Starting job
[2023-11-13 00:19:48.362108] Starting job
[2023-11-13 00:19:48.362103] Starting job
[2023-11-13 00:19:48.362109] Starting job
[2023-11-13 00:19:48.362182] Starting job
| distributed init (rank 10, world 12): env://
| distributed init (rank 8, world 12): env://
| distributed init (rank 6, world 12): env://
| distributed init (rank 9, world 12): env://
| distributed init (rank 7, world 12): env://
| distributed init (rank 11, world 12): env://
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 146/4722]  eta: 5:19:40  lr: 0.000010  loss: 6.5253  loss_lm: 6.5253 (6.5253)  time: 4.1916  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 146/4722]  eta: 5:19:40  lr: 0.000010  loss: 6.7781  loss_lm: 6.7781 (6.7781)  time: 4.1915  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 146/4722]  eta: 5:19:43  lr: 0.000010  loss: 6.5907  loss_lm: 6.5907 (6.5907)  time: 4.1923  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 146/4722]  eta: 5:19:36  lr: 0.000010  loss: 7.1718  loss_lm: 7.1718 (7.1718)  time: 4.1907  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 146/4722]  eta: 5:19:40  lr: 0.000010  loss: 6.7549  loss_lm: 6.7549 (6.7549)  time: 4.1916  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 146/4722]  eta: 5:19:36  lr: 0.000010  loss: 6.7516  loss_lm: 6.7516 (6.7516)  time: 4.1906  data: 0.0000  max mem: 18866

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 147/4722]  eta: 3:36:26  lr: 0.000010  loss: 6.9608  loss_lm: 6.5253 (6.7430)  time: 2.8385  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 147/4722]  eta: 3:36:25  lr: 0.000010  loss: 6.7396  loss_lm: 6.7396 (6.7589)  time: 2.8384  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [ 147/4722]  eta: 3:36:27  lr: 0.000010  loss: 6.8996  loss_lm: 6.5907 (6.7451)  time: 2.8388  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 147/4722]  eta: 3:36:23  lr: 0.000010  loss: 6.7240  loss_lm: 6.7240 (6.7378)  time: 2.8379  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 147/4722]  eta: 3:36:25  lr: 0.000010  loss: 7.1081  loss_lm: 6.7549 (6.9315)  time: 2.8384  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 147/4722]  eta: 3:36:23  lr: 0.000010  loss: 6.5892  loss_lm: 6.5892 (6.8805)  time: 2.8380  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 148/4722]  eta: 3:01:40  lr: 0.000010  loss: 6.6571  loss_lm: 6.7396 (6.7249)  time: 2.3832  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 148/4722]  eta: 3:01:40  lr: 0.000010  loss: 7.4284  loss_lm: 6.9608 (6.9715)  time: 2.3832  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 148/4722]  eta: 3:01:41  lr: 0.000010  loss: 7.2264  loss_lm: 6.8996 (6.9056)  time: 2.3834  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 148/4722]  eta: 3:01:39  lr: 0.000010  loss: 7.4607  loss_lm: 6.7516 (6.9788)  time: 2.3828  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 148/4722]  eta: 3:01:40  lr: 0.000010  loss: 7.2156  loss_lm: 7.1081 (7.0262)  time: 2.3832  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 148/4722]  eta: 3:01:39  lr: 0.000010  loss: 6.2387  loss_lm: 6.5892 (6.6666)  time: 2.3829  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 149/4722]  eta: 2:44:20  lr: 0.000010  loss: 7.0377  loss_lm: 6.9608 (6.9880)  time: 2.1563  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 149/4722]  eta: 2:44:20  lr: 0.000010  loss: 6.6996  loss_lm: 6.6996 (6.7186)  time: 2.1562  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 149/4722]  eta: 2:44:19  lr: 0.000010  loss: 7.1256  loss_lm: 6.7516 (7.0155)  time: 2.1559  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 149/4722]  eta: 2:44:20  lr: 0.000010  loss: 6.8165  loss_lm: 6.8165 (6.9737)  time: 2.1562  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 149/4722]  eta: 2:44:21  lr: 0.000010  loss: 6.6446  loss_lm: 6.6446 (6.8403)  time: 2.1564  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 149/4722]  eta: 2:44:19  lr: 0.000010  loss: 6.6332  loss_lm: 6.5892 (6.6582)  time: 2.1560  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 150/4722]  eta: 3:55:55  lr: 0.000010  loss: 6.4844  loss_lm: 6.7516 (6.9093)  time: 3.0961  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 150/4722]  eta: 3:55:55  lr: 0.000010  loss: 6.4577  loss_lm: 6.5892 (6.6181)  time: 3.0961  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 150/4722]  eta: 3:55:56  lr: 0.000010  loss: 6.9923  loss_lm: 6.8996 (6.8707)  time: 3.0964  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 150/4722]  eta: 3:55:56  lr: 0.000010  loss: 6.6731  loss_lm: 6.8165 (6.9136)  time: 3.0963  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 150/4722]  eta: 3:55:56  lr: 0.000010  loss: 6.9657  loss_lm: 6.9657 (6.9836)  time: 3.0963  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 150/4722]  eta: 3:55:56  lr: 0.000010  loss: 6.8812  loss_lm: 6.7396 (6.7511)  time: 3.0963  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:29  lr: 0.000010  loss: 7.1337  loss_lm: 6.8165 (6.9503)  time: 2.8286  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:29  lr: 0.000010  loss: 6.6768  loss_lm: 6.6996 (6.7387)  time: 2.8287  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:30  lr: 0.000010  loss: 6.7750  loss_lm: 6.7750 (6.8548)  time: 2.8288  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:29  lr: 0.000010  loss: 7.3410  loss_lm: 6.7516 (6.9812)  time: 2.8285  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:29  lr: 0.000010  loss: 7.0069  loss_lm: 6.9657 (6.9875)  time: 2.8287  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:29  lr: 0.000010  loss: 7.1982  loss_lm: 6.5892 (6.7148)  time: 2.8285  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 152/4722]  eta: 3:20:49  lr: 0.000010  loss: 6.8200  loss_lm: 6.7396 (6.7503)  time: 2.6366  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 152/4722]  eta: 3:20:49  lr: 0.000010  loss: 7.1228  loss_lm: 7.1081 (6.9749)  time: 2.6366  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 152/4722]  eta: 3:20:48  lr: 0.000010  loss: 7.0746  loss_lm: 6.6332 (6.7662)  time: 2.6365  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 152/4722]  eta: 3:20:48  lr: 0.000010  loss: 6.7772  loss_lm: 6.7772 (6.9521)  time: 2.6365  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 152/4722]  eta: 3:20:49  lr: 0.000010  loss: 6.8700  loss_lm: 6.9657 (6.9707)  time: 2.6366  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 152/4722]  eta: 3:20:49  lr: 0.000010  loss: 6.9137  loss_lm: 6.8996 (6.8632)  time: 2.6367  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 153/4722]  eta: 3:09:43  lr: 0.000010  loss: 7.0204  loss_lm: 6.7772 (6.9606)  time: 2.4914  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 153/4722]  eta: 3:09:43  lr: 0.000010  loss: 6.7802  loss_lm: 6.7396 (6.7541)  time: 2.4915  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 153/4722]  eta: 3:09:43  lr: 0.000010  loss: 7.2135  loss_lm: 7.1081 (7.0048)  time: 2.4915  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 153/4722]  eta: 3:09:44  lr: 0.000010  loss: 6.9434  loss_lm: 6.8996 (6.8732)  time: 2.4916  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 153/4722]  eta: 3:09:43  lr: 0.000010  loss: 7.4702  loss_lm: 6.9657 (7.0331)  time: 2.4915  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 153/4722]  eta: 3:09:43  lr: 0.000010  loss: 6.8340  loss_lm: 6.6332 (6.7747)  time: 2.4914  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 154/4722]  eta: 3:01:06  lr: 0.000010  loss: 6.9799  loss_lm: 7.1081 (7.0020)  time: 2.3789  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 154/4722]  eta: 3:01:06  lr: 0.000010  loss: 7.1681  loss_lm: 7.0069 (7.0481)  time: 2.3789  data: 0.0000  max mem: 18876Train: data epoch: [0]  [ 154/4722]  eta: 3:01:06  lr: 0.000010  loss: 6.6895  loss_lm: 6.8996 (6.8528)  time: 2.3789  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 154/4722]  eta: 3:01:06  lr: 0.000010  loss: 6.7676  loss_lm: 6.7772 (6.9392)  time: 2.3788  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 154/4722]  eta: 3:01:06  lr: 0.000010  loss: 7.2778  loss_lm: 6.8340 (6.8306)  time: 2.3787  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 154/4722]  eta: 3:01:06  lr: 0.000010  loss: 6.7194  loss_lm: 6.7396 (6.7502)  time: 2.3789  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:24  lr: 0.000010  loss: 6.8194  loss_lm: 6.9657 (7.0252)  time: 2.8169  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:24  lr: 0.000010  loss: 6.8940  loss_lm: 6.7396 (6.7646)  time: 2.8169  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:24  lr: 0.000010  loss: 6.9409  loss_lm: 6.7772 (6.9393)  time: 2.8168  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:24  lr: 0.000010  loss: 6.7945  loss_lm: 6.7945 (6.8470)  time: 2.8169  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:24  lr: 0.000010  loss: 6.8316  loss_lm: 6.8316 (6.8307)  time: 2.8168  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:24  lr: 0.000010  loss: 6.8726  loss_lm: 6.9799 (6.9891)  time: 2.8169  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 156/4722]  eta: 3:25:08  lr: 0.000010  loss: 6.8349  loss_lm: 6.9799 (6.9750)  time: 2.6956  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 156/4722]  eta: 3:25:08  lr: 0.000010  loss: 6.8731  loss_lm: 6.7781 (6.7745)  time: 2.6956  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 156/4722]  eta: 3:25:07  lr: 0.000010  loss: 6.8860  loss_lm: 6.8340 (6.8357)  time: 2.6955  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 156/4722]  eta: 3:25:07  lr: 0.000010  loss: 6.5303  loss_lm: 6.7772 (6.9022)  time: 2.6955  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 156/4722]  eta: 3:25:08  lr: 0.000010  loss: 7.0656  loss_lm: 6.8996 (6.8668)  time: 2.6957  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 156/4722]  eta: 3:25:08  lr: 0.000010  loss: 6.9507  loss_lm: 6.9657 (7.0185)  time: 2.6956  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [ 157/4722]  eta: 3:17:23  lr: 0.000010  loss: 6.7928  loss_lm: 6.7781 (6.7760)  time: 2.5944  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 157/4722]  eta: 3:17:23  lr: 0.000010  loss: 7.0593  loss_lm: 6.9799 (6.9821)  time: 2.5943  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 157/4722]  eta: 3:17:23  lr: 0.000010  loss: 7.1116  loss_lm: 6.9657 (7.0262)  time: 2.5944  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 157/4722]  eta: 3:17:22  lr: 0.000010  loss: 6.8817  loss_lm: 6.8340 (6.8395)  time: 2.5942  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 157/4722]  eta: 3:17:22  lr: 0.000010  loss: 6.4311  loss_lm: 6.7676 (6.8629)  time: 2.5943  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 157/4722]  eta: 3:17:23  lr: 0.000010  loss: 7.4565  loss_lm: 6.8996 (6.9160)  time: 2.5944  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:47  lr: 0.000010  loss: 6.6765  loss_lm: 6.9657 (6.9993)  time: 2.5082  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:47  lr: 0.000010  loss: 7.0239  loss_lm: 7.0239 (6.9853)  time: 2.5082  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:47  lr: 0.000010  loss: 7.6187  loss_lm: 6.9137 (6.9700)  time: 2.5082  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 158/4722]  eta: 3:10:46  lr: 0.000010  loss: 7.0636  loss_lm: 6.7772 (6.8783)  time: 2.5081  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 158/4722]  eta: 3:10:46  lr: 0.000010  loss: 6.9063  loss_lm: 6.8817 (6.8447)  time: 2.5081  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:47  lr: 0.000010  loss: 6.5683  loss_lm: 6.7781 (6.7600)  time: 2.5082  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 159/4722]  eta: 3:05:07  lr: 0.000010  loss: 6.6381  loss_lm: 6.8996 (6.9463)  time: 2.4343  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 159/4722]  eta: 3:05:07  lr: 0.000010  loss: 6.5894  loss_lm: 6.9608 (6.9700)  time: 2.4343  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 159/4722]  eta: 3:05:07  lr: 0.000010  loss: 6.7860  loss_lm: 6.7772 (6.8717)  time: 2.4342  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 159/4722]  eta: 3:05:07  lr: 0.000010  loss: 7.0154  loss_lm: 6.8817 (6.8569)  time: 2.4342  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 159/4722]  eta: 3:05:07  lr: 0.000010  loss: 6.8581  loss_lm: 6.7781 (6.7670)  time: 2.4343  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 159/4722]  eta: 3:05:07  lr: 0.000010  loss: 6.7705  loss_lm: 6.9799 (6.9699)  time: 2.4342  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:56  lr: 0.000010  loss: 7.1150  loss_lm: 6.7860 (6.8880)  time: 2.7217  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:56  lr: 0.000010  loss: 7.0274  loss_lm: 6.9657 (6.9739)  time: 2.7217  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:56  lr: 0.000010  loss: 6.8298  loss_lm: 6.7802 (6.7712)  time: 2.7217  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:56  lr: 0.000010  loss: 6.8794  loss_lm: 6.8817 (6.8584)  time: 2.7216  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:56  lr: 0.000010  loss: 6.9717  loss_lm: 6.9799 (6.9701)  time: 2.7217  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:56  lr: 0.000010  loss: 6.3037  loss_lm: 6.8996 (6.9035)  time: 2.7218  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 161/4722]  eta: 3:21:18  lr: 0.000010  loss: 6.7991  loss_lm: 6.9608 (6.9629)  time: 2.6483  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 161/4722]  eta: 3:21:18  lr: 0.000010  loss: 6.6253  loss_lm: 6.7945 (6.8861)  time: 2.6483  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 161/4722]  eta: 3:21:18  lr: 0.000010  loss: 7.3029  loss_lm: 6.8817 (6.8861)  time: 2.6482  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 161/4722]  eta: 3:21:18  lr: 0.000010  loss: 6.5928  loss_lm: 6.7781 (6.7601)  time: 2.6483  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 161/4722]  eta: 3:21:18  lr: 0.000010  loss: 6.7502  loss_lm: 6.9717 (6.9563)  time: 2.6483  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 161/4722]  eta: 3:21:18  lr: 0.000010  loss: 6.8811  loss_lm: 6.7860 (6.8875)  time: 2.6482  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:03  lr: 0.000010  loss: 6.7925  loss_lm: 6.7945 (6.8806)  time: 2.5798  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:03  lr: 0.000010  loss: 6.9863  loss_lm: 6.9657 (6.9643)  time: 2.5798  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:03  lr: 0.000010  loss: 7.1517  loss_lm: 6.8811 (6.9031)  time: 2.5797  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:03  lr: 0.000010  loss: 6.7078  loss_lm: 6.7781 (6.7570)  time: 2.5797  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:03  lr: 0.000010  loss: 6.4332  loss_lm: 6.9717 (6.9256)  time: 2.5797  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:03  lr: 0.000010  loss: 6.9315  loss_lm: 6.8860 (6.8888)  time: 2.5797  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 163/4722]  eta: 3:11:20  lr: 0.000010  loss: 6.7786  loss_lm: 6.7925 (6.8749)  time: 2.5183  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 163/4722]  eta: 3:11:20  lr: 0.000010  loss: 6.2452  loss_lm: 6.9608 (6.9244)  time: 2.5183  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 163/4722]  eta: 3:11:20  lr: 0.000010  loss: 6.9574  loss_lm: 6.8811 (6.9061)  time: 2.5182  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 163/4722]  eta: 3:11:20  lr: 0.000010  loss: 7.1419  loss_lm: 6.7781 (6.7784)  time: 2.5182  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 163/4722]  eta: 3:11:20  lr: 0.000010  loss: 6.9166  loss_lm: 6.9166 (6.9251)  time: 2.5182  data: 0.0000  max mem: 18865


Train: data epoch: [0]  [ 163/4722]  eta: 3:11:20  lr: 0.000010  loss: 6.2981  loss_lm: 6.8817 (6.8560)  time: 2.5182  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:08  lr: 0.000010  loss: 7.0808  loss_lm: 6.7945 (6.8858)  time: 2.4634  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:08  lr: 0.000010  loss: 6.5212  loss_lm: 6.9608 (6.9032)  time: 2.4634  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:07  lr: 0.000010  loss: 7.0608  loss_lm: 6.9409 (6.9142)  time: 2.4633  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:07  lr: 0.000010  loss: 6.7461  loss_lm: 6.8817 (6.8502)  time: 2.4633  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:08  lr: 0.000010  loss: 6.5292  loss_lm: 6.9166 (6.9042)  time: 2.4634  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:08  lr: 0.000010  loss: 6.9043  loss_lm: 6.7802 (6.7850)  time: 2.4634  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:01  lr: 0.000010  loss: 6.9445  loss_lm: 6.9507 (6.9052)  time: 2.6732  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:01  lr: 0.000010  loss: 6.5799  loss_lm: 6.8726 (6.8880)  time: 2.6732  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:01  lr: 0.000010  loss: 6.8303  loss_lm: 6.7802 (6.7873)  time: 2.6732  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:01  lr: 0.000010  loss: 6.8853  loss_lm: 6.8853 (6.9128)  time: 2.6731  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:01  lr: 0.000010  loss: 6.8411  loss_lm: 6.7945 (6.8835)  time: 2.6732  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 165/4722]  eta: 3:23:01  lr: 0.000010  loss: 6.8744  loss_lm: 6.8794 (6.8514)  time: 2.6731  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 166/4722]  eta: 3:18:39  lr: 0.000010  loss: 6.4885  loss_lm: 6.7945 (6.8647)  time: 2.5375  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:39  lr: 0.000010  loss: 6.7390  loss_lm: 6.7802 (6.7850)  time: 2.5375  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:39  lr: 0.000010  loss: 6.5607  loss_lm: 6.9507 (6.8888)  time: 2.5375  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:39  lr: 0.000010  loss: 6.8120  loss_lm: 6.8744 (6.8495)  time: 2.5375  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:39  lr: 0.000010  loss: 6.7295  loss_lm: 6.8853 (6.9041)  time: 2.5375  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:39  lr: 0.000010  loss: 6.8594  loss_lm: 6.8726 (6.8866)  time: 2.5375  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:42  lr: 0.000010  loss: 6.7438  loss_lm: 6.9445 (6.8822)  time: 2.5375  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:42  lr: 0.000010  loss: 6.8654  loss_lm: 6.7945 (6.8648)  time: 2.5374  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:42  lr: 0.000010  loss: 7.1475  loss_lm: 6.9409 (6.9151)  time: 2.5374  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:42  lr: 0.000010  loss: 6.8655  loss_lm: 6.8655 (6.8857)  time: 2.5374  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:42  lr: 0.000010  loss: 6.6548  loss_lm: 6.8744 (6.8407)  time: 2.5374  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:42  lr: 0.000010  loss: 7.0743  loss_lm: 6.7928 (6.7981)  time: 2.5374  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 168/4722]  eta: 3:11:04  lr: 0.000010  loss: 6.8725  loss_lm: 6.7945 (6.8651)  time: 2.5376  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 168/4722]  eta: 3:11:04  lr: 0.000010  loss: 6.4240  loss_lm: 6.8853 (6.8938)  time: 2.5376  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 168/4722]  eta: 3:11:04  lr: 0.000010  loss: 6.4866  loss_lm: 6.8700 (6.8650)  time: 2.5377  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 168/4722]  eta: 3:11:04  lr: 0.000010  loss: 6.9764  loss_lm: 6.8200 (6.8059)  time: 2.5376  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 168/4722]  eta: 3:11:04  lr: 0.000010  loss: 7.2696  loss_lm: 6.8655 (6.9024)  time: 2.5376  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 168/4722]  eta: 3:11:04  lr: 0.000010  loss: 6.9792  loss_lm: 6.8794 (6.8467)  time: 2.5376  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 169/4722]  eta: 3:07:43  lr: 0.000010  loss: 6.7126  loss_lm: 6.7945 (6.8587)  time: 2.5374  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 169/4722]  eta: 3:07:43  lr: 0.000010  loss: 6.8110  loss_lm: 6.8194 (6.8628)  time: 2.5374  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [ 169/4722]  eta: 3:07:43  lr: 0.000010  loss: 6.9435  loss_lm: 6.8853 (6.8958)  time: 2.5374  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 169/4722]  eta: 3:07:43  lr: 0.000010  loss: 6.7014  loss_lm: 6.8200 (6.8015)  time: 2.5374  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 169/4722]  eta: 3:07:43  lr: 0.000010  loss: 6.6043  loss_lm: 6.8655 (6.8900)  time: 2.5374  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 169/4722]  eta: 3:07:43  lr: 0.000010  loss: 6.8141  loss_lm: 6.8794 (6.8454)  time: 2.5374  data: 0.0000  max mem: 18865
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2749889) of binary: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/bin/python
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-11-13_00:24:37
  host      : sc4
  rank      : 7 (local_rank: 1)
  exitcode  : 1 (pid: 2749890)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-11-13_00:24:37
  host      : sc4
  rank      : 8 (local_rank: 2)
  exitcode  : 1 (pid: 2749891)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-11-13_00:24:37
  host      : sc4
  rank      : 9 (local_rank: 3)
  exitcode  : 1 (pid: 2749892)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-11-13_00:24:37
  host      : sc4
  rank      : 10 (local_rank: 4)
  exitcode  : 1 (pid: 2749893)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-11-13_00:24:37
  host      : sc4
  rank      : 11 (local_rank: 5)
  exitcode  : 1 (pid: 2749894)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-13_00:24:37
  host      : sc4
  rank      : 6 (local_rank: 0)
  exitcode  : 1 (pid: 2749889)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

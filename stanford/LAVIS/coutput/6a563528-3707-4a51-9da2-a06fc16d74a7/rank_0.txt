WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-12 23:42:54.876208] Starting job[2023-11-12 23:42:54.876212] Starting job

[2023-11-12 23:42:54.876212] Starting job
[2023-11-12 23:42:54.876217] Starting job[2023-11-12 23:42:54.876222] Starting job[2023-11-12 23:42:54.876222] Starting job


| distributed init (rank 0, world 12): env://
| distributed init (rank 5, world 12): env://
| distributed init (rank 2, world 12): env://
| distributed init (rank 1, world 12): env://
| distributed init (rank 4, world 12): env://
| distributed init (rank 3, world 12): env://
2023-11-12 23:42:58,390 [INFO] 
=====  Running Parameters    =====
2023-11-12 23:42:58,391 [INFO] {
    "amp": true,
    "batch_size_eval": 10,
    "batch_size_train": 10,
    "checkpoint_freq": 5,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "eval_freq": 2,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 1e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 20,
    "min_len": 5,
    "min_lr": 0,
    "num_beams": 3,
    "num_workers": 3,
    "output_dir": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint",
    "rank": 0,
    "resume_ckpt_path": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth",
    "runner": "runner_base",
    "seed": 42,
    "task": "captioning",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "weight_decay": 0.05,
    "world_size": 12
}
2023-11-12 23:42:58,391 [INFO] 
======  Dataset Attributes  ======
2023-11-12 23:42:58,391 [INFO] 
======== coco_caption =======
2023-11-12 23:42:58,391 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/mnt/.node1/Open-Datasets/coco"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        },
        "train": {
            "name": "blip_caption",
            "prompt": "a picture of "
        }
    },
    "vis_processor": {
        "eval": {
            "name": "blip_image_eval"
        },
        "train": {
            "name": "blip_image_train"
        }
    }
}
2023-11-12 23:42:58,391 [INFO] 
======  Model Attributes  ======
2023-11-12 23:42:58,392 [INFO] {
    "arch": "blip_caption",
    "finetuned": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth",
    "image_size": 384,
    "load_finetuned": false,
    "load_pretrained": false,
    "med_config_path": "configs/models/med_large_config.json",
    "model_type": "large_coco",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth",
    "prompt": "a picture of ",
    "vit_ckpt_layer": 5,
    "vit_grad_ckpt": true,
    "vit_type": "large"
}
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-11-12 23:42:58,393 [INFO] Building datasets...
2023-11-12 23:43:05,113 [INFO] Checkpoint path is: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth
2023-11-12 23:43:05,113 [INFO] Checkpoint path specified but not found, starting from scratch instead.
2023-11-12 23:43:05,113 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-11-12 23:43:05,113 [INFO] Loaded 566747 records for train split from the dataset.
2023-11-12 23:43:05,113 [INFO] Loaded 5000 records for val split from the dataset.
2023-11-12 23:43:05,113 [INFO] Loaded 5000 records for test split from the dataset.
2023-11-12 23:43:05,114 [INFO] Start training
2023-11-12 23:43:06,075 [INFO] number of trainable parameters: 446290492
2023-11-12 23:43:06,076 [INFO] Start training epoch 0, 4722 iters per inner epoch.
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   1/4722]  eta: 6:01:21  lr: 0.000010  loss: 10.4962  loss_lm: 10.4962 (10.4962)  time: 4.5925  data: 0.0000  max mem: 15459Train: data epoch: [0]  [   1/4722]  eta: 6:01:15  lr: 0.000010  loss: 10.4487  loss_lm: 10.4487 (10.4487)  time: 4.5913  data: 0.0000  max mem: 15461

Train: data epoch: [0]  [   1/4722]  eta: 6:01:15  lr: 0.000010  loss: 10.4112  loss_lm: 10.4112 (10.4112)  time: 4.5913  data: 0.0000  max mem: 15460Train: data epoch: [0]  [   1/4722]  eta: 6:01:20  lr: 0.000010  loss: 10.4045  loss_lm: 10.4045 (10.4045)  time: 4.5923  data: 0.0000  max mem: 15460
Train: data epoch: [0]  [   1/4722]  eta: 6:01:15  lr: 0.000010  loss: 10.4579  loss_lm: 10.4579 (10.4579)  time: 4.5913  data: 0.0000  max mem: 15465

Train: data epoch: [0]  [   1/4722]  eta: 6:01:16  lr: 0.000010  loss: 10.3626  loss_lm: 10.3626 (10.3626)  time: 4.5916  data: 0.0000  max mem: 15461
2023-11-12 23:43:10,676 [INFO] Reducer buckets have been rebuilt in this iteration.
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   2/4722]  eta: 3:58:35  lr: 0.000010  loss: 9.8384  loss_lm: 9.8384 (10.1435)  time: 3.0330  data: 0.0000  max mem: 18857
Train: data epoch: [0]  [   2/4722]  eta: 3:58:38  lr: 0.000010  loss: 9.7323  loss_lm: 9.7323 (10.1143)  time: 3.0336  data: 0.0000  max mem: 18872
Train: data epoch: [0]  [   2/4722]  eta: 3:58:35  lr: 0.000010  loss: 9.7306  loss_lm: 9.7306 (10.0709)  time: 3.0330  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [   2/4722]  eta: 3:58:35  lr: 0.000010  loss: 9.8779  loss_lm: 9.8779 (10.1202)  time: 3.0330  data: 0.0000  max mem: 18857
Train: data epoch: [0]  [   2/4722]  eta: 3:58:38  lr: 0.000010  loss: 9.9044  loss_lm: 9.9044 (10.1544)  time: 3.0335  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [   2/4722]  eta: 3:58:35  lr: 0.000010  loss: 9.8077  loss_lm: 9.8077 (10.1328)  time: 3.0330  data: 0.0000  max mem: 18860
Train: data epoch: [0]  [   3/4722]  eta: 3:17:20  lr: 0.000010  loss: 9.2446  loss_lm: 9.8384 (9.8439)  time: 2.5090  data: 0.0000  max mem: 18864Train: data epoch: [0]  [   3/4722]  eta: 3:17:19  lr: 0.000010  loss: 9.4529  loss_lm: 9.8779 (9.8978)  time: 2.5090  data: 0.0000  max mem: 18861

Train: data epoch: [0]  [   3/4722]  eta: 3:17:21  lr: 0.000010  loss: 9.4852  loss_lm: 9.9044 (9.9313)  time: 2.5093  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [   3/4722]  eta: 3:17:21  lr: 0.000010  loss: 9.7294  loss_lm: 9.7323 (9.9860)  time: 2.5094  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [   3/4722]  eta: 3:17:19  lr: 0.000010  loss: 9.4403  loss_lm: 9.8077 (9.9019)  time: 2.5090  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [   3/4722]  eta: 3:17:19  lr: 0.000010  loss: 9.6477  loss_lm: 9.7306 (9.9298)  time: 2.5090  data: 0.0000  max mem: 18873
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   4/4722]  eta: 2:56:43  lr: 0.000010  loss: 9.2046  loss_lm: 9.4852 (9.7497)  time: 2.2474  data: 0.0000  max mem: 18871
Train: data epoch: [0]  [   4/4722]  eta: 2:56:42  lr: 0.000010  loss: 9.1741  loss_lm: 9.4529 (9.7169)  time: 2.2472  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [   4/4722]  eta: 2:56:42  lr: 0.000010  loss: 9.3771  loss_lm: 9.3771 (9.7272)  time: 2.2472  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [   4/4722]  eta: 2:56:43  lr: 0.000010  loss: 9.2210  loss_lm: 9.7294 (9.7947)  time: 2.2475  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [   4/4722]  eta: 2:56:42  lr: 0.000010  loss: 9.2295  loss_lm: 9.4403 (9.7338)  time: 2.2472  data: 0.0000  max mem: 18861Train: data epoch: [0]  [   4/4722]  eta: 2:56:42  lr: 0.000010  loss: 9.2732  loss_lm: 9.6477 (9.7657)  time: 2.2472  data: 0.0000  max mem: 18873

2023-11-12 23:43:16,529 [INFO] Saving checkpoint at iters: 5 and epoch: 0
2023-11-12 23:43:16,534 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:43:21,262 [INFO] Saved successfully
2023-11-12 23:43:21,262 [INFO] Averaged stats: lr: 0.0000  loss: 9.5994  loss_lm: 9.5994
Train: data epoch: [0]  [   5/4722]  eta: 3:58:43  lr: 0.000010  loss: 9.1294  loss_lm: 9.4529 (9.5994)  time: 3.0366  data: 0.0000  max mem: 18861
Train: data epoch: [0]  [   5/4722]  eta: 3:58:43  lr: 0.000010  loss: 9.2901  loss_lm: 9.4403 (9.6451)  time: 3.0366  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [   5/4722]  eta: 3:58:44  lr: 0.000010  loss: 9.1479  loss_lm: 9.7294 (9.6654)  time: 3.0369  data: 0.0000  max mem: 18876Train: data epoch: [0]  [   5/4722]  eta: 3:58:43  lr: 0.000010  loss: 8.9342  loss_lm: 9.3771 (9.5686)  time: 3.0366  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [   5/4722]  eta: 3:58:43  lr: 0.000010  loss: 9.0325  loss_lm: 9.6477 (9.6190)  time: 3.0366  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [   5/4722]  eta: 3:58:44  lr: 0.000010  loss: 9.1791  loss_lm: 9.4852 (9.6356)  time: 3.0368  data: 0.0000  max mem: 18871
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   6/4722]  eta: 3:38:18  lr: 0.000010  loss: 9.2100  loss_lm: 9.2100 (9.5646)  time: 2.7775  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [   6/4722]  eta: 3:38:17  lr: 0.000010  loss: 8.8080  loss_lm: 9.2901 (9.5056)  time: 2.7773  data: 0.0000  max mem: 18865Train: data epoch: [0]  [   6/4722]  eta: 3:38:18  lr: 0.000010  loss: 8.9020  loss_lm: 9.1741 (9.4831)  time: 2.7775  data: 0.0000  max mem: 18862

Train: data epoch: [0]  [   6/4722]  eta: 3:38:17  lr: 0.000010  loss: 9.1120  loss_lm: 9.2732 (9.5345)  time: 2.7773  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [   6/4722]  eta: 3:38:17  lr: 0.000010  loss: 9.0456  loss_lm: 9.2446 (9.4814)  time: 2.7773  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [   6/4722]  eta: 3:38:18  lr: 0.000010  loss: 9.0269  loss_lm: 9.2210 (9.5590)  time: 2.7775  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [   7/4722]  eta: 3:23:27  lr: 0.000010  loss: 9.0136  loss_lm: 9.1741 (9.4161)  time: 2.5891  data: 0.0000  max mem: 18862
Train: data epoch: [0]  [   7/4722]  eta: 3:23:27  lr: 0.000010  loss: 9.1074  loss_lm: 9.2100 (9.4993)  time: 2.5891  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [   7/4722]  eta: 3:23:26  lr: 0.000010  loss: 8.6539  loss_lm: 9.2732 (9.4087)  time: 2.5889  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [   7/4722]  eta: 3:23:26  lr: 0.000010  loss: 8.9094  loss_lm: 9.2446 (9.3997)  time: 2.5889  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [   7/4722]  eta: 3:23:27  lr: 0.000010  loss: 8.9434  loss_lm: 9.2210 (9.4710)  time: 2.5891  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [   7/4722]  eta: 3:23:26  lr: 0.000010  loss: 9.0301  loss_lm: 9.2901 (9.4376)  time: 2.5889  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [   8/4722]  eta: 3:12:18  lr: 0.000010  loss: 8.7594  loss_lm: 9.1120 (9.3276)  time: 2.4478  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [   8/4722]  eta: 3:12:19  lr: 0.000010  loss: 8.6562  loss_lm: 9.2046 (9.3939)  time: 2.4479  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [   8/4722]  eta: 3:12:18  lr: 0.000010  loss: 9.0656  loss_lm: 9.0656 (9.3580)  time: 2.4478  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [   8/4722]  eta: 3:12:19  lr: 0.000010  loss: 8.5681  loss_lm: 9.1479 (9.3582)  time: 2.4479  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [   8/4722]  eta: 3:12:18  lr: 0.000010  loss: 8.7896  loss_lm: 9.2295 (9.3566)  time: 2.4478  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [   8/4722]  eta: 3:12:19  lr: 0.000010  loss: 9.0040  loss_lm: 9.1294 (9.3646)  time: 2.4479  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [   9/4722]  eta: 3:03:41  lr: 0.000010  loss: 8.6398  loss_lm: 9.2046 (9.3101)  time: 2.3385  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [   9/4722]  eta: 3:03:40  lr: 0.000010  loss: 8.9553  loss_lm: 9.1120 (9.2862)  time: 2.3384  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [   9/4722]  eta: 3:03:41  lr: 0.000010  loss: 9.1400  loss_lm: 9.1400 (9.3396)  time: 2.3385  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [   9/4722]  eta: 3:03:40  lr: 0.000010  loss: 8.9719  loss_lm: 9.0656 (9.3151)  time: 2.3384  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [   9/4722]  eta: 3:03:41  lr: 0.000010  loss: 8.9203  loss_lm: 9.1479 (9.3095)  time: 2.3385  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [   9/4722]  eta: 3:03:40  lr: 0.000010  loss: 8.8070  loss_lm: 9.2295 (9.2956)  time: 2.3384  data: 0.0000  max mem: 18865
2023-11-12 23:43:28,604 [INFO] Saving checkpoint at iters: 10 and epoch: 0
2023-11-12 23:43:28,608 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:43:33,952 [INFO] Saved successfully
2023-11-12 23:43:33,953 [INFO] Averaged stats: lr: 0.0000  loss: 9.2694  loss_lm: 9.2694
Train: data epoch: [0]  [  10/4722]  eta: 3:38:52  lr: 0.000010  loss: 8.6372  loss_lm: 9.1294 (9.2694)  time: 2.7871  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  10/4722]  eta: 3:38:52  lr: 0.000010  loss: 8.6904  loss_lm: 9.0269 (9.2476)  time: 2.7871  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  10/4722]  eta: 3:38:52  lr: 0.000010  loss: 8.5090  loss_lm: 9.0301 (9.2169)  time: 2.7870  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  10/4722]  eta: 3:38:52  lr: 0.000010  loss: 8.7043  loss_lm: 9.0456 (9.2540)  time: 2.7870  data: 0.0000  max mem: 18870

Train: data epoch: [0]  [  10/4722]  eta: 3:38:52  lr: 0.000010  loss: 8.6432  loss_lm: 9.0325 (9.2219)  time: 2.7870  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  10/4722]  eta: 3:38:52  lr: 0.000010  loss: 8.9725  loss_lm: 9.1791 (9.2764)  time: 2.7871  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  11/4722]  eta: 3:29:22  lr: 0.000010  loss: 8.6903  loss_lm: 9.1791 (9.2231)  time: 2.6666  data: 0.0000  max mem: 18873Train: data epoch: [0]  [  11/4722]  eta: 3:29:22  lr: 0.000010  loss: 8.7905  loss_lm: 9.0269 (9.2061)  time: 2.6666  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  11/4722]  eta: 3:29:21  lr: 0.000010  loss: 9.0051  loss_lm: 9.0325 (9.2022)  time: 2.6665  data: 0.0000  max mem: 18885

Train: data epoch: [0]  [  11/4722]  eta: 3:29:21  lr: 0.000010  loss: 8.9398  loss_lm: 9.0456 (9.2254)  time: 2.6665  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  11/4722]  eta: 3:29:21  lr: 0.000010  loss: 8.7892  loss_lm: 9.0301 (9.1780)  time: 2.6665  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  11/4722]  eta: 3:29:22  lr: 0.000010  loss: 8.4584  loss_lm: 9.1294 (9.1956)  time: 2.6667  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  12/4722]  eta: 3:21:26  lr: 0.000010  loss: 8.7556  loss_lm: 9.1074 (9.1841)  time: 2.5660  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  12/4722]  eta: 3:21:25  lr: 0.000010  loss: 8.4098  loss_lm: 8.9434 (9.1397)  time: 2.5660  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  12/4722]  eta: 3:21:26  lr: 0.000010  loss: 8.8687  loss_lm: 9.0136 (9.1684)  time: 2.5661  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  12/4722]  eta: 3:21:25  lr: 0.000010  loss: 8.6990  loss_lm: 8.9719 (9.1815)  time: 2.5659  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  12/4722]  eta: 3:21:25  lr: 0.000010  loss: 8.4779  loss_lm: 8.8080 (9.1197)  time: 2.5659  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  12/4722]  eta: 3:21:25  lr: 0.000010  loss: 8.8890  loss_lm: 9.0051 (9.1761)  time: 2.5659  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  13/4722]  eta: 3:14:43  lr: 0.000010  loss: 8.7606  loss_lm: 9.1074 (9.1516)  time: 2.4810  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  13/4722]  eta: 3:14:42  lr: 0.000010  loss: 8.7244  loss_lm: 8.8080 (9.0893)  time: 2.4809  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  13/4722]  eta: 3:14:43  lr: 0.000010  loss: 8.5670  loss_lm: 8.9434 (9.0956)  time: 2.4810  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  13/4722]  eta: 3:14:42  lr: 0.000010  loss: 8.4650  loss_lm: 9.0051 (9.1214)  time: 2.4809  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  13/4722]  eta: 3:14:42  lr: 0.000010  loss: 8.7165  loss_lm: 8.9719 (9.1458)  time: 2.4809  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  13/4722]  eta: 3:14:43  lr: 0.000010  loss: 8.7755  loss_lm: 9.0136 (9.1382)  time: 2.4811  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  14/4722]  eta: 3:08:55  lr: 0.000010  loss: 8.8327  loss_lm: 8.8080 (9.0709)  time: 2.4078  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  14/4722]  eta: 3:08:56  lr: 0.000010  loss: 8.8152  loss_lm: 8.9725 (9.1275)  time: 2.4079  data: 0.0000  max mem: 18873

Train: data epoch: [0]  [  14/4722]  eta: 3:08:56  lr: 0.000010  loss: 8.5469  loss_lm: 8.9203 (9.0565)  time: 2.4079  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  14/4722]  eta: 3:08:55  lr: 0.000010  loss: 8.8351  loss_lm: 8.9398 (9.1236)  time: 2.4078  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  14/4722]  eta: 3:08:55  lr: 0.000010  loss: 8.8075  loss_lm: 8.9553 (9.0990)  time: 2.4078  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  14/4722]  eta: 3:08:56  lr: 0.000010  loss: 8.7275  loss_lm: 9.0040 (9.1088)  time: 2.4079  data: 0.0000  max mem: 18868
2023-11-12 23:43:41,253 [INFO] Saving checkpoint at iters: 15 and epoch: 0
2023-11-12 23:43:41,258 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:43:46,578 [INFO] Saved successfully
2023-11-12 23:43:46,579 [INFO] Averaged stats: lr: 0.0000  loss: 9.0675  loss_lm: 9.0675
Train: data epoch: [0]  [  15/4722]  eta: 3:31:46  lr: 0.000010  loss: 8.4893  loss_lm: 9.0040 (9.0675)  time: 2.6996  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  15/4722]  eta: 3:31:46  lr: 0.000010  loss: 8.7052  loss_lm: 8.9203 (9.0330)  time: 2.6995  data: 0.0000  max mem: 18876Train: data epoch: [0]  [  15/4722]  eta: 3:31:46  lr: 0.000010  loss: 8.5643  loss_lm: 8.9553 (9.0633)  time: 2.6994  data: 0.0000  max mem: 18885

Train: data epoch: [0]  [  15/4722]  eta: 3:31:46  lr: 0.000010  loss: 8.5498  loss_lm: 8.9398 (9.0853)  time: 2.6994  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  15/4722]  eta: 3:31:46  lr: 0.000010  loss: 8.3916  loss_lm: 8.9725 (9.0785)  time: 2.6996  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  15/4722]  eta: 3:31:46  lr: 0.000010  loss: 8.6958  loss_lm: 8.8080 (9.0459)  time: 2.6995  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  16/4722]  eta: 3:25:39  lr: 0.000010  loss: 8.1517  loss_lm: 8.8152 (9.0205)  time: 2.6220  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  16/4722]  eta: 3:25:38  lr: 0.000010  loss: 8.4734  loss_lm: 8.8890 (9.0265)  time: 2.6219  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  16/4722]  eta: 3:25:39  lr: 0.000010  loss: 8.3853  loss_lm: 8.7905 (8.9925)  time: 2.6220  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  16/4722]  eta: 3:25:39  lr: 0.000010  loss: 8.8408  loss_lm: 8.9020 (9.0534)  time: 2.6221  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  16/4722]  eta: 3:25:38  lr: 0.000010  loss: 8.6142  loss_lm: 8.9342 (9.0559)  time: 2.6219  data: 0.0000  max mem: 18870Train: data epoch: [0]  [  16/4722]  eta: 3:25:38  lr: 0.000010  loss: 8.8278  loss_lm: 8.8080 (9.0323)  time: 2.6219  data: 0.0000  max mem: 18866

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  17/4722]  eta: 3:20:15  lr: 0.000010  loss: 8.7694  loss_lm: 8.8152 (9.0058)  time: 2.5538  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  17/4722]  eta: 3:20:16  lr: 0.000010  loss: 8.5260  loss_lm: 8.9020 (9.0223)  time: 2.5539  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  17/4722]  eta: 3:20:15  lr: 0.000010  loss: 8.8716  loss_lm: 8.8278 (9.0228)  time: 2.5538  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  17/4722]  eta: 3:20:15  lr: 0.000010  loss: 8.4930  loss_lm: 8.8890 (8.9951)  time: 2.5537  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  17/4722]  eta: 3:20:15  lr: 0.000010  loss: 8.4988  loss_lm: 8.7905 (8.9635)  time: 2.5538  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  17/4722]  eta: 3:20:15  lr: 0.000010  loss: 8.6176  loss_lm: 8.9342 (9.0301)  time: 2.5537  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  18/4722]  eta: 3:15:26  lr: 0.000010  loss: 8.4377  loss_lm: 8.7694 (8.9742)  time: 2.4929  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  18/4722]  eta: 3:15:26  lr: 0.000010  loss: 8.7724  loss_lm: 8.8687 (9.0085)  time: 2.4929  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  18/4722]  eta: 3:15:26  lr: 0.000010  loss: 8.9521  loss_lm: 8.8890 (8.9927)  time: 2.4928  data: 0.0000  max mem: 18885Train: data epoch: [0]  [  18/4722]  eta: 3:15:26  lr: 0.000010  loss: 8.5308  loss_lm: 8.9094 (9.0024)  time: 2.4928  data: 0.0000  max mem: 18870Train: data epoch: [0]  [  18/4722]  eta: 3:15:26  lr: 0.000010  loss: 8.5857  loss_lm: 8.7052 (8.9425)  time: 2.4929  data: 0.0000  max mem: 18876

Train: data epoch: [0]  [  18/4722]  eta: 3:15:26  lr: 0.000010  loss: 8.6314  loss_lm: 8.8080 (9.0011)  time: 2.4928  data: 0.0000  max mem: 19003

Train: data epoch: [0]  [  19/4722]  eta: 3:11:07  lr: 0.000010  loss: 8.7533  loss_lm: 8.8687 (8.9950)  time: 2.4383  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  19/4722]  eta: 3:11:07  lr: 0.000010  loss: 8.5292  loss_lm: 8.7694 (8.9508)  time: 2.4383  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  19/4722]  eta: 3:11:07  lr: 0.000010  loss: 8.4185  loss_lm: 8.7052 (8.9149)  time: 2.4383  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  19/4722]  eta: 3:11:06  lr: 0.000010  loss: 8.6551  loss_lm: 8.9094 (8.9841)  time: 2.4382  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  19/4722]  eta: 3:11:06  lr: 0.000010  loss: 8.3808  loss_lm: 8.8890 (8.9605)  time: 2.4382  data: 0.0000  max mem: 18885Train: data epoch: [0]  [  19/4722]  eta: 3:11:06  lr: 0.000010  loss: 8.3669  loss_lm: 8.8080 (8.9677)  time: 2.4382  data: 0.0000  max mem: 19003

2023-11-12 23:43:53,872 [INFO] Saving checkpoint at iters: 20 and epoch: 0
2023-11-12 23:43:53,877 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:43:59,236 [INFO] Saved successfully
2023-11-12 23:43:59,237 [INFO] Averaged stats: lr: 0.0000  loss: 8.9808  loss_lm: 8.9808
Train: data epoch: [0]  [  20/4722]  eta: 3:28:15  lr: 0.000010  loss: 8.7114  loss_lm: 8.8408 (8.9808)  time: 2.6575  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  20/4722]  eta: 3:28:14  lr: 0.000010  loss: 8.4944  loss_lm: 8.8070 (8.9440)  time: 2.6573  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  20/4722]  eta: 3:28:14  lr: 0.000010  loss: 8.4580  loss_lm: 8.8075 (8.9354)  time: 2.6573  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  20/4722]  eta: 3:28:15  lr: 0.000010  loss: 8.6816  loss_lm: 8.6904 (8.9033)  time: 2.6574  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  20/4722]  eta: 3:28:14  lr: 0.000010  loss: 8.4580  loss_lm: 8.8351 (8.9578)  time: 2.6573  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  20/4722]  eta: 3:28:15  lr: 0.000010  loss: 8.6414  loss_lm: 8.7606 (8.9353)  time: 2.6574  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  21/4722]  eta: 3:23:45  lr: 0.000010  loss: 8.2785  loss_lm: 8.7556 (8.9040)  time: 2.5009  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  21/4722]  eta: 3:23:44  lr: 0.000010  loss: 8.6827  loss_lm: 8.7896 (8.9316)  time: 2.5009  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  21/4722]  eta: 3:23:44  lr: 0.000010  loss: 8.5061  loss_lm: 8.7165 (8.9363)  time: 2.5009  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  21/4722]  eta: 3:23:45  lr: 0.000010  loss: 8.3794  loss_lm: 8.6816 (8.8783)  time: 2.5009  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  21/4722]  eta: 3:23:45  lr: 0.000010  loss: 8.9670  loss_lm: 8.8408 (8.9802)  time: 2.5011  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  21/4722]  eta: 3:23:44  lr: 0.000010  loss: 8.5310  loss_lm: 8.7594 (8.9161)  time: 2.5009  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  22/4722]  eta: 3:19:44  lr: 0.000010  loss: 8.8710  loss_lm: 8.8408 (8.9752)  time: 2.5015  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  22/4722]  eta: 3:19:43  lr: 0.000010  loss: 8.7275  loss_lm: 8.7275 (8.9075)  time: 2.5014  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  22/4722]  eta: 3:19:43  lr: 0.000010  loss: 8.6717  loss_lm: 8.6717 (8.8689)  time: 2.5014  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  22/4722]  eta: 3:19:43  lr: 0.000010  loss: 8.6538  loss_lm: 8.7043 (8.9234)  time: 2.5014  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  22/4722]  eta: 3:19:43  lr: 0.000010  loss: 8.4699  loss_lm: 8.7892 (8.9106)  time: 2.5014  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  22/4722]  eta: 3:19:44  lr: 0.000010  loss: 8.5581  loss_lm: 8.6903 (8.8883)  time: 2.5014  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  23/4722]  eta: 3:15:58  lr: 0.000010  loss: 8.0792  loss_lm: 8.6539 (8.8715)  time: 2.5012  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  23/4722]  eta: 3:15:58  lr: 0.000010  loss: 8.6132  loss_lm: 8.7244 (8.8977)  time: 2.5013  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  23/4722]  eta: 3:15:58  lr: 0.000010  loss: 8.3951  loss_lm: 8.7755 (8.9500)  time: 2.5014  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  23/4722]  eta: 3:15:58  lr: 0.000010  loss: 8.4253  loss_lm: 8.6990 (8.9018)  time: 2.5013  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  23/4722]  eta: 3:15:58  lr: 0.000010  loss: 8.5282  loss_lm: 8.5857 (8.8541)  time: 2.5013  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  23/4722]  eta: 3:15:58  lr: 0.000010  loss: 8.8516  loss_lm: 8.6903 (8.8867)  time: 2.5013  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  24/4722]  eta: 3:12:31  lr: 0.000010  loss: 8.4248  loss_lm: 8.7724 (8.9281)  time: 2.5011  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  24/4722]  eta: 3:12:30  lr: 0.000010  loss: 8.3060  loss_lm: 8.6432 (8.8480)  time: 2.5009  data: 0.0000  max mem: 18885
Train: data epoch: [0]  [  24/4722]  eta: 3:12:30  lr: 0.000010  loss: 8.1268  loss_lm: 8.6551 (8.8695)  time: 2.5010  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  24/4722]  eta: 3:12:31  lr: 0.000010  loss: 8.9313  loss_lm: 8.5857 (8.8573)  time: 2.5010  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  24/4722]  eta: 3:12:31  lr: 0.000010  loss: 8.2725  loss_lm: 8.6562 (8.8611)  time: 2.5010  data: 0.0000  max mem: 18873Train: data epoch: [0]  [  24/4722]  eta: 3:12:30  lr: 0.000010  loss: 8.4697  loss_lm: 8.6958 (8.8798)  time: 2.5010  data: 0.0000  max mem: 19003

2023-11-12 23:44:06,564 [INFO] Saving checkpoint at iters: 25 and epoch: 0
2023-11-12 23:44:06,569 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:44:11,837 [INFO] Saved successfully
2023-11-12 23:44:11,838 [INFO] Averaged stats: lr: 0.0000  loss: 8.9120  loss_lm: 8.9120
Train: data epoch: [0]  [  25/4722]  eta: 3:25:52  lr: 0.000010  loss: 8.5261  loss_lm: 8.7533 (8.9120)  time: 2.5282  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  25/4722]  eta: 3:25:52  lr: 0.000010  loss: 8.6045  loss_lm: 8.6827 (8.8688)  time: 2.5281  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  25/4722]  eta: 3:25:51  lr: 0.000010  loss: 8.8381  loss_lm: 8.6551 (8.8682)  time: 2.5280  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  25/4722]  eta: 3:25:52  lr: 0.000010  loss: 8.5748  loss_lm: 8.5748 (8.8460)  time: 2.5281  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  25/4722]  eta: 3:25:52  lr: 0.000010  loss: 8.5915  loss_lm: 8.6414 (8.8503)  time: 2.5281  data: 0.0000  max mem: 18873Train: data epoch: [0]  [  25/4722]  eta: 3:25:51  lr: 0.000010  loss: 8.2573  loss_lm: 8.5643 (8.8243)  time: 2.5280  data: 0.0000  max mem: 18885

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  26/4722]  eta: 3:22:17  lr: 0.000010  loss: 8.7635  loss_lm: 8.5748 (8.8428)  time: 2.5268  data: 0.0000  max mem: 18876Train: data epoch: [0]  [  26/4722]  eta: 3:22:17  lr: 0.000010  loss: 8.5168  loss_lm: 8.7275 (8.8968)  time: 2.5269  data: 0.0000  max mem: 19003

Train: data epoch: [0]  [  26/4722]  eta: 3:22:17  lr: 0.000010  loss: 8.3648  loss_lm: 8.6538 (8.8489)  time: 2.5268  data: 0.0000  max mem: 18870Train: data epoch: [0]  [  26/4722]  eta: 3:22:17  lr: 0.000010  loss: 8.2407  loss_lm: 8.5310 (8.8019)  time: 2.5268  data: 0.0000  max mem: 18885

Train: data epoch: [0]  [  26/4722]  eta: 3:22:17  lr: 0.000010  loss: 8.4306  loss_lm: 8.6314 (8.8520)  time: 2.5268  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  26/4722]  eta: 3:22:17  lr: 0.000010  loss: 8.4687  loss_lm: 8.6398 (8.8357)  time: 2.5268  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  27/4722]  eta: 3:18:58  lr: 0.000010  loss: 7.9519  loss_lm: 8.5681 (8.8099)  time: 2.5267  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  27/4722]  eta: 3:18:59  lr: 0.000010  loss: 8.5404  loss_lm: 8.7114 (8.8836)  time: 2.5268  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  27/4722]  eta: 3:18:58  lr: 0.000010  loss: 8.5524  loss_lm: 8.5915 (8.8252)  time: 2.5267  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  27/4722]  eta: 3:18:58  lr: 0.000010  loss: 8.4136  loss_lm: 8.6176 (8.8327)  time: 2.5267  data: 0.0000  max mem: 18870Train: data epoch: [0]  [  27/4722]  eta: 3:18:58  lr: 0.000010  loss: 8.0512  loss_lm: 8.4930 (8.7741)  time: 2.5267  data: 0.0000  max mem: 18885Train: data epoch: [0]  [  27/4722]  eta: 3:18:58  lr: 0.000010  loss: 8.5697  loss_lm: 8.6132 (8.8415)  time: 2.5267  data: 0.0000  max mem: 19003


Train: data epoch: [0]  [  28/4722]  eta: 3:15:54  lr: 0.000010  loss: 8.3275  loss_lm: 8.6372 (8.8638)  time: 2.5267  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  28/4722]  eta: 3:15:54  lr: 0.000010  loss: 8.5532  loss_lm: 8.5670 (8.8007)  time: 2.5265  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  28/4722]  eta: 3:15:53  lr: 0.000010  loss: 8.3532  loss_lm: 8.6045 (8.8241)  time: 2.5265  data: 0.0000  max mem: 19003
Train: data epoch: [0]  [  28/4722]  eta: 3:15:53  lr: 0.000010  loss: 8.4951  loss_lm: 8.6142 (8.8207)  time: 2.5265  data: 0.0000  max mem: 19000
Train: data epoch: [0]  [  28/4722]  eta: 3:15:54  lr: 0.000010  loss: 8.4255  loss_lm: 8.5581 (8.8109)  time: 2.5265  data: 0.0000  max mem: 18873
Train: data epoch: [0]  [  28/4722]  eta: 3:15:53  lr: 0.000010  loss: 8.4201  loss_lm: 8.4734 (8.7614)  time: 2.5265  data: 0.0000  max mem: 18885
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-12 23:49:08.447008] Starting job
[2023-11-12 23:49:08.471952] Starting job
[2023-11-12 23:49:08.478146] Starting job
[2023-11-12 23:49:08.486989] Starting job
[2023-11-12 23:49:08.495925] Starting job
[2023-11-12 23:49:08.496933] Starting job
| distributed init (rank 4, world 12): env://
| distributed init (rank 0, world 12): env://
| distributed init (rank 2, world 12): env://
| distributed init (rank 1, world 12): env://
| distributed init (rank 3, world 12): env://
| distributed init (rank 5, world 12): env://
2023-11-12 23:49:12,439 [INFO] 
=====  Running Parameters    =====
2023-11-12 23:49:12,439 [INFO] {
    "amp": true,
    "batch_size_eval": 10,
    "batch_size_train": 10,
    "checkpoint_freq": 5,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "eval_freq": 2,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 1e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 20,
    "min_len": 5,
    "min_lr": 0,
    "num_beams": 3,
    "num_workers": 3,
    "output_dir": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint",
    "rank": 0,
    "resume_ckpt_path": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth",
    "runner": "runner_base",
    "seed": 42,
    "task": "captioning",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "weight_decay": 0.05,
    "world_size": 12
}
2023-11-12 23:49:12,439 [INFO] 
======  Dataset Attributes  ======
2023-11-12 23:49:12,439 [INFO] 
======== coco_caption =======
2023-11-12 23:49:12,440 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/mnt/.node1/Open-Datasets/coco"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        },
        "train": {
            "name": "blip_caption",
            "prompt": "a picture of "
        }
    },
    "vis_processor": {
        "eval": {
            "name": "blip_image_eval"
        },
        "train": {
            "name": "blip_image_train"
        }
    }
}
2023-11-12 23:49:12,440 [INFO] 
======  Model Attributes  ======
2023-11-12 23:49:12,440 [INFO] {
    "arch": "blip_caption",
    "finetuned": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth",
    "image_size": 384,
    "load_finetuned": false,
    "load_pretrained": false,
    "med_config_path": "configs/models/med_large_config.json",
    "model_type": "large_coco",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth",
    "prompt": "a picture of ",
    "vit_ckpt_layer": 5,
    "vit_grad_ckpt": true,
    "vit_type": "large"
}
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-11-12 23:49:12,441 [INFO] Building datasets...
2023-11-12 23:49:27,318 [INFO] number of trainable parameters: 446290492
2023-11-12 23:49:27,623 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-11-12 23:49:27,623 [INFO] Loaded 566747 records for train split from the dataset.
2023-11-12 23:49:27,623 [INFO] Loaded 5000 records for val split from the dataset.
2023-11-12 23:49:27,623 [INFO] Loaded 5000 records for test split from the dataset.
2023-11-12 23:49:27,624 [INFO] Resume checkpoint from /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth
2023-11-12 23:49:27,626 [INFO] Start training
2023-11-12 23:49:27,637 [INFO] Start training epoch 0, 4722 iters per inner epoch.
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  26/4722]  eta: 5:37:46  lr: 0.000010  loss: 8.3881  loss_lm: 8.3881 (8.3881)  time: 4.3158  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  26/4722]  eta: 5:37:39  lr: 0.000010  loss: 8.4401  loss_lm: 8.4401 (8.4401)  time: 4.3143  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  26/4722]  eta: 5:37:38  lr: 0.000010  loss: 8.2671  loss_lm: 8.2671 (8.2671)  time: 4.3140  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  26/4722]  eta: 5:37:44  lr: 0.000010  loss: 8.4947  loss_lm: 8.4947 (8.4947)  time: 4.3152  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  26/4722]  eta: 5:37:50  lr: 0.000010  loss: 8.7541  loss_lm: 8.7541 (8.7541)  time: 4.3164  data: 0.0000  max mem: 18863Train: data epoch: [0]  [  26/4722]  eta: 5:37:48  lr: 0.000010  loss: 8.4935  loss_lm: 8.4935 (8.4935)  time: 4.3160  data: 0.0000  max mem: 18864

2023-11-12 23:49:31,964 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  27/4722]  eta: 3:47:06  lr: 0.000010  loss: 8.5561  loss_lm: 8.4401 (8.4981)  time: 2.9023  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  27/4722]  eta: 3:47:09  lr: 0.000010  loss: 8.3813  loss_lm: 8.3813 (8.3847)  time: 2.9030  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  27/4722]  eta: 3:47:10  lr: 0.000010  loss: 8.5461  loss_lm: 8.4935 (8.5198)  time: 2.9032  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  27/4722]  eta: 3:47:08  lr: 0.000010  loss: 8.5648  loss_lm: 8.4947 (8.5297)  time: 2.9028  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  27/4722]  eta: 3:47:05  lr: 0.000010  loss: 8.0822  loss_lm: 8.0822 (8.1746)  time: 2.9022  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  27/4722]  eta: 3:47:11  lr: 0.000010  loss: 7.9765  loss_lm: 7.9765 (8.3653)  time: 2.9034  data: 0.0000  max mem: 18863
Train: data epoch: [0]  [  28/4722]  eta: 3:09:56  lr: 0.000010  loss: 8.5302  loss_lm: 8.3881 (8.4332)  time: 2.4280  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  28/4722]  eta: 3:09:54  lr: 0.000010  loss: 8.3369  loss_lm: 8.4401 (8.4444)  time: 2.4275  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  28/4722]  eta: 3:09:57  lr: 0.000010  loss: 8.5386  loss_lm: 8.5386 (8.4231)  time: 2.4282  data: 0.0000  max mem: 18863Train: data epoch: [0]  [  28/4722]  eta: 3:09:57  lr: 0.000010  loss: 8.3050  loss_lm: 8.4935 (8.4482)  time: 2.4280  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  28/4722]  eta: 3:09:53  lr: 0.000010  loss: 8.4246  loss_lm: 8.2671 (8.2580)  time: 2.4273  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  28/4722]  eta: 3:09:55  lr: 0.000010  loss: 8.4218  loss_lm: 8.4947 (8.4938)  time: 2.4277  data: 0.0000  max mem: 18864

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  29/4722]  eta: 2:51:17  lr: 0.000010  loss: 8.4875  loss_lm: 8.3881 (8.4468)  time: 2.1899  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  29/4722]  eta: 2:51:15  lr: 0.000010  loss: 8.0639  loss_lm: 8.3369 (8.3493)  time: 2.1895  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  29/4722]  eta: 2:51:14  lr: 0.000010  loss: 8.4867  loss_lm: 8.2671 (8.3151)  time: 2.1894  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  29/4722]  eta: 2:51:17  lr: 0.000010  loss: 8.3376  loss_lm: 8.3376 (8.4205)  time: 2.1899  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  29/4722]  eta: 2:51:17  lr: 0.000010  loss: 8.5825  loss_lm: 8.5386 (8.4630)  time: 2.1900  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  29/4722]  eta: 2:51:16  lr: 0.000010  loss: 8.4421  loss_lm: 8.4421 (8.4809)  time: 2.1897  data: 0.0000  max mem: 18976
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-12 23:49:37,882 [INFO] Saving checkpoint at iters: 30 and epoch: 0
2023-11-12 23:49:37,888 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:49:43,471 [INFO] Saved successfully
2023-11-12 23:49:43,472 [INFO] Averaged stats: lr: 0.0000  loss: 8.3931  loss_lm: 8.3931
Train: data epoch: [0]  [  30/4722]  eta: 4:07:34  lr: 0.000010  loss: 8.2835  loss_lm: 8.3376 (8.3931)  time: 3.1660  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  30/4722]  eta: 4:07:32  lr: 0.000010  loss: 8.2053  loss_lm: 8.3369 (8.3205)  time: 3.1656  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  30/4722]  eta: 4:07:34  lr: 0.000010  loss: 8.3195  loss_lm: 8.3881 (8.4213)  time: 3.1659  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  30/4722]  eta: 4:07:32  lr: 0.000010  loss: 7.8927  loss_lm: 8.2671 (8.2306)  time: 3.1655  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  30/4722]  eta: 4:07:34  lr: 0.000010  loss: 8.7087  loss_lm: 8.5825 (8.5121)  time: 3.1660  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  30/4722]  eta: 4:07:33  lr: 0.000010  loss: 8.5765  loss_lm: 8.4947 (8.5000)  time: 3.1657  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  31/4722]  eta: 3:45:34  lr: 0.000010  loss: 8.0536  loss_lm: 8.2053 (8.2760)  time: 2.8853  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  31/4722]  eta: 3:45:36  lr: 0.000010  loss: 8.3178  loss_lm: 8.3813 (8.4041)  time: 2.8856  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  31/4722]  eta: 3:45:34  lr: 0.000010  loss: 8.4856  loss_lm: 8.2671 (8.2731)  time: 2.8852  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  31/4722]  eta: 3:45:36  lr: 0.000010  loss: 8.2806  loss_lm: 8.5386 (8.4735)  time: 2.8856  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  31/4722]  eta: 3:45:35  lr: 0.000010  loss: 8.3075  loss_lm: 8.4421 (8.4679)  time: 2.8854  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  31/4722]  eta: 3:45:36  lr: 0.000010  loss: 8.2268  loss_lm: 8.3050 (8.3654)  time: 2.8857  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  32/4722]  eta: 3:29:50  lr: 0.000010  loss: 8.3375  loss_lm: 8.3375 (8.2823)  time: 2.6846  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  32/4722]  eta: 3:29:52  lr: 0.000010  loss: 8.5433  loss_lm: 8.5433 (8.4835)  time: 2.6849  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  32/4722]  eta: 3:29:52  lr: 0.000010  loss: 7.8623  loss_lm: 8.3813 (8.3267)  time: 2.6849  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  32/4722]  eta: 3:29:51  lr: 0.000010  loss: 8.6880  loss_lm: 8.3369 (8.3348)  time: 2.6847  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  32/4722]  eta: 3:29:51  lr: 0.000010  loss: 8.3230  loss_lm: 8.4421 (8.4472)  time: 2.6847  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  32/4722]  eta: 3:29:52  lr: 0.000010  loss: 8.5409  loss_lm: 8.3376 (8.3905)  time: 2.6850  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  33/4722]  eta: 3:18:14  lr: 0.000010  loss: 8.3511  loss_lm: 8.4218 (8.4352)  time: 2.5367  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  33/4722]  eta: 3:18:14  lr: 0.000010  loss: 8.0819  loss_lm: 8.2053 (8.3032)  time: 2.5367  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  33/4722]  eta: 3:18:15  lr: 0.000010  loss: 8.2312  loss_lm: 8.5386 (8.4520)  time: 2.5369  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  33/4722]  eta: 3:18:15  lr: 0.000010  loss: 7.9848  loss_lm: 8.3195 (8.2839)  time: 2.5369  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  33/4722]  eta: 3:18:15  lr: 0.000010  loss: 8.0232  loss_lm: 8.3050 (8.3446)  time: 2.5369  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  33/4722]  eta: 3:18:14  lr: 0.000010  loss: 8.5891  loss_lm: 8.3375 (8.3207)  time: 2.5366  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  34/4722]  eta: 3:09:05  lr: 0.000010  loss: 8.4509  loss_lm: 8.3813 (8.3025)  time: 2.4201  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  34/4722]  eta: 3:09:04  lr: 0.000010  loss: 8.1016  loss_lm: 8.4218 (8.3981)  time: 2.4200  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  34/4722]  eta: 3:09:04  lr: 0.000010  loss: 8.1819  loss_lm: 8.2053 (8.2897)  time: 2.4199  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  34/4722]  eta: 3:09:04  lr: 0.000010  loss: 8.1588  loss_lm: 8.3375 (8.3027)  time: 2.4198  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  34/4722]  eta: 3:09:05  lr: 0.000010  loss: 8.2576  loss_lm: 8.5386 (8.4304)  time: 2.4201  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  34/4722]  eta: 3:09:05  lr: 0.000010  loss: 8.5328  loss_lm: 8.3376 (8.3655)  time: 2.4202  data: 0.0000  max mem: 18864
2023-11-12 23:49:50,908 [INFO] Saving checkpoint at iters: 35 and epoch: 0
2023-11-12 23:49:50,913 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:49:56,083 [INFO] Saved successfully
2023-11-12 23:49:56,084 [INFO] Averaged stats: lr: 0.0000  loss: 8.3545  loss_lm: 8.3545
Train: data epoch: [0]  [  35/4722]  eta: 3:42:09  lr: 0.000010  loss: 8.2554  loss_lm: 8.3050 (8.3545)  time: 2.8439  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  35/4722]  eta: 3:42:08  lr: 0.000010  loss: 8.3284  loss_lm: 8.3511 (8.3912)  time: 2.8437  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  35/4722]  eta: 3:42:07  lr: 0.000010  loss: 8.2985  loss_lm: 8.2985 (8.3023)  time: 2.8436  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  35/4722]  eta: 3:42:08  lr: 0.000010  loss: 8.4053  loss_lm: 8.2053 (8.3013)  time: 2.8437  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  35/4722]  eta: 3:42:09  lr: 0.000010  loss: 8.3696  loss_lm: 8.3696 (8.4243)  time: 2.8438  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  35/4722]  eta: 3:42:09  lr: 0.000010  loss: 8.3854  loss_lm: 8.3813 (8.3108)  time: 2.8438  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  36/4722]  eta: 3:32:24  lr: 0.000010  loss: 8.6146  loss_lm: 8.3369 (8.3298)  time: 2.7197  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  36/4722]  eta: 3:32:24  lr: 0.000010  loss: 8.1448  loss_lm: 8.3511 (8.3688)  time: 2.7198  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  36/4722]  eta: 3:32:25  lr: 0.000010  loss: 8.4036  loss_lm: 8.3376 (8.3589)  time: 2.7200  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  36/4722]  eta: 3:32:25  lr: 0.000010  loss: 8.4431  loss_lm: 8.3854 (8.3228)  time: 2.7199  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  36/4722]  eta: 3:32:25  lr: 0.000010  loss: 7.9906  loss_lm: 8.3696 (8.3849)  time: 2.7199  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  36/4722]  eta: 3:32:24  lr: 0.000010  loss: 8.0497  loss_lm: 8.2985 (8.2793)  time: 2.7197  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  37/4722]  eta: 3:24:20  lr: 0.000010  loss: 8.2305  loss_lm: 8.3050 (8.3482)  time: 2.6170  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  37/4722]  eta: 3:24:19  lr: 0.000010  loss: 7.8532  loss_lm: 8.3284 (8.3258)  time: 2.6168  data: 0.0000  max mem: 18976Train: data epoch: [0]  [  37/4722]  eta: 3:24:19  lr: 0.000010  loss: 8.3791  loss_lm: 8.3369 (8.3339)  time: 2.6167  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  37/4722]  eta: 3:24:19  lr: 0.000010  loss: 8.2198  loss_lm: 8.3813 (8.3142)  time: 2.6168  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  37/4722]  eta: 3:24:18  lr: 0.000010  loss: 7.8631  loss_lm: 8.2671 (8.2446)  time: 2.6166  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  37/4722]  eta: 3:24:19  lr: 0.000010  loss: 7.9820  loss_lm: 8.2806 (8.3513)  time: 2.6169  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  38/4722]  eta: 3:17:27  lr: 0.000010  loss: 8.5517  loss_lm: 8.3511 (8.3432)  time: 2.5294  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  38/4722]  eta: 3:17:27  lr: 0.000010  loss: 8.3938  loss_lm: 8.3791 (8.3385)  time: 2.5294  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  38/4722]  eta: 3:17:28  lr: 0.000010  loss: 8.0786  loss_lm: 8.3050 (8.3275)  time: 2.5296  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  38/4722]  eta: 3:17:28  lr: 0.000010  loss: 8.1307  loss_lm: 8.3813 (8.3001)  time: 2.5295  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  38/4722]  eta: 3:17:27  lr: 0.000010  loss: 8.1809  loss_lm: 8.2671 (8.2397)  time: 2.5293  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  38/4722]  eta: 3:17:28  lr: 0.000010  loss: 8.1267  loss_lm: 8.2806 (8.3340)  time: 2.5295  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  39/4722]  eta: 3:11:42  lr: 0.000010  loss: 7.9843  loss_lm: 8.3284 (8.3175)  time: 2.4562  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  39/4722]  eta: 3:11:42  lr: 0.000010  loss: 8.1374  loss_lm: 8.3369 (8.3241)  time: 2.4561  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  39/4722]  eta: 3:11:43  lr: 0.000010  loss: 8.0722  loss_lm: 8.2835 (8.3093)  time: 2.4564  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  39/4722]  eta: 3:11:41  lr: 0.000010  loss: 8.1223  loss_lm: 8.1809 (8.2313)  time: 2.4561  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  39/4722]  eta: 3:11:42  lr: 0.000010  loss: 8.0379  loss_lm: 8.2576 (8.3129)  time: 2.4563  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  39/4722]  eta: 3:11:42  lr: 0.000010  loss: 7.9816  loss_lm: 8.3195 (8.2774)  time: 2.4562  data: 0.0000  max mem: 19014

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-12 23:50:03,514 [INFO] Saving checkpoint at iters: 40 and epoch: 0
2023-11-12 23:50:03,519 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:50:08,937 [INFO] Saved successfully
2023-11-12 23:50:08,938 [INFO] Averaged stats: lr: 0.0000  loss: 8.2865  loss_lm: 8.2865
Train: data epoch: [0]  [  40/4722]  eta: 3:34:48  lr: 0.000010  loss: 7.9673  loss_lm: 8.2835 (8.2865)  time: 2.7527  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  40/4722]  eta: 3:34:47  lr: 0.000010  loss: 8.0559  loss_lm: 8.3284 (8.3001)  time: 2.7525  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  40/4722]  eta: 3:34:47  lr: 0.000010  loss: 7.8201  loss_lm: 8.3369 (8.2905)  time: 2.7525  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  40/4722]  eta: 3:34:46  lr: 0.000010  loss: 8.1205  loss_lm: 8.1809 (8.2239)  time: 2.7524  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  40/4722]  eta: 3:34:47  lr: 0.000010  loss: 8.2638  loss_lm: 8.2638 (8.3096)  time: 2.7526  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  40/4722]  eta: 3:34:47  lr: 0.000010  loss: 8.0615  loss_lm: 8.3195 (8.2630)  time: 2.7526  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  41/4722]  eta: 3:28:32  lr: 0.000010  loss: 7.9852  loss_lm: 8.1588 (8.2090)  time: 2.6731  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  41/4722]  eta: 3:28:33  lr: 0.000010  loss: 8.1615  loss_lm: 8.3178 (8.2566)  time: 2.6733  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [  41/4722]  eta: 3:28:33  lr: 0.000010  loss: 8.2155  loss_lm: 8.3230 (8.2948)  time: 2.6732  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  41/4722]  eta: 3:28:34  lr: 0.000010  loss: 8.2035  loss_lm: 8.2554 (8.2813)  time: 2.6734  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  41/4722]  eta: 3:28:33  lr: 0.000010  loss: 7.9142  loss_lm: 8.2576 (8.2849)  time: 2.6733  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  41/4722]  eta: 3:28:33  lr: 0.000010  loss: 8.2229  loss_lm: 8.2229 (8.2863)  time: 2.6732  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  42/4722]  eta: 3:23:02  lr: 0.000010  loss: 8.0279  loss_lm: 8.3230 (8.2791)  time: 2.6031  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  42/4722]  eta: 3:23:03  lr: 0.000010  loss: 8.3771  loss_lm: 8.2835 (8.2869)  time: 2.6033  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  42/4722]  eta: 3:23:02  lr: 0.000010  loss: 8.3694  loss_lm: 8.1809 (8.2185)  time: 2.6030  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  42/4722]  eta: 3:23:02  lr: 0.000010  loss: 8.1998  loss_lm: 8.2229 (8.2812)  time: 2.6031  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  42/4722]  eta: 3:23:02  lr: 0.000010  loss: 7.8822  loss_lm: 8.3178 (8.2346)  time: 2.6032  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  42/4722]  eta: 3:23:02  lr: 0.000010  loss: 8.3794  loss_lm: 8.2638 (8.2904)  time: 2.6032  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  43/4722]  eta: 3:18:07  lr: 0.000010  loss: 8.0165  loss_lm: 8.2198 (8.2225)  time: 2.5406  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  43/4722]  eta: 3:18:08  lr: 0.000010  loss: 8.2836  loss_lm: 8.2835 (8.2867)  time: 2.5407  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  43/4722]  eta: 3:18:06  lr: 0.000010  loss: 7.7544  loss_lm: 8.1588 (8.1927)  time: 2.5405  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  43/4722]  eta: 3:18:07  lr: 0.000010  loss: 7.9080  loss_lm: 8.2053 (8.2605)  time: 2.5405  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  43/4722]  eta: 3:18:07  lr: 0.000010  loss: 8.3519  loss_lm: 8.3230 (8.2832)  time: 2.5406  data: 0.0000  max mem: 18976

Train: data epoch: [0]  [  43/4722]  eta: 3:18:07  lr: 0.000010  loss: 7.9220  loss_lm: 8.2576 (8.2700)  time: 2.5406  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  44/4722]  eta: 3:13:42  lr: 0.000010  loss: 7.7413  loss_lm: 8.1588 (8.1689)  time: 2.4845  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  44/4722]  eta: 3:13:43  lr: 0.000010  loss: 7.6655  loss_lm: 8.3230 (8.2506)  time: 2.4846  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  44/4722]  eta: 3:13:43  lr: 0.000010  loss: 8.4054  loss_lm: 8.3178 (8.2321)  time: 2.4847  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  44/4722]  eta: 3:13:42  lr: 0.000010  loss: 7.8893  loss_lm: 8.2053 (8.2410)  time: 2.4846  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  44/4722]  eta: 3:13:43  lr: 0.000010  loss: 8.0436  loss_lm: 8.2835 (8.2739)  time: 2.4848  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  44/4722]  eta: 3:13:43  lr: 0.000010  loss: 8.1897  loss_lm: 8.2576 (8.2658)  time: 2.4847  data: 0.0000  max mem: 18864
2023-11-12 23:50:16,342 [INFO] Saving checkpoint at iters: 45 and epoch: 0
2023-11-12 23:50:16,347 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:50:21,669 [INFO] Saved successfully
2023-11-12 23:50:21,670 [INFO] Averaged stats: lr: 0.0000  loss: 8.2638  loss_lm: 8.2638
Train: data epoch: [0]  [  45/4722]  eta: 3:30:32  lr: 0.000010  loss: 8.0706  loss_lm: 8.2554 (8.2638)  time: 2.7010  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  45/4722]  eta: 3:30:31  lr: 0.000010  loss: 7.9925  loss_lm: 8.3075 (8.2377)  time: 2.7008  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  45/4722]  eta: 3:30:31  lr: 0.000010  loss: 7.9259  loss_lm: 8.2312 (8.2488)  time: 2.7008  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  45/4722]  eta: 3:30:31  lr: 0.000010  loss: 7.8892  loss_lm: 8.1998 (8.2234)  time: 2.7008  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  45/4722]  eta: 3:30:31  lr: 0.000010  loss: 7.9278  loss_lm: 8.1223 (8.1569)  time: 2.7007  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  45/4722]  eta: 3:30:31  lr: 0.000010  loss: 7.5890  loss_lm: 8.2198 (8.2000)  time: 2.7008  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  46/4722]  eta: 3:25:57  lr: 0.000010  loss: 8.2247  loss_lm: 8.2247 (8.2371)  time: 2.5591  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  46/4722]  eta: 3:25:58  lr: 0.000010  loss: 8.0459  loss_lm: 8.2305 (8.2534)  time: 2.5592  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  46/4722]  eta: 3:25:57  lr: 0.000010  loss: 8.2200  loss_lm: 8.2198 (8.2009)  time: 2.5591  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  46/4722]  eta: 3:25:57  lr: 0.000010  loss: 8.0866  loss_lm: 8.1819 (8.2169)  time: 2.5591  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  46/4722]  eta: 3:25:56  lr: 0.000010  loss: 7.7707  loss_lm: 8.1205 (8.1385)  time: 2.5591  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  46/4722]  eta: 3:25:57  lr: 0.000010  loss: 8.1205  loss_lm: 8.1897 (8.2426)  time: 2.5590  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  47/4722]  eta: 3:21:47  lr: 0.000010  loss: 8.1369  loss_lm: 8.2155 (8.2326)  time: 2.5584  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  47/4722]  eta: 3:21:47  lr: 0.000010  loss: 7.9055  loss_lm: 8.1615 (8.1875)  time: 2.5584  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  47/4722]  eta: 3:21:46  lr: 0.000010  loss: 7.8689  loss_lm: 8.1374 (8.2010)  time: 2.5584  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  47/4722]  eta: 3:21:46  lr: 0.000010  loss: 8.1384  loss_lm: 8.1223 (8.1385)  time: 2.5584  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  47/4722]  eta: 3:21:47  lr: 0.000010  loss: 7.8136  loss_lm: 8.1897 (8.2231)  time: 2.5584  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  47/4722]  eta: 3:21:47  lr: 0.000010  loss: 7.5450  loss_lm: 8.2268 (8.2212)  time: 2.5586  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  48/4722]  eta: 3:18:00  lr: 0.000010  loss: 8.1427  loss_lm: 8.1427 (8.1855)  time: 2.5589  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  48/4722]  eta: 3:18:00  lr: 0.000010  loss: 7.9594  loss_lm: 8.1448 (8.2207)  time: 2.5589  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  48/4722]  eta: 3:17:59  lr: 0.000010  loss: 8.2035  loss_lm: 8.1223 (8.1413)  time: 2.5589  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  48/4722]  eta: 3:18:00  lr: 0.000010  loss: 8.3737  loss_lm: 8.1374 (8.2085)  time: 2.5589  data: 0.0000  max mem: 18888

Train: data epoch: [0]  [  48/4722]  eta: 3:18:00  lr: 0.000010  loss: 8.1936  loss_lm: 8.1897 (8.2219)  time: 2.5588  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  48/4722]  eta: 3:18:01  lr: 0.000010  loss: 7.7892  loss_lm: 8.2035 (8.2024)  time: 2.5590  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  49/4722]  eta: 3:14:31  lr: 0.000010  loss: 7.4756  loss_lm: 8.1307 (8.1559)  time: 2.5592  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  49/4722]  eta: 3:14:31  lr: 0.000010  loss: 8.2220  loss_lm: 8.1448 (8.2207)  time: 2.5592  data: 0.0000  max mem: 18976
Train: data epoch: [0]  [  49/4722]  eta: 3:14:31  lr: 0.000010  loss: 7.8769  loss_lm: 8.1374 (8.1947)  time: 2.5592  data: 0.0000  max mem: 18888Train: data epoch: [0]  [  49/4722]  eta: 3:14:31  lr: 0.000010  loss: 8.0776  loss_lm: 8.1205 (8.1386)  time: 2.5592  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  49/4722]  eta: 3:14:31  lr: 0.000010  loss: 8.4260  loss_lm: 8.1897 (8.2304)  time: 2.5592  data: 0.0000  max mem: 19014Train: data epoch: [0]  [  49/4722]  eta: 3:14:32  lr: 0.000010  loss: 8.0855  loss_lm: 8.0855 (8.1975)  time: 2.5593  data: 0.0000  max mem: 18866

2023-11-12 23:50:29,081 [INFO] Saving checkpoint at iters: 50 and epoch: 0
2023-11-12 23:50:29,086 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-12 23:53:55.467383] Starting job[2023-11-12 23:53:55.467387] Starting job
[2023-11-12 23:53:55.467387] Starting job[2023-11-12 23:53:55.467390] Starting job
[2023-11-12 23:53:55.467389] Starting job
[2023-11-12 23:53:55.467391] Starting job


| distributed init (rank 0, world 12): env://
| distributed init (rank 1, world 12): env://
| distributed init (rank 2, world 12): env://
| distributed init (rank 4, world 12): env://
| distributed init (rank 5, world 12): env://
| distributed init (rank 3, world 12): env://
2023-11-12 23:53:58,283 [INFO] 
=====  Running Parameters    =====
2023-11-12 23:53:58,284 [INFO] {
    "amp": true,
    "batch_size_eval": 10,
    "batch_size_train": 10,
    "checkpoint_freq": 5,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "eval_freq": 2,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 1e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 20,
    "min_len": 5,
    "min_lr": 0,
    "num_beams": 3,
    "num_workers": 3,
    "output_dir": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint",
    "rank": 0,
    "resume_ckpt_path": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth",
    "runner": "runner_base",
    "seed": 42,
    "task": "captioning",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "weight_decay": 0.05,
    "world_size": 12
}
2023-11-12 23:53:58,284 [INFO] 
======  Dataset Attributes  ======
2023-11-12 23:53:58,284 [INFO] 
======== coco_caption =======
2023-11-12 23:53:58,284 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/mnt/.node1/Open-Datasets/coco"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        },
        "train": {
            "name": "blip_caption",
            "prompt": "a picture of "
        }
    },
    "vis_processor": {
        "eval": {
            "name": "blip_image_eval"
        },
        "train": {
            "name": "blip_image_train"
        }
    }
}
2023-11-12 23:53:58,284 [INFO] 
======  Model Attributes  ======
2023-11-12 23:53:58,284 [INFO] {
    "arch": "blip_caption",
    "finetuned": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth",
    "image_size": 384,
    "load_finetuned": false,
    "load_pretrained": false,
    "med_config_path": "configs/models/med_large_config.json",
    "model_type": "large_coco",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth",
    "prompt": "a picture of ",
    "vit_ckpt_layer": 5,
    "vit_grad_ckpt": true,
    "vit_type": "large"
}
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-11-12 23:53:58,287 [INFO] Building datasets...
2023-11-12 23:54:13,574 [INFO] number of trainable parameters: 446290492
2023-11-12 23:54:13,874 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-11-12 23:54:13,874 [INFO] Loaded 566747 records for train split from the dataset.
2023-11-12 23:54:13,874 [INFO] Loaded 5000 records for val split from the dataset.
2023-11-12 23:54:13,874 [INFO] Loaded 5000 records for test split from the dataset.
2023-11-12 23:54:13,875 [INFO] Resume checkpoint from /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth
2023-11-12 23:54:13,878 [INFO] Start training
2023-11-12 23:54:13,888 [INFO] Start training epoch 0, 4722 iters per inner epoch.
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  46/4722]  eta: 5:36:13  lr: 0.000010  loss: 8.1961  loss_lm: 8.1961 (8.1961)  time: 4.3143  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  46/4722]  eta: 5:36:09  lr: 0.000010  loss: 7.7844  loss_lm: 7.7844 (7.7844)  time: 4.3133  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  46/4722]  eta: 5:36:08  lr: 0.000010  loss: 8.1145  loss_lm: 8.1145 (8.1145)  time: 4.3132  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  46/4722]  eta: 5:36:37  lr: 0.000010  loss: 8.0053  loss_lm: 8.0053 (8.0053)  time: 4.3193  data: 0.0000  max mem: 18863


Train: data epoch: [0]  [  46/4722]  eta: 5:36:05  lr: 0.000010  loss: 8.2098  loss_lm: 8.2098 (8.2098)  time: 4.3126  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  46/4722]  eta: 5:36:06  lr: 0.000010  loss: 8.0958  loss_lm: 8.0958 (8.0958)  time: 4.3127  data: 0.0000  max mem: 18865
2023-11-12 23:54:18,220 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  47/4722]  eta: 3:46:12  lr: 0.000010  loss: 7.7921  loss_lm: 7.7921 (7.9533)  time: 2.9033  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  47/4722]  eta: 3:46:27  lr: 0.000010  loss: 7.5857  loss_lm: 7.5857 (7.7955)  time: 2.9064  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  47/4722]  eta: 3:46:15  lr: 0.000010  loss: 8.1457  loss_lm: 8.1457 (8.1709)  time: 2.9039  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  47/4722]  eta: 3:46:13  lr: 0.000010  loss: 8.1302  loss_lm: 7.7844 (7.9573)  time: 2.9034  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  47/4722]  eta: 3:46:11  lr: 0.000010  loss: 7.8949  loss_lm: 7.8949 (8.0523)  time: 2.9030  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  47/4722]  eta: 3:46:11  lr: 0.000010  loss: 7.8376  loss_lm: 7.8376 (7.9667)  time: 2.9030  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  48/4722]  eta: 3:10:09  lr: 0.000010  loss: 8.1759  loss_lm: 8.1145 (8.0275)  time: 2.4411  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  48/4722]  eta: 3:10:09  lr: 0.000010  loss: 8.2064  loss_lm: 8.1302 (8.0403)  time: 2.4411  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  48/4722]  eta: 3:10:08  lr: 0.000010  loss: 8.3687  loss_lm: 8.0958 (8.1007)  time: 2.4409  data: 0.0000  max mem: 18888

Train: data epoch: [0]  [  48/4722]  eta: 3:10:11  lr: 0.000010  loss: 7.9554  loss_lm: 8.1457 (8.0991)  time: 2.4415  data: 0.0000  max mem: 18867Train: data epoch: [0]  [  48/4722]  eta: 3:10:19  lr: 0.000010  loss: 7.7716  loss_lm: 7.7716 (7.7875)  time: 2.4432  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  48/4722]  eta: 3:10:08  lr: 0.000010  loss: 8.1591  loss_lm: 8.1591 (8.0879)  time: 2.4409  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [  49/4722]  eta: 2:52:01  lr: 0.000010  loss: 8.4258  loss_lm: 8.1145 (8.1271)  time: 2.2087  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  49/4722]  eta: 2:52:08  lr: 0.000010  loss: 8.0981  loss_lm: 7.7716 (7.8652)  time: 2.2102  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  49/4722]  eta: 2:52:01  lr: 0.000010  loss: 8.0443  loss_lm: 8.0443 (8.0413)  time: 2.2087  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  49/4722]  eta: 2:52:02  lr: 0.000010  loss: 8.2134  loss_lm: 8.1457 (8.1277)  time: 2.2090  data: 0.0000  max mem: 18867Train: data epoch: [0]  [  49/4722]  eta: 2:52:00  lr: 0.000010  loss: 7.4775  loss_lm: 7.8949 (7.9353)  time: 2.2085  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [  49/4722]  eta: 2:52:00  lr: 0.000010  loss: 7.9036  loss_lm: 7.9036 (8.0514)  time: 2.2086  data: 0.0000  max mem: 18888
2023-11-12 23:54:24,248 [INFO] Saving checkpoint at iters: 50 and epoch: 0
2023-11-12 23:54:24,254 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:54:29,574 [INFO] Saved successfully
2023-11-12 23:54:29,575 [INFO] Averaged stats: lr: 0.0000  loss: 7.8563  loss_lm: 7.8563
Train: data epoch: [0]  [  50/4722]  eta: 4:04:13  lr: 0.000010  loss: 7.8208  loss_lm: 7.8208 (7.8563)  time: 3.1365  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  50/4722]  eta: 4:04:07  lr: 0.000010  loss: 8.2430  loss_lm: 8.1591 (7.9968)  time: 3.1352  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  50/4722]  eta: 4:04:09  lr: 0.000010  loss: 8.2198  loss_lm: 8.1961 (8.1461)  time: 3.1356  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  50/4722]  eta: 4:04:08  lr: 0.000010  loss: 8.3103  loss_lm: 8.1759 (8.1637)  time: 3.1354  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  50/4722]  eta: 4:04:08  lr: 0.000010  loss: 8.1661  loss_lm: 8.1302 (8.0663)  time: 3.1354  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  50/4722]  eta: 4:04:07  lr: 0.000010  loss: 7.8161  loss_lm: 7.9036 (8.0043)  time: 3.1352  data: 0.0000  max mem: 18888

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  51/4722]  eta: 3:43:44  lr: 0.000010  loss: 7.8666  loss_lm: 8.1145 (8.1142)  time: 2.8740  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  51/4722]  eta: 3:43:43  lr: 0.000010  loss: 7.9502  loss_lm: 7.9036 (7.9953)  time: 2.8739  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  51/4722]  eta: 3:43:44  lr: 0.000010  loss: 7.5941  loss_lm: 8.0443 (7.9876)  time: 2.8740  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  51/4722]  eta: 3:43:45  lr: 0.000010  loss: 7.7716  loss_lm: 8.1457 (8.0837)  time: 2.8742  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  51/4722]  eta: 3:43:43  lr: 0.000010  loss: 8.2724  loss_lm: 8.1591 (8.0428)  time: 2.8739  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  51/4722]  eta: 3:43:49  lr: 0.000010  loss: 7.6094  loss_lm: 7.7716 (7.8152)  time: 2.8751  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  52/4722]  eta: 3:29:05  lr: 0.000010  loss: 7.6919  loss_lm: 8.1145 (8.0539)  time: 2.6864  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  52/4722]  eta: 3:29:05  lr: 0.000010  loss: 8.0367  loss_lm: 8.0443 (7.9946)  time: 2.6864  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  52/4722]  eta: 3:29:04  lr: 0.000010  loss: 7.6866  loss_lm: 7.9036 (7.9512)  time: 2.6863  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  52/4722]  eta: 3:29:06  lr: 0.000010  loss: 8.0588  loss_lm: 8.1457 (8.0801)  time: 2.6866  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  52/4722]  eta: 3:29:09  lr: 0.000010  loss: 7.8312  loss_lm: 7.8208 (7.8174)  time: 2.6873  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  52/4722]  eta: 3:29:04  lr: 0.000010  loss: 7.8727  loss_lm: 8.1591 (8.0185)  time: 2.6863  data: 0.0000  max mem: 18868

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  53/4722]  eta: 3:17:31  lr: 0.000010  loss: 7.6052  loss_lm: 7.8666 (7.9978)  time: 2.5384  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  53/4722]  eta: 3:17:31  lr: 0.000010  loss: 7.9214  loss_lm: 8.0367 (7.9855)  time: 2.5384  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  53/4722]  eta: 3:17:35  lr: 0.000010  loss: 7.8500  loss_lm: 7.8208 (7.8215)  time: 2.5392  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  53/4722]  eta: 3:17:31  lr: 0.000010  loss: 7.8991  loss_lm: 7.8991 (7.9447)  time: 2.5383  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  53/4722]  eta: 3:17:32  lr: 0.000010  loss: 7.6613  loss_lm: 8.0588 (8.0278)  time: 2.5385  data: 0.0000  max mem: 18867Train: data epoch: [0]  [  53/4722]  eta: 3:17:31  lr: 0.000010  loss: 7.9043  loss_lm: 7.9043 (8.0042)  time: 2.5383  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [  54/4722]  eta: 3:08:24  lr: 0.000010  loss: 7.7015  loss_lm: 7.8666 (7.9649)  time: 2.4217  data: 0.0000  max mem: 19014Train: data epoch: [0]  [  54/4722]  eta: 3:08:27  lr: 0.000010  loss: 8.0832  loss_lm: 7.8312 (7.8506)  time: 2.4224  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  54/4722]  eta: 3:08:24  lr: 0.000010  loss: 7.7068  loss_lm: 8.0588 (7.9921)  time: 2.4218  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  54/4722]  eta: 3:08:24  lr: 0.000010  loss: 7.8889  loss_lm: 8.0367 (7.9747)  time: 2.4217  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  54/4722]  eta: 3:08:23  lr: 0.000010  loss: 7.6544  loss_lm: 7.8991 (7.9124)  time: 2.4216  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  54/4722]  eta: 3:08:23  lr: 0.000010  loss: 7.4123  loss_lm: 7.9043 (7.9384)  time: 2.4216  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-12 23:54:37,183 [INFO] Saving checkpoint at iters: 55 and epoch: 0
2023-11-12 23:54:37,187 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:54:42,620 [INFO] Saved successfully
2023-11-12 23:54:42,621 [INFO] Averaged stats: lr: 0.0000  loss: 7.8616  loss_lm: 7.8616
Train: data epoch: [0]  [  55/4722]  eta: 3:43:24  lr: 0.000010  loss: 7.9604  loss_lm: 7.8312 (7.8616)  time: 2.8722  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  55/4722]  eta: 3:43:21  lr: 0.000010  loss: 8.3540  loss_lm: 7.8991 (7.9566)  time: 2.8715  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  55/4722]  eta: 3:43:21  lr: 0.000010  loss: 8.4854  loss_lm: 8.0588 (8.0414)  time: 2.8716  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  55/4722]  eta: 3:43:21  lr: 0.000010  loss: 7.7564  loss_lm: 7.8949 (7.9202)  time: 2.8714  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  55/4722]  eta: 3:43:21  lr: 0.000010  loss: 7.8856  loss_lm: 7.8666 (7.9570)  time: 2.8716  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  55/4722]  eta: 3:43:21  lr: 0.000010  loss: 8.0865  loss_lm: 8.0367 (7.9859)  time: 2.8715  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  56/4722]  eta: 3:33:31  lr: 0.000010  loss: 8.0731  loss_lm: 8.0443 (7.9938)  time: 2.7456  data: 0.0000  max mem: 19004Train: data epoch: [0]  [  56/4722]  eta: 3:33:31  lr: 0.000010  loss: 7.3234  loss_lm: 7.8666 (7.8994)  time: 2.7456  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [  56/4722]  eta: 3:33:31  lr: 0.000010  loss: 7.8200  loss_lm: 8.0588 (8.0213)  time: 2.7457  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  56/4722]  eta: 3:33:30  lr: 0.000010  loss: 7.9762  loss_lm: 7.9036 (7.9584)  time: 2.7456  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  56/4722]  eta: 3:33:30  lr: 0.000010  loss: 7.9143  loss_lm: 7.9043 (7.9197)  time: 2.7455  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  56/4722]  eta: 3:33:34  lr: 0.000010  loss: 8.3374  loss_lm: 7.8500 (7.9048)  time: 2.7463  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  57/4722]  eta: 3:25:17  lr: 0.000010  loss: 7.9719  loss_lm: 8.0367 (7.9920)  time: 2.6404  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  57/4722]  eta: 3:25:17  lr: 0.000010  loss: 8.3384  loss_lm: 7.8666 (7.9359)  time: 2.6404  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  57/4722]  eta: 3:25:17  lr: 0.000010  loss: 7.8464  loss_lm: 7.9554 (8.0067)  time: 2.6405  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  57/4722]  eta: 3:25:17  lr: 0.000010  loss: 8.0294  loss_lm: 7.9043 (7.9288)  time: 2.6403  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  57/4722]  eta: 3:25:20  lr: 0.000010  loss: 7.7919  loss_lm: 7.8312 (7.8954)  time: 2.6410  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  57/4722]  eta: 3:25:17  lr: 0.000010  loss: 7.9259  loss_lm: 7.9036 (7.9557)  time: 2.6404  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  58/4722]  eta: 3:18:24  lr: 0.000010  loss: 7.9995  loss_lm: 7.8856 (7.9408)  time: 2.5523  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  58/4722]  eta: 3:18:23  lr: 0.000010  loss: 8.0725  loss_lm: 7.9143 (7.9399)  time: 2.5522  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  58/4722]  eta: 3:18:26  lr: 0.000010  loss: 7.7873  loss_lm: 7.8312 (7.8871)  time: 2.5529  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  58/4722]  eta: 3:18:23  lr: 0.000010  loss: 7.5318  loss_lm: 8.0367 (7.9566)  time: 2.5523  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  58/4722]  eta: 3:18:24  lr: 0.000010  loss: 7.7514  loss_lm: 7.9554 (7.9871)  time: 2.5524  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  58/4722]  eta: 3:18:23  lr: 0.000010  loss: 7.6313  loss_lm: 7.9036 (7.9307)  time: 2.5522  data: 0.0000  max mem: 18888
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  59/4722]  eta: 3:12:39  lr: 0.000010  loss: 8.0892  loss_lm: 7.8312 (7.9015)  time: 2.4790  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  59/4722]  eta: 3:12:37  lr: 0.000010  loss: 7.6591  loss_lm: 7.8464 (7.9637)  time: 2.4786  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  59/4722]  eta: 3:12:37  lr: 0.000010  loss: 7.9670  loss_lm: 7.8856 (7.9427)  time: 2.4785  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  59/4722]  eta: 3:12:37  lr: 0.000010  loss: 7.9150  loss_lm: 7.9719 (7.9536)  time: 2.4785  data: 0.0000  max mem: 19004Train: data epoch: [0]  [  59/4722]  eta: 3:12:36  lr: 0.000010  loss: 7.8896  loss_lm: 7.9043 (7.9363)  time: 2.4784  data: 0.0000  max mem: 18868

Train: data epoch: [0]  [  59/4722]  eta: 3:12:36  lr: 0.000010  loss: 7.8367  loss_lm: 7.8991 (7.9240)  time: 2.4784  data: 0.0000  max mem: 18888
2023-11-12 23:54:50,097 [INFO] Saving checkpoint at iters: 60 and epoch: 0
2023-11-12 23:54:50,102 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:54:55,493 [INFO] Saved successfully
2023-11-12 23:54:55,494 [INFO] Averaged stats: lr: 0.0000  loss: 7.8953  loss_lm: 7.8953
Train: data epoch: [0]  [  60/4722]  eta: 3:35:26  lr: 0.000010  loss: 7.8076  loss_lm: 7.8312 (7.8953)  time: 2.7728  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  60/4722]  eta: 3:35:24  lr: 0.000010  loss: 8.1098  loss_lm: 7.9143 (7.9479)  time: 2.7723  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  60/4722]  eta: 3:35:24  lr: 0.000010  loss: 7.5595  loss_lm: 7.9719 (7.9274)  time: 2.7723  data: 0.0000  max mem: 19004Train: data epoch: [0]  [  60/4722]  eta: 3:35:24  lr: 0.000010  loss: 8.0964  loss_lm: 7.9554 (7.9725)  time: 2.7724  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [  60/4722]  eta: 3:35:24  lr: 0.000010  loss: 7.4472  loss_lm: 7.8856 (7.9097)  time: 2.7724  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  60/4722]  eta: 3:35:24  lr: 0.000010  loss: 7.9106  loss_lm: 7.9036 (7.9231)  time: 2.7723  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  61/4722]  eta: 3:29:05  lr: 0.000010  loss: 7.7412  loss_lm: 7.8666 (7.8991)  time: 2.6916  data: 0.0000  max mem: 19014Train: data epoch: [0]  [  61/4722]  eta: 3:29:05  lr: 0.000010  loss: 7.8531  loss_lm: 7.8991 (7.9187)  time: 2.6916  data: 0.0000  max mem: 18888Train: data epoch: [0]  [  61/4722]  eta: 3:29:05  lr: 0.000010  loss: 8.2055  loss_lm: 7.9143 (7.9640)  time: 2.6916  data: 0.0000  max mem: 18868


Train: data epoch: [0]  [  61/4722]  eta: 3:29:05  lr: 0.000010  loss: 7.6476  loss_lm: 7.9214 (7.9099)  time: 2.6916  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  61/4722]  eta: 3:29:05  lr: 0.000010  loss: 7.5985  loss_lm: 7.8464 (7.9491)  time: 2.6917  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  61/4722]  eta: 3:29:07  lr: 0.000010  loss: 7.9622  loss_lm: 7.8312 (7.8995)  time: 2.6921  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  62/4722]  eta: 3:23:31  lr: 0.000010  loss: 8.1903  loss_lm: 7.9036 (7.9347)  time: 2.6205  data: 0.0000  max mem: 18888Train: data epoch: [0]  [  62/4722]  eta: 3:23:31  lr: 0.000010  loss: 7.8628  loss_lm: 7.8666 (7.8970)  time: 2.6205  data: 0.0000  max mem: 19014Train: data epoch: [0]  [  62/4722]  eta: 3:23:31  lr: 0.000010  loss: 7.8886  loss_lm: 7.8886 (7.9456)  time: 2.6206  data: 0.0000  max mem: 18867


Train: data epoch: [0]  [  62/4722]  eta: 3:23:31  lr: 0.000010  loss: 7.5916  loss_lm: 7.9143 (7.9421)  time: 2.6205  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  62/4722]  eta: 3:23:31  lr: 0.000010  loss: 7.5114  loss_lm: 7.9214 (7.8864)  time: 2.6205  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  62/4722]  eta: 3:23:33  lr: 0.000010  loss: 7.7004  loss_lm: 7.8312 (7.8877)  time: 2.6210  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  63/4722]  eta: 3:18:33  lr: 0.000010  loss: 7.7640  loss_lm: 7.8628 (7.8896)  time: 2.5570  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  63/4722]  eta: 3:18:32  lr: 0.000010  loss: 8.0170  loss_lm: 7.9036 (7.9393)  time: 2.5569  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  63/4722]  eta: 3:18:35  lr: 0.000010  loss: 7.7915  loss_lm: 7.8208 (7.8824)  time: 2.5574  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  63/4722]  eta: 3:18:33  lr: 0.000010  loss: 7.7419  loss_lm: 7.8464 (7.9343)  time: 2.5571  data: 0.0000  max mem: 18867Train: data epoch: [0]  [  63/4722]  eta: 3:18:33  lr: 0.000010  loss: 7.6468  loss_lm: 7.9150 (7.8731)  time: 2.5570  data: 0.0000  max mem: 19004

Train: data epoch: [0]  [  63/4722]  eta: 3:18:32  lr: 0.000010  loss: 7.7120  loss_lm: 7.9043 (7.9293)  time: 2.5570  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  64/4722]  eta: 3:14:06  lr: 0.000010  loss: 7.6565  loss_lm: 7.8628 (7.8773)  time: 2.5004  data: 0.0000  max mem: 19014Train: data epoch: [0]  [  64/4722]  eta: 3:14:06  lr: 0.000010  loss: 8.0349  loss_lm: 7.9106 (7.9443)  time: 2.5004  data: 0.0000  max mem: 18888

Train: data epoch: [0]  [  64/4722]  eta: 3:14:06  lr: 0.000010  loss: 7.6004  loss_lm: 7.9043 (7.9120)  time: 2.5003  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  64/4722]  eta: 3:14:08  lr: 0.000010  loss: 7.7354  loss_lm: 7.8208 (7.8747)  time: 2.5008  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  64/4722]  eta: 3:14:07  lr: 0.000010  loss: 7.7662  loss_lm: 7.8464 (7.9254)  time: 2.5004  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  64/4722]  eta: 3:14:06  lr: 0.000010  loss: 8.1355  loss_lm: 7.9214 (7.8869)  time: 2.5004  data: 0.0000  max mem: 19004
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-12 23:55:02,909 [INFO] Saving checkpoint at iters: 65 and epoch: 0
2023-11-12 23:55:02,914 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-12 23:55:08,149 [INFO] Saved successfully
2023-11-12 23:55:08,150 [INFO] Averaged stats: lr: 0.0000  loss: 7.8598  loss_lm: 7.8598
Train: data epoch: [0]  [  65/4722]  eta: 3:30:30  lr: 0.000010  loss: 7.5776  loss_lm: 7.8076 (7.8598)  time: 2.7123  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  65/4722]  eta: 3:30:28  lr: 0.000010  loss: 7.7862  loss_lm: 7.8949 (7.9057)  time: 2.7118  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  65/4722]  eta: 3:30:29  lr: 0.000010  loss: 7.5578  loss_lm: 7.9150 (7.8705)  time: 2.7119  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  65/4722]  eta: 3:30:29  lr: 0.000010  loss: 7.8279  loss_lm: 7.8279 (7.8749)  time: 2.7119  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  65/4722]  eta: 3:30:28  lr: 0.000010  loss: 7.7390  loss_lm: 7.9036 (7.9340)  time: 2.7118  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  65/4722]  eta: 3:30:29  lr: 0.000010  loss: 7.6014  loss_lm: 7.8200 (7.9092)  time: 2.7119  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  66/4722]  eta: 3:26:14  lr: 0.000010  loss: 7.6403  loss_lm: 7.8896 (7.8930)  time: 2.5751  data: 0.0000  max mem: 18868Train: data epoch: [0]  [  66/4722]  eta: 3:26:14  lr: 0.000010  loss: 7.5155  loss_lm: 7.8991 (7.9141)  time: 2.5751  data: 0.0000  max mem: 18888

Train: data epoch: [0]  [  66/4722]  eta: 3:26:15  lr: 0.000010  loss: 7.9177  loss_lm: 7.8279 (7.8769)  time: 2.5751  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  66/4722]  eta: 3:26:15  lr: 0.000010  loss: 7.5494  loss_lm: 7.7716 (7.8921)  time: 2.5751  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  66/4722]  eta: 3:26:16  lr: 0.000010  loss: 8.0516  loss_lm: 7.8076 (7.8689)  time: 2.5752  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  66/4722]  eta: 3:26:14  lr: 0.000010  loss: 7.3847  loss_lm: 7.9150 (7.8473)  time: 2.5751  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  67/4722]  eta: 3:22:03  lr: 0.000010  loss: 7.6110  loss_lm: 7.8279 (7.8648)  time: 2.5746  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  67/4722]  eta: 3:22:03  lr: 0.000010  loss: 7.7663  loss_lm: 7.8991 (7.9074)  time: 2.5746  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  67/4722]  eta: 3:22:03  lr: 0.000010  loss: 7.9268  loss_lm: 7.9150 (7.8510)  time: 2.5746  data: 0.0000  max mem: 19004
Train: data epoch: [0]  [  67/4722]  eta: 3:22:05  lr: 0.000010  loss: 7.6846  loss_lm: 7.8076 (7.8606)  time: 2.5747  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  67/4722]  eta: 3:22:03  lr: 0.000010  loss: 7.8959  loss_lm: 7.7716 (7.8922)  time: 2.5746  data: 0.0000  max mem: 18867
Train: data epoch: [0]  [  67/4722]  eta: 3:22:03  lr: 0.000010  loss: 7.8721  loss_lm: 7.8727 (7.8921)  time: 2.5746  data: 0.0000  max mem: 18868
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  68/4722]  eta: 3:18:15  lr: 0.000010  loss: 7.6665  loss_lm: 7.8721 (7.8823)  time: 2.5732  data: 0.0000  max mem: 18868Train: data epoch: [0]  [  68/4722]  eta: 3:18:15  lr: 0.000010  loss: 7.8435  loss_lm: 7.8279 (7.8639)  time: 2.5733  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [  68/4722]  eta: 3:18:17  lr: 0.000010  loss: 7.6247  loss_lm: 7.8076 (7.8503)  time: 2.5733  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  68/4722]  eta: 3:18:15  lr: 0.000010  loss: 7.6965  loss_lm: 7.8889 (7.8442)  time: 2.5732  data: 0.0000  max mem: 19004Train: data epoch: [0]  [  68/4722]  eta: 3:18:15  lr: 0.000010  loss: 7.6966  loss_lm: 7.7662 (7.8837)  time: 2.5732  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [  68/4722]  eta: 3:18:15  lr: 0.000010  loss: 7.5537  loss_lm: 7.8531 (7.8920)  time: 2.5732  data: 0.0000  max mem: 18888
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  69/4722]  eta: 3:14:45  lr: 0.000010  loss: 7.4824  loss_lm: 7.8721 (7.8656)  time: 2.5719  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [  69/4722]  eta: 3:14:45  lr: 0.000010  loss: 7.6004  loss_lm: 7.7640 (7.8529)  time: 2.5719  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [  69/4722]  eta: 3:14:45  lr: 0.000010  loss: 7.8943  loss_lm: 7.8889 (7.8463)  time: 2.5718  data: 0.0000  max mem: 19004Train: data epoch: [0]  [  69/4722]  eta: 3:14:45  lr: 0.000010  loss: 7.8277  loss_lm: 7.7662 (7.8814)  time: 2.5718  data: 0.0000  max mem: 18867

Train: data epoch: [0]  [  69/4722]  eta: 3:14:45  lr: 0.000010  loss: 7.9814  loss_lm: 7.8531 (7.8957)  time: 2.5718  data: 0.0000  max mem: 18888
Train: data epoch: [0]  [  69/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.4823  loss_lm: 7.7919 (7.8350)  time: 2.5720  data: 0.0000  max mem: 18864
2023-11-12 23:55:15,674 [INFO] Saving checkpoint at iters: 70 and epoch: 0
2023-11-12 23:55:15,678 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:00:08.445288] Starting job[2023-11-13 00:00:08.445287] Starting job[2023-11-13 00:00:08.445288] Starting job[2023-11-13 00:00:08.445291] Starting job[2023-11-13 00:00:08.445288] Starting job

[2023-11-13 00:00:08.445292] Starting job



| distributed init (rank 5, world 12): env://
| distributed init (rank 3, world 12): env://
| distributed init (rank 4, world 12): env://
| distributed init (rank 1, world 12): env://
| distributed init (rank 2, world 12): env://
| distributed init (rank 0, world 12): env://
2023-11-13 00:00:11,055 [INFO] 
=====  Running Parameters    =====
2023-11-13 00:00:11,055 [INFO] {
    "amp": true,
    "batch_size_eval": 10,
    "batch_size_train": 10,
    "checkpoint_freq": 5,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "eval_freq": 2,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 1e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 20,
    "min_len": 5,
    "min_lr": 0,
    "num_beams": 3,
    "num_workers": 3,
    "output_dir": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint",
    "rank": 0,
    "resume_ckpt_path": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth",
    "runner": "runner_base",
    "seed": 42,
    "task": "captioning",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "weight_decay": 0.05,
    "world_size": 12
}
2023-11-13 00:00:11,055 [INFO] 
======  Dataset Attributes  ======
2023-11-13 00:00:11,055 [INFO] 
======== coco_caption =======
2023-11-13 00:00:11,056 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/mnt/.node1/Open-Datasets/coco"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        },
        "train": {
            "name": "blip_caption",
            "prompt": "a picture of "
        }
    },
    "vis_processor": {
        "eval": {
            "name": "blip_image_eval"
        },
        "train": {
            "name": "blip_image_train"
        }
    }
}
2023-11-13 00:00:11,056 [INFO] 
======  Model Attributes  ======
2023-11-13 00:00:11,056 [INFO] {
    "arch": "blip_caption",
    "finetuned": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth",
    "image_size": 384,
    "load_finetuned": false,
    "load_pretrained": false,
    "med_config_path": "configs/models/med_large_config.json",
    "model_type": "large_coco",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth",
    "prompt": "a picture of ",
    "vit_ckpt_layer": 5,
    "vit_grad_ckpt": true,
    "vit_type": "large"
}
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-11-13 00:00:11,058 [INFO] Building datasets...
2023-11-13 00:00:26,096 [INFO] number of trainable parameters: 446290492
2023-11-13 00:00:26,398 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-11-13 00:00:26,398 [INFO] Loaded 566747 records for train split from the dataset.
2023-11-13 00:00:26,398 [INFO] Loaded 5000 records for val split from the dataset.
2023-11-13 00:00:26,398 [INFO] Loaded 5000 records for test split from the dataset.
2023-11-13 00:00:26,399 [INFO] Resume checkpoint from /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth
2023-11-13 00:00:26,401 [INFO] Start training
2023-11-13 00:00:26,412 [INFO] Start training epoch 0, 4722 iters per inner epoch.
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  66/4722]  eta: 5:37:47  lr: 0.000010  loss: 7.9328  loss_lm: 7.9328 (7.9328)  time: 4.3531  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  66/4722]  eta: 5:37:52  lr: 0.000010  loss: 7.4015  loss_lm: 7.4015 (7.4015)  time: 4.3540  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  66/4722]  eta: 5:38:06  lr: 0.000010  loss: 8.0448  loss_lm: 8.0448 (8.0448)  time: 4.3570  data: 0.0000  max mem: 18870Train: data epoch: [0]  [  66/4722]  eta: 5:37:56  lr: 0.000010  loss: 7.5268  loss_lm: 7.5268 (7.5268)  time: 4.3549  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  66/4722]  eta: 5:38:00  lr: 0.000010  loss: 7.5252  loss_lm: 7.5252 (7.5252)  time: 4.3557  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  66/4722]  eta: 5:37:51  lr: 0.000010  loss: 7.6541  loss_lm: 7.6541 (7.6541)  time: 4.3539  data: 0.0000  max mem: 18863
2023-11-13 00:00:30,779 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  67/4722]  eta: 3:46:46  lr: 0.000010  loss: 7.6050  loss_lm: 7.6050 (7.7689)  time: 2.9229  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  67/4722]  eta: 3:46:48  lr: 0.000010  loss: 7.9482  loss_lm: 7.4015 (7.6748)  time: 2.9234  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  67/4722]  eta: 3:46:52  lr: 0.000010  loss: 7.9237  loss_lm: 7.5252 (7.7245)  time: 2.9242  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  67/4722]  eta: 3:46:48  lr: 0.000010  loss: 7.8747  loss_lm: 7.6541 (7.7644)  time: 2.9233  data: 0.0000  max mem: 18863Train: data epoch: [0]  [  67/4722]  eta: 3:46:50  lr: 0.000010  loss: 7.7795  loss_lm: 7.5268 (7.6531)  time: 2.9238  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  67/4722]  eta: 3:46:55  lr: 0.000010  loss: 7.7196  loss_lm: 7.7196 (7.8822)  time: 2.9248  data: 0.0000  max mem: 18870



Train: data epoch: [0]  [  68/4722]  eta: 3:09:51  lr: 0.000010  loss: 7.8599  loss_lm: 7.8599 (7.7992)  time: 2.4476  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  68/4722]  eta: 3:09:52  lr: 0.000010  loss: 7.6899  loss_lm: 7.6899 (7.6798)  time: 2.4479  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  68/4722]  eta: 3:09:57  lr: 0.000010  loss: 7.6049  loss_lm: 7.7196 (7.7898)  time: 2.4489  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  68/4722]  eta: 3:09:52  lr: 0.000010  loss: 7.6796  loss_lm: 7.6796 (7.7361)  time: 2.4479  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  68/4722]  eta: 3:09:55  lr: 0.000010  loss: 7.7627  loss_lm: 7.7627 (7.7372)  time: 2.4485  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  68/4722]  eta: 3:09:54  lr: 0.000010  loss: 7.5442  loss_lm: 7.5442 (7.6168)  time: 2.4482  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  69/4722]  eta: 2:51:05  lr: 0.000010  loss: 7.6003  loss_lm: 7.6050 (7.7495)  time: 2.2062  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  69/4722]  eta: 2:51:06  lr: 0.000010  loss: 7.8817  loss_lm: 7.6899 (7.7303)  time: 2.2064  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  69/4722]  eta: 2:51:07  lr: 0.000010  loss: 8.0105  loss_lm: 7.5442 (7.7152)  time: 2.2066  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  69/4722]  eta: 2:51:06  lr: 0.000010  loss: 7.5455  loss_lm: 7.6541 (7.6885)  time: 2.2064  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  69/4722]  eta: 2:51:08  lr: 0.000010  loss: 7.8481  loss_lm: 7.7627 (7.7649)  time: 2.2068  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  69/4722]  eta: 2:51:09  lr: 0.000010  loss: 7.4899  loss_lm: 7.6049 (7.7148)  time: 2.2071  data: 0.0000  max mem: 18870
2023-11-13 00:00:36,728 [INFO] Saving checkpoint at iters: 70 and epoch: 0
2023-11-13 00:00:36,733 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:00:42,378 [INFO] Saved successfully
2023-11-13 00:00:42,379 [INFO] Averaged stats: lr: 0.0000  loss: 7.7186  loss_lm: 7.7186
Train: data epoch: [0]  [  70/4722]  eta: 4:07:31  lr: 0.000010  loss: 7.7339  loss_lm: 7.7196 (7.7186)  time: 3.1924  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  70/4722]  eta: 4:07:27  lr: 0.000010  loss: 8.0506  loss_lm: 7.8599 (7.8097)  time: 3.1917  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  70/4722]  eta: 4:07:29  lr: 0.000010  loss: 7.5228  loss_lm: 7.5442 (7.6767)  time: 3.1920  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  70/4722]  eta: 4:07:28  lr: 0.000010  loss: 7.6126  loss_lm: 7.6541 (7.6733)  time: 3.1918  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  70/4722]  eta: 4:07:28  lr: 0.000010  loss: 7.2471  loss_lm: 7.6899 (7.6337)  time: 3.1918  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  70/4722]  eta: 4:07:29  lr: 0.000010  loss: 7.7032  loss_lm: 7.7627 (7.7526)  time: 3.1921  data: 0.0000  max mem: 19015
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  71/4722]  eta: 3:45:19  lr: 0.000010  loss: 7.9265  loss_lm: 7.7196 (7.7533)  time: 2.9067  data: 0.0000  max mem: 18870Train: data epoch: [0]  [  71/4722]  eta: 3:45:16  lr: 0.000010  loss: 7.5938  loss_lm: 7.5938 (7.6270)  time: 2.9061  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [  71/4722]  eta: 3:45:16  lr: 0.000010  loss: 7.6109  loss_lm: 7.6126 (7.6629)  time: 2.9061  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  71/4722]  eta: 3:45:17  lr: 0.000010  loss: 7.5883  loss_lm: 7.7032 (7.7252)  time: 2.9064  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  71/4722]  eta: 3:45:15  lr: 0.000010  loss: 7.4465  loss_lm: 7.6050 (7.7492)  time: 2.9060  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  71/4722]  eta: 3:45:16  lr: 0.000010  loss: 7.6684  loss_lm: 7.5442 (7.6753)  time: 2.9063  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  72/4722]  eta: 3:29:27  lr: 0.000010  loss: 7.9875  loss_lm: 7.7339 (7.7867)  time: 2.7027  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  72/4722]  eta: 3:29:24  lr: 0.000010  loss: 7.3886  loss_lm: 7.6050 (7.6977)  time: 2.7021  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  72/4722]  eta: 3:29:24  lr: 0.000010  loss: 7.1035  loss_lm: 7.6126 (7.5830)  time: 2.7021  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  72/4722]  eta: 3:29:25  lr: 0.000010  loss: 7.6887  loss_lm: 7.6684 (7.6773)  time: 2.7023  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  72/4722]  eta: 3:29:25  lr: 0.000010  loss: 7.5251  loss_lm: 7.5938 (7.6125)  time: 2.7022  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  72/4722]  eta: 3:29:26  lr: 0.000010  loss: 7.7252  loss_lm: 7.7252 (7.7252)  time: 2.7024  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  73/4722]  eta: 3:17:30  lr: 0.000010  loss: 7.4068  loss_lm: 7.6003 (7.6613)  time: 2.5489  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  73/4722]  eta: 3:17:32  lr: 0.000010  loss: 7.5934  loss_lm: 7.7196 (7.7626)  time: 2.5495  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  73/4722]  eta: 3:17:31  lr: 0.000010  loss: 7.8527  loss_lm: 7.7252 (7.7411)  time: 2.5492  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  73/4722]  eta: 3:17:30  lr: 0.000010  loss: 7.2369  loss_lm: 7.5251 (7.5655)  time: 2.5490  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  73/4722]  eta: 3:17:30  lr: 0.000010  loss: 7.3181  loss_lm: 7.6109 (7.5499)  time: 2.5490  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  73/4722]  eta: 3:17:30  lr: 0.000010  loss: 8.3837  loss_lm: 7.6684 (7.7656)  time: 2.5491  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  74/4722]  eta: 3:08:16  lr: 0.000010  loss: 7.6192  loss_lm: 7.7196 (7.7466)  time: 2.4304  data: 0.0000  max mem: 18870Train: data epoch: [0]  [  74/4722]  eta: 3:08:14  lr: 0.000010  loss: 7.8420  loss_lm: 7.6050 (7.6814)  time: 2.4299  data: 0.0000  max mem: 18876

Train: data epoch: [0]  [  74/4722]  eta: 3:08:14  lr: 0.000010  loss: 7.8128  loss_lm: 7.6126 (7.5791)  time: 2.4299  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  74/4722]  eta: 3:08:14  lr: 0.000010  loss: 7.3257  loss_lm: 7.5251 (7.5389)  time: 2.4300  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  74/4722]  eta: 3:08:15  lr: 0.000010  loss: 7.7323  loss_lm: 7.7323 (7.7402)  time: 2.4301  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  74/4722]  eta: 3:08:14  lr: 0.000010  loss: 7.7630  loss_lm: 7.6887 (7.7653)  time: 2.4301  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:00:49,788 [INFO] Saving checkpoint at iters: 75 and epoch: 0
2023-11-13 00:00:49,793 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:00:54,945 [INFO] Saved successfully
2023-11-13 00:00:54,947 [INFO] Averaged stats: lr: 0.0000  loss: 7.6975  loss_lm: 7.6975
Train: data epoch: [0]  [  75/4722]  eta: 3:40:56  lr: 0.000010  loss: 7.2552  loss_lm: 7.6192 (7.6975)  time: 2.8527  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  75/4722]  eta: 3:40:55  lr: 0.000010  loss: 7.2545  loss_lm: 7.6684 (7.7142)  time: 2.8524  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  75/4722]  eta: 3:40:54  lr: 0.000010  loss: 7.5142  loss_lm: 7.6109 (7.5726)  time: 2.8523  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  75/4722]  eta: 3:40:55  lr: 0.000010  loss: 7.1150  loss_lm: 7.7252 (7.6776)  time: 2.8525  data: 0.0000  max mem: 19015Train: data epoch: [0]  [  75/4722]  eta: 3:40:54  lr: 0.000010  loss: 8.0468  loss_lm: 7.5251 (7.5897)  time: 2.8524  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [  75/4722]  eta: 3:40:54  lr: 0.000010  loss: 7.7601  loss_lm: 7.6050 (7.6893)  time: 2.8523  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  76/4722]  eta: 3:31:34  lr: 0.000010  loss: 8.0509  loss_lm: 7.6126 (7.6161)  time: 2.7324  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  76/4722]  eta: 3:31:35  lr: 0.000010  loss: 7.9144  loss_lm: 7.7323 (7.6992)  time: 2.7325  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  76/4722]  eta: 3:31:34  lr: 0.000010  loss: 7.5029  loss_lm: 7.6050 (7.6723)  time: 2.7323  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  76/4722]  eta: 3:31:34  lr: 0.000010  loss: 7.4691  loss_lm: 7.5251 (7.5787)  time: 2.7324  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  76/4722]  eta: 3:31:34  lr: 0.000010  loss: 7.6963  loss_lm: 7.6887 (7.7126)  time: 2.7324  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  76/4722]  eta: 3:31:36  lr: 0.000010  loss: 7.6533  loss_lm: 7.6533 (7.6935)  time: 2.7328  data: 0.0000  max mem: 18870
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  77/4722]  eta: 3:23:25  lr: 0.000010  loss: 7.9218  loss_lm: 7.6050 (7.6931)  time: 2.6277  data: 0.0000  max mem: 18876Train: data epoch: [0]  [  77/4722]  eta: 3:23:25  lr: 0.000010  loss: 7.9967  loss_lm: 7.5251 (7.6135)  time: 2.6277  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [  77/4722]  eta: 3:23:26  lr: 0.000010  loss: 7.4014  loss_lm: 7.6684 (7.6866)  time: 2.6278  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  77/4722]  eta: 3:23:26  lr: 0.000010  loss: 7.8895  loss_lm: 7.7323 (7.7150)  time: 2.6279  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  77/4722]  eta: 3:23:25  lr: 0.000010  loss: 7.5535  loss_lm: 7.6109 (7.6109)  time: 2.6277  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  77/4722]  eta: 3:23:27  lr: 0.000010  loss: 7.4559  loss_lm: 7.6192 (7.6737)  time: 2.6281  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  78/4722]  eta: 3:16:30  lr: 0.000010  loss: 8.3549  loss_lm: 7.7601 (7.7440)  time: 2.5388  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  78/4722]  eta: 3:16:30  lr: 0.000010  loss: 7.3236  loss_lm: 7.6684 (7.6587)  time: 2.5389  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  78/4722]  eta: 3:16:30  lr: 0.000010  loss: 7.6262  loss_lm: 7.7323 (7.7082)  time: 2.5389  data: 0.0000  max mem: 19015

Train: data epoch: [0]  [  78/4722]  eta: 3:16:30  lr: 0.000010  loss: 7.6430  loss_lm: 7.5938 (7.6158)  time: 2.5388  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  78/4722]  eta: 3:16:31  lr: 0.000010  loss: 7.8624  loss_lm: 7.6533 (7.6882)  time: 2.5392  data: 0.0000  max mem: 18870

Train: data epoch: [0]  [  78/4722]  eta: 3:16:30  lr: 0.000010  loss: 7.8998  loss_lm: 7.6126 (7.6331)  time: 2.5388  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  79/4722]  eta: 3:10:37  lr: 0.000010  loss: 7.2176  loss_lm: 7.7252 (7.6731)  time: 2.4633  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  79/4722]  eta: 3:10:37  lr: 0.000010  loss: 7.8683  loss_lm: 7.6684 (7.6737)  time: 2.4633  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  79/4722]  eta: 3:10:36  lr: 0.000010  loss: 7.5874  loss_lm: 7.5874 (7.6138)  time: 2.4632  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [  79/4722]  eta: 3:10:36  lr: 0.000010  loss: 7.9297  loss_lm: 7.7601 (7.7573)  time: 2.4632  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  79/4722]  eta: 3:10:36  lr: 0.000010  loss: 7.4294  loss_lm: 7.6109 (7.6186)  time: 2.4632  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  79/4722]  eta: 3:10:38  lr: 0.000010  loss: 7.7423  loss_lm: 7.6533 (7.6920)  time: 2.4636  data: 0.0000  max mem: 18870
2023-11-13 00:01:02,395 [INFO] Saving checkpoint at iters: 80 and epoch: 0
2023-11-13 00:01:02,400 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:01:07,877 [INFO] Saved successfully
2023-11-13 00:01:07,878 [INFO] Averaged stats: lr: 0.0000  loss: 7.6966  loss_lm: 7.6966
Train: data epoch: [0]  [  80/4722]  eta: 3:33:49  lr: 0.000010  loss: 7.7609  loss_lm: 7.7196 (7.6966)  time: 2.7637  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  80/4722]  eta: 3:33:47  lr: 0.000010  loss: 7.6100  loss_lm: 7.6109 (7.6180)  time: 2.7634  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  80/4722]  eta: 3:33:47  lr: 0.000010  loss: 7.3921  loss_lm: 7.5874 (7.5990)  time: 2.7635  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  80/4722]  eta: 3:33:48  lr: 0.000010  loss: 7.5022  loss_lm: 7.6684 (7.6623)  time: 2.7635  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  80/4722]  eta: 3:33:48  lr: 0.000010  loss: 7.8361  loss_lm: 7.7323 (7.6840)  time: 2.7636  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  80/4722]  eta: 3:33:47  lr: 0.000010  loss: 7.8038  loss_lm: 7.8038 (7.7604)  time: 2.7634  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  81/4722]  eta: 3:27:35  lr: 0.000010  loss: 7.5137  loss_lm: 7.7601 (7.7450)  time: 2.6838  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  81/4722]  eta: 3:27:35  lr: 0.000010  loss: 7.8488  loss_lm: 7.5874 (7.6146)  time: 2.6839  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  81/4722]  eta: 3:27:36  lr: 0.000010  loss: 7.3445  loss_lm: 7.7252 (7.6628)  time: 2.6839  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  81/4722]  eta: 3:27:35  lr: 0.000010  loss: 7.8660  loss_lm: 7.6109 (7.6335)  time: 2.6838  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  81/4722]  eta: 3:27:36  lr: 0.000010  loss: 7.3334  loss_lm: 7.5442 (7.6417)  time: 2.6839  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  81/4722]  eta: 3:27:37  lr: 0.000010  loss: 7.5430  loss_lm: 7.6533 (7.6870)  time: 2.6842  data: 0.0000  max mem: 18870
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  82/4722]  eta: 3:22:04  lr: 0.000010  loss: 7.5462  loss_lm: 7.7252 (7.6559)  time: 2.6131  data: 0.0000  max mem: 19015Train: data epoch: [0]  [  82/4722]  eta: 3:22:04  lr: 0.000010  loss: 7.4674  loss_lm: 7.5874 (7.6059)  time: 2.6130  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [  82/4722]  eta: 3:22:04  lr: 0.000010  loss: 7.5500  loss_lm: 7.7601 (7.7335)  time: 2.6130  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  82/4722]  eta: 3:22:04  lr: 0.000010  loss: 7.4229  loss_lm: 7.5442 (7.6288)  time: 2.6131  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  82/4722]  eta: 3:22:04  lr: 0.000010  loss: 7.4361  loss_lm: 7.6109 (7.6219)  time: 2.6130  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  82/4722]  eta: 3:22:05  lr: 0.000010  loss: 7.4641  loss_lm: 7.6533 (7.6739)  time: 2.6133  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  83/4722]  eta: 3:17:08  lr: 0.000010  loss: 7.9551  loss_lm: 7.7252 (7.6726)  time: 2.5498  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  83/4722]  eta: 3:17:08  lr: 0.000010  loss: 7.5436  loss_lm: 7.5436 (7.6025)  time: 2.5497  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  83/4722]  eta: 3:17:08  lr: 0.000010  loss: 7.3889  loss_lm: 7.6050 (7.7144)  time: 2.5497  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  83/4722]  eta: 3:17:08  lr: 0.000010  loss: 7.3923  loss_lm: 7.6100 (7.6091)  time: 2.5497  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  83/4722]  eta: 3:17:08  lr: 0.000010  loss: 7.7810  loss_lm: 7.5442 (7.6373)  time: 2.5498  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  83/4722]  eta: 3:17:09  lr: 0.000010  loss: 7.5945  loss_lm: 7.6192 (7.6695)  time: 2.5500  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  84/4722]  eta: 3:12:43  lr: 0.000010  loss: 7.5715  loss_lm: 7.7252 (7.6672)  time: 2.4932  data: 0.0000  max mem: 19015Train: data epoch: [0]  [  84/4722]  eta: 3:12:42  lr: 0.000010  loss: 7.5696  loss_lm: 7.6050 (7.7067)  time: 2.4931  data: 0.0000  max mem: 18876

Train: data epoch: [0]  [  84/4722]  eta: 3:12:42  lr: 0.000010  loss: 7.4001  loss_lm: 7.5436 (7.5918)  time: 2.4931  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  84/4722]  eta: 3:12:42  lr: 0.000010  loss: 7.6673  loss_lm: 7.6109 (7.6122)  time: 2.4931  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  84/4722]  eta: 3:12:43  lr: 0.000010  loss: 7.7641  loss_lm: 7.6684 (7.6440)  time: 2.4931  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  84/4722]  eta: 3:12:44  lr: 0.000010  loss: 6.9067  loss_lm: 7.6192 (7.6294)  time: 2.4933  data: 0.0000  max mem: 18870

2023-11-13 00:01:15,271 [INFO] Saving checkpoint at iters: 85 and epoch: 0
2023-11-13 00:01:15,275 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:01:20,782 [INFO] Saved successfully
2023-11-13 00:01:20,783 [INFO] Averaged stats: lr: 0.0000  loss: 7.6206  loss_lm: 7.6206
Train: data epoch: [0]  [  85/4722]  eta: 3:30:02  lr: 0.000010  loss: 7.4540  loss_lm: 7.6049 (7.6206)  time: 2.7179  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  85/4722]  eta: 3:30:01  lr: 0.000010  loss: 7.1870  loss_lm: 7.6100 (7.5909)  time: 2.7176  data: 0.0000  max mem: 18951Train: data epoch: [0]  [  85/4722]  eta: 3:30:01  lr: 0.000010  loss: 7.3446  loss_lm: 7.5442 (7.6290)  time: 2.7177  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  85/4722]  eta: 3:30:01  lr: 0.000010  loss: 7.7720  loss_lm: 7.5436 (7.6008)  time: 2.7177  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  85/4722]  eta: 3:30:02  lr: 0.000010  loss: 7.3495  loss_lm: 7.7032 (7.6513)  time: 2.7177  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  85/4722]  eta: 3:30:01  lr: 0.000010  loss: 7.4843  loss_lm: 7.6003 (7.6956)  time: 2.7176  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  86/4722]  eta: 3:25:26  lr: 0.000010  loss: 7.4217  loss_lm: 7.7032 (7.6404)  time: 2.5741  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  86/4722]  eta: 3:25:26  lr: 0.000010  loss: 7.5179  loss_lm: 7.5696 (7.6872)  time: 2.5741  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  86/4722]  eta: 3:25:26  lr: 0.000010  loss: 7.7332  loss_lm: 7.6100 (7.5977)  time: 2.5741  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  86/4722]  eta: 3:25:26  lr: 0.000010  loss: 7.5073  loss_lm: 7.5442 (7.6232)  time: 2.5741  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  86/4722]  eta: 3:25:27  lr: 0.000010  loss: 7.3943  loss_lm: 7.5945 (7.6098)  time: 2.5742  data: 0.0000  max mem: 18870Train: data epoch: [0]  [  86/4722]  eta: 3:25:26  lr: 0.000010  loss: 7.7037  loss_lm: 7.5874 (7.6057)  time: 2.5741  data: 0.0000  max mem: 18910

Train: data epoch: [0]  [  87/4722]  eta: 3:21:15  lr: 0.000010  loss: 7.4677  loss_lm: 7.5934 (7.6034)  time: 2.5734  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  87/4722]  eta: 3:21:14  lr: 0.000010  loss: 7.5576  loss_lm: 7.5576 (7.6035)  time: 2.5733  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  87/4722]  eta: 3:21:14  lr: 0.000010  loss: 7.6895  loss_lm: 7.5442 (7.6262)  time: 2.5733  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  87/4722]  eta: 3:21:14  lr: 0.000010  loss: 7.3787  loss_lm: 7.6262 (7.6285)  time: 2.5733  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  87/4722]  eta: 3:21:14  lr: 0.000010  loss: 7.6722  loss_lm: 7.5696 (7.6865)  time: 2.5733  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  87/4722]  eta: 3:21:14  lr: 0.000010  loss: 7.3919  loss_lm: 7.5535 (7.5884)  time: 2.5733  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  88/4722]  eta: 3:17:24  lr: 0.000010  loss: 7.4773  loss_lm: 7.5500 (7.6774)  time: 2.5723  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  88/4722]  eta: 3:17:24  lr: 0.000010  loss: 7.4156  loss_lm: 7.5436 (7.5954)  time: 2.5723  data: 0.0000  max mem: 18910
Train: data epoch: [0]  [  88/4722]  eta: 3:17:25  lr: 0.000010  loss: 6.9537  loss_lm: 7.5883 (7.5992)  time: 2.5723  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  88/4722]  eta: 3:17:25  lr: 0.000010  loss: 7.8509  loss_lm: 7.5934 (7.6141)  time: 2.5724  data: 0.0000  max mem: 18870Train: data epoch: [0]  [  88/4722]  eta: 3:17:24  lr: 0.000010  loss: 7.2733  loss_lm: 7.5455 (7.5747)  time: 2.5723  data: 0.0000  max mem: 18951

Train: data epoch: [0]  [  88/4722]  eta: 3:17:24  lr: 0.000010  loss: 7.7317  loss_lm: 7.6684 (7.6308)  time: 2.5723  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  89/4722]  eta: 3:13:53  lr: 0.000010  loss: 7.3713  loss_lm: 7.5179 (7.6646)  time: 2.5720  data: 0.0000  max mem: 18876
Train: data epoch: [0]  [  89/4722]  eta: 3:13:53  lr: 0.000010  loss: 7.4643  loss_lm: 7.5715 (7.5936)  time: 2.5720  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [  89/4722]  eta: 3:13:54  lr: 0.000010  loss: 7.3318  loss_lm: 7.5934 (7.6024)  time: 2.5721  data: 0.0000  max mem: 18870
Train: data epoch: [0]  [  89/4722]  eta: 3:13:53  lr: 0.000010  loss: 7.7489  loss_lm: 7.5436 (7.6018)  time: 2.5720  data: 0.0000  max mem: 18910Train: data epoch: [0]  [  89/4722]  eta: 3:13:53  lr: 0.000010  loss: 7.3014  loss_lm: 7.5142 (7.5633)  time: 2.5720  data: 0.0000  max mem: 18951

Train: data epoch: [0]  [  89/4722]  eta: 3:13:53  lr: 0.000010  loss: 7.1128  loss_lm: 7.5228 (7.6092)  time: 2.5720  data: 0.0000  max mem: 18865
2023-11-13 00:01:28,173 [INFO] Saving checkpoint at iters: 90 and epoch: 0
2023-11-13 00:01:28,177 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:03:25.570805] Starting job[2023-11-13 00:03:25.570806] Starting job[2023-11-13 00:03:25.570805] Starting job[2023-11-13 00:03:25.570805] Starting job[2023-11-13 00:03:25.570807] Starting job[2023-11-13 00:03:25.570807] Starting job





| distributed init (rank 1, world 12): env://
| distributed init (rank 0, world 12): env://
| distributed init (rank 4, world 12): env://
| distributed init (rank 2, world 12): env://
| distributed init (rank 3, world 12): env://
| distributed init (rank 5, world 12): env://
2023-11-13 00:03:32,491 [INFO] 
=====  Running Parameters    =====
2023-11-13 00:03:32,491 [INFO] {
    "amp": true,
    "batch_size_eval": 10,
    "batch_size_train": 10,
    "checkpoint_freq": 5,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "eval_freq": 2,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 1e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 20,
    "min_len": 5,
    "min_lr": 0,
    "num_beams": 3,
    "num_workers": 3,
    "output_dir": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint",
    "rank": 0,
    "resume_ckpt_path": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth",
    "runner": "runner_base",
    "seed": 42,
    "task": "captioning",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "weight_decay": 0.05,
    "world_size": 12
}
2023-11-13 00:03:32,492 [INFO] 
======  Dataset Attributes  ======
2023-11-13 00:03:32,492 [INFO] 
======== coco_caption =======
2023-11-13 00:03:32,492 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/mnt/.node1/Open-Datasets/coco"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        },
        "train": {
            "name": "blip_caption",
            "prompt": "a picture of "
        }
    },
    "vis_processor": {
        "eval": {
            "name": "blip_image_eval"
        },
        "train": {
            "name": "blip_image_train"
        }
    }
}
2023-11-13 00:03:32,492 [INFO] 
======  Model Attributes  ======
2023-11-13 00:03:32,492 [INFO] {
    "arch": "blip_caption",
    "finetuned": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth",
    "image_size": 384,
    "load_finetuned": false,
    "load_pretrained": false,
    "med_config_path": "configs/models/med_large_config.json",
    "model_type": "large_coco",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth",
    "prompt": "a picture of ",
    "vit_ckpt_layer": 5,
    "vit_grad_ckpt": true,
    "vit_type": "large"
}
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-11-13 00:03:32,493 [INFO] Building datasets...
2023-11-13 00:03:47,294 [INFO] number of trainable parameters: 446290492
2023-11-13 00:03:47,593 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-11-13 00:03:47,593 [INFO] Loaded 566747 records for train split from the dataset.
2023-11-13 00:03:47,593 [INFO] Loaded 5000 records for val split from the dataset.
2023-11-13 00:03:47,593 [INFO] Loaded 5000 records for test split from the dataset.
2023-11-13 00:03:47,594 [INFO] Resume checkpoint from /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth
2023-11-13 00:03:47,596 [INFO] Start training
2023-11-13 00:03:47,607 [INFO] Start training epoch 0, 4722 iters per inner epoch.
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  86/4722]  eta: 5:08:39  lr: 0.000010  loss: 7.4662  loss_lm: 7.4662 (7.4662)  time: 3.9947  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  86/4722]  eta: 5:09:01  lr: 0.000010  loss: 7.4333  loss_lm: 7.4333 (7.4333)  time: 3.9995  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  86/4722]  eta: 5:08:50  lr: 0.000010  loss: 7.4037  loss_lm: 7.4037 (7.4037)  time: 3.9971  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  86/4722]  eta: 5:08:34  lr: 0.000010  loss: 7.6988  loss_lm: 7.6988 (7.6988)  time: 3.9936  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  86/4722]  eta: 5:08:33  lr: 0.000010  loss: 7.5187  loss_lm: 7.5187 (7.5187)  time: 3.9934  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  86/4722]  eta: 5:08:45  lr: 0.000010  loss: 7.7473  loss_lm: 7.7473 (7.7473)  time: 3.9960  data: 0.0000  max mem: 18864

2023-11-13 00:03:51,614 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  87/4722]  eta: 3:31:42  lr: 0.000010  loss: 7.5782  loss_lm: 7.5782 (7.6385)  time: 2.7405  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  87/4722]  eta: 3:31:45  lr: 0.000010  loss: 7.6379  loss_lm: 7.4662 (7.5520)  time: 2.7411  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [  87/4722]  eta: 3:31:50  lr: 0.000010  loss: 7.3825  loss_lm: 7.3825 (7.3931)  time: 2.7423  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  87/4722]  eta: 3:31:56  lr: 0.000010  loss: 7.4580  loss_lm: 7.4333 (7.4457)  time: 2.7435  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  87/4722]  eta: 3:31:41  lr: 0.000010  loss: 7.6751  loss_lm: 7.5187 (7.5969)  time: 2.7404  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  87/4722]  eta: 3:31:47  lr: 0.000010  loss: 7.3448  loss_lm: 7.3448 (7.5461)  time: 2.7417  data: 0.0000  max mem: 18951
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  88/4722]  eta: 2:59:08  lr: 0.000010  loss: 7.7787  loss_lm: 7.6379 (7.6276)  time: 2.3194  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  88/4722]  eta: 2:59:06  lr: 0.000010  loss: 7.3903  loss_lm: 7.5782 (7.5558)  time: 2.3190  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  88/4722]  eta: 2:59:11  lr: 0.000010  loss: 6.9234  loss_lm: 7.3825 (7.2365)  time: 2.3201  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  88/4722]  eta: 2:59:09  lr: 0.000010  loss: 7.2313  loss_lm: 7.3448 (7.4411)  time: 2.3197  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  88/4722]  eta: 2:59:15  lr: 0.000010  loss: 7.8520  loss_lm: 7.4580 (7.5811)  time: 2.3210  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  88/4722]  eta: 2:59:05  lr: 0.000010  loss: 7.5296  loss_lm: 7.5296 (7.5745)  time: 2.3189  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  89/4722]  eta: 2:42:45  lr: 0.000010  loss: 7.7387  loss_lm: 7.5782 (7.6015)  time: 2.1077  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  89/4722]  eta: 2:42:46  lr: 0.000010  loss: 7.1182  loss_lm: 7.4662 (7.5002)  time: 2.1080  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  89/4722]  eta: 2:42:47  lr: 0.000010  loss: 7.2904  loss_lm: 7.2904 (7.4034)  time: 2.1083  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  89/4722]  eta: 2:42:51  lr: 0.000010  loss: 7.3421  loss_lm: 7.4333 (7.5214)  time: 2.1092  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  89/4722]  eta: 2:42:49  lr: 0.000010  loss: 7.4299  loss_lm: 7.3825 (7.2849)  time: 2.1086  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  89/4722]  eta: 2:42:44  lr: 0.000010  loss: 7.3581  loss_lm: 7.5187 (7.5204)  time: 2.1076  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:03:57,517 [INFO] Saving checkpoint at iters: 90 and epoch: 0
2023-11-13 00:03:57,522 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:04:03,553 [INFO] Saved successfully
2023-11-13 00:04:03,555 [INFO] Averaged stats: lr: 0.0000  loss: 7.4937  loss_lm: 7.4937
Train: data epoch: [0]  [  90/4722]  eta: 4:06:12  lr: 0.000010  loss: 7.3829  loss_lm: 7.4333 (7.4937)  time: 3.1893  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  90/4722]  eta: 4:06:08  lr: 0.000010  loss: 8.1019  loss_lm: 7.6379 (7.6206)  time: 3.1884  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  90/4722]  eta: 4:06:09  lr: 0.000010  loss: 7.5311  loss_lm: 7.3448 (7.4290)  time: 3.1886  data: 0.0000  max mem: 18951

Train: data epoch: [0]  [  90/4722]  eta: 4:06:07  lr: 0.000010  loss: 7.8691  loss_lm: 7.6988 (7.6550)  time: 3.1881  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  90/4722]  eta: 4:06:10  lr: 0.000010  loss: 7.4100  loss_lm: 7.4037 (7.3099)  time: 3.1888  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  90/4722]  eta: 4:06:06  lr: 0.000010  loss: 7.1647  loss_lm: 7.5187 (7.4492)  time: 3.1880  data: 0.0000  max mem: 18865

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  91/4722]  eta: 3:44:11  lr: 0.000010  loss: 7.1667  loss_lm: 7.2904 (7.3853)  time: 2.9047  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  91/4722]  eta: 3:44:09  lr: 0.000010  loss: 7.1182  loss_lm: 7.5782 (7.5655)  time: 2.9043  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  91/4722]  eta: 3:44:10  lr: 0.000010  loss: 7.1395  loss_lm: 7.4662 (7.5404)  time: 2.9045  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  91/4722]  eta: 3:44:14  lr: 0.000010  loss: 7.9009  loss_lm: 7.4333 (7.5616)  time: 2.9054  data: 0.0000  max mem: 18866Train: data epoch: [0]  [  91/4722]  eta: 3:44:09  lr: 0.000010  loss: 7.8378  loss_lm: 7.5187 (7.5140)  time: 2.9042  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  91/4722]  eta: 3:44:12  lr: 0.000010  loss: 7.7355  loss_lm: 7.4037 (7.3808)  time: 2.9048  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  92/4722]  eta: 3:28:24  lr: 0.000010  loss: 7.8278  loss_lm: 7.4100 (7.4447)  time: 2.7007  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  92/4722]  eta: 3:28:22  lr: 0.000010  loss: 7.3845  loss_lm: 7.4662 (7.5181)  time: 2.7004  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  92/4722]  eta: 3:28:22  lr: 0.000010  loss: 7.5186  loss_lm: 7.5782 (7.5588)  time: 2.7003  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  92/4722]  eta: 3:28:23  lr: 0.000010  loss: 7.3858  loss_lm: 7.3448 (7.3853)  time: 2.7006  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  92/4722]  eta: 3:28:21  lr: 0.000010  loss: 7.2125  loss_lm: 7.5187 (7.4709)  time: 2.7002  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  92/4722]  eta: 3:28:26  lr: 0.000010  loss: 7.3261  loss_lm: 7.4333 (7.5279)  time: 2.7012  data: 0.0000  max mem: 18866

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  93/4722]  eta: 3:16:33  lr: 0.000010  loss: 7.6708  loss_lm: 7.3448 (7.4210)  time: 2.5477  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  93/4722]  eta: 3:16:32  lr: 0.000010  loss: 7.3531  loss_lm: 7.3845 (7.4975)  time: 2.5476  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  93/4722]  eta: 3:16:35  lr: 0.000010  loss: 7.7706  loss_lm: 7.4333 (7.5583)  time: 2.5483  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [  93/4722]  eta: 3:16:34  lr: 0.000010  loss: 7.0801  loss_lm: 7.4037 (7.3991)  time: 2.5479  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  93/4722]  eta: 3:16:32  lr: 0.000010  loss: 7.2725  loss_lm: 7.5186 (7.5230)  time: 2.5475  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  93/4722]  eta: 3:16:31  lr: 0.000010  loss: 7.4857  loss_lm: 7.4857 (7.4728)  time: 2.5474  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [  94/4722]  eta: 3:07:20  lr: 0.000010  loss: 7.2640  loss_lm: 7.4333 (7.5256)  time: 2.4289  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  94/4722]  eta: 3:07:19  lr: 0.000010  loss: 7.2764  loss_lm: 7.4037 (7.3855)  time: 2.4286  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  94/4722]  eta: 3:07:18  lr: 0.000010  loss: 7.8922  loss_lm: 7.4662 (7.5413)  time: 2.4283  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  94/4722]  eta: 3:07:17  lr: 0.000010  loss: 7.6495  loss_lm: 7.5187 (7.4924)  time: 2.4281  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  94/4722]  eta: 3:07:18  lr: 0.000010  loss: 7.6754  loss_lm: 7.3858 (7.4493)  time: 2.4284  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  94/4722]  eta: 3:07:17  lr: 0.000010  loss: 7.5076  loss_lm: 7.5186 (7.5213)  time: 2.4282  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:04:10,947 [INFO] Saving checkpoint at iters: 95 and epoch: 0
2023-11-13 00:04:10,952 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:04:15,956 [INFO] Saved successfully
2023-11-13 00:04:15,957 [INFO] Averaged stats: lr: 0.0000  loss: 7.4720  loss_lm: 7.4720
Train: data epoch: [0]  [  95/4722]  eta: 3:38:34  lr: 0.000010  loss: 6.9896  loss_lm: 7.3829 (7.4720)  time: 2.8344  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  95/4722]  eta: 3:38:31  lr: 0.000010  loss: 7.2874  loss_lm: 7.5076 (7.4979)  time: 2.8338  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  95/4722]  eta: 3:38:32  lr: 0.000010  loss: 7.4724  loss_lm: 7.3858 (7.4516)  time: 2.8340  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  95/4722]  eta: 3:38:32  lr: 0.000010  loss: 7.3683  loss_lm: 7.3845 (7.5240)  time: 2.8339  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  95/4722]  eta: 3:38:31  lr: 0.000010  loss: 7.2682  loss_lm: 7.4857 (7.4700)  time: 2.8337  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  95/4722]  eta: 3:38:33  lr: 0.000010  loss: 7.4760  loss_lm: 7.4037 (7.3945)  time: 2.8341  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  96/4722]  eta: 3:28:57  lr: 0.000010  loss: 7.5423  loss_lm: 7.5186 (7.5020)  time: 2.7102  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  96/4722]  eta: 3:29:00  lr: 0.000010  loss: 7.1075  loss_lm: 7.3829 (7.4388)  time: 2.7108  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  96/4722]  eta: 3:28:57  lr: 0.000010  loss: 6.9767  loss_lm: 7.3845 (7.4743)  time: 2.7103  data: 0.0000  max mem: 18864Train: data epoch: [0]  [  96/4722]  eta: 3:28:57  lr: 0.000010  loss: 7.3782  loss_lm: 7.4857 (7.4616)  time: 2.7101  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  96/4722]  eta: 3:28:58  lr: 0.000010  loss: 7.3617  loss_lm: 7.4037 (7.3915)  time: 2.7105  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  96/4722]  eta: 3:28:58  lr: 0.000010  loss: 7.1986  loss_lm: 7.3858 (7.4286)  time: 2.7104  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  97/4722]  eta: 3:21:01  lr: 0.000010  loss: 6.9986  loss_lm: 7.3421 (7.4022)  time: 2.6079  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  97/4722]  eta: 3:20:58  lr: 0.000010  loss: 7.4245  loss_lm: 7.5076 (7.4955)  time: 2.6072  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  97/4722]  eta: 3:20:59  lr: 0.000010  loss: 7.4873  loss_lm: 7.3858 (7.4335)  time: 2.6074  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  97/4722]  eta: 3:20:58  lr: 0.000010  loss: 7.7973  loss_lm: 7.3845 (7.5012)  time: 2.6073  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  97/4722]  eta: 3:20:58  lr: 0.000010  loss: 7.6775  loss_lm: 7.4857 (7.4796)  time: 2.6072  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  97/4722]  eta: 3:20:59  lr: 0.000010  loss: 7.8711  loss_lm: 7.4037 (7.4315)  time: 2.6075  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  98/4722]  eta: 3:14:15  lr: 0.000010  loss: 7.4188  loss_lm: 7.3829 (7.4034)  time: 2.5207  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  98/4722]  eta: 3:14:14  lr: 0.000010  loss: 7.3515  loss_lm: 7.4037 (7.4253)  time: 2.5204  data: 0.0000  max mem: 18865Train: data epoch: [0]  [  98/4722]  eta: 3:14:12  lr: 0.000010  loss: 7.4775  loss_lm: 7.4857 (7.4795)  time: 2.5201  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  98/4722]  eta: 3:14:13  lr: 0.000010  loss: 7.3074  loss_lm: 7.3845 (7.4863)  time: 2.5202  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  98/4722]  eta: 3:14:13  lr: 0.000010  loss: 7.6071  loss_lm: 7.5186 (7.5041)  time: 2.5201  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  98/4722]  eta: 3:14:13  lr: 0.000010  loss: 7.9312  loss_lm: 7.4724 (7.4718)  time: 2.5203  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [  99/4722]  eta: 3:08:27  lr: 0.000010  loss: 7.5232  loss_lm: 7.4724 (7.4754)  time: 2.4460  data: 0.0000  max mem: 18951Train: data epoch: [0]  [  99/4722]  eta: 3:08:26  lr: 0.000010  loss: 7.7731  loss_lm: 7.4857 (7.5004)  time: 2.4458  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [  99/4722]  eta: 3:08:29  lr: 0.000010  loss: 7.6043  loss_lm: 7.3829 (7.4178)  time: 2.4464  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [  99/4722]  eta: 3:08:27  lr: 0.000010  loss: 7.0074  loss_lm: 7.3683 (7.4521)  time: 2.4459  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [  99/4722]  eta: 3:08:28  lr: 0.000010  loss: 7.7157  loss_lm: 7.4037 (7.4461)  time: 2.4461  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [  99/4722]  eta: 3:08:27  lr: 0.000010  loss: 7.3910  loss_lm: 7.5076 (7.4960)  time: 2.4458  data: 0.0000  max mem: 18865
2023-11-13 00:04:23,340 [INFO] Saving checkpoint at iters: 100 and epoch: 0
2023-11-13 00:04:23,345 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:04:28,724 [INFO] Saved successfully
2023-11-13 00:04:28,725 [INFO] Averaged stats: lr: 0.0000  loss: 7.4187  loss_lm: 7.4187
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:07  lr: 0.000010  loss: 7.4315  loss_lm: 7.4188 (7.4187)  time: 2.7407  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:04  lr: 0.000010  loss: 7.3746  loss_lm: 7.4857 (7.4920)  time: 2.7401  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:05  lr: 0.000010  loss: 7.7958  loss_lm: 7.5186 (7.5160)  time: 2.7402  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:05  lr: 0.000010  loss: 7.3607  loss_lm: 7.3683 (7.4460)  time: 2.7403  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:05  lr: 0.000010  loss: 6.9912  loss_lm: 7.4724 (7.4432)  time: 2.7403  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 100/4722]  eta: 3:31:06  lr: 0.000010  loss: 7.3704  loss_lm: 7.4037 (7.4410)  time: 2.7404  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 101/4722]  eta: 3:25:00  lr: 0.000010  loss: 7.6860  loss_lm: 7.4188 (7.4354)  time: 2.6618  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 101/4722]  eta: 3:24:58  lr: 0.000010  loss: 7.3015  loss_lm: 7.3825 (7.4323)  time: 2.6615  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 101/4722]  eta: 3:24:58  lr: 0.000010  loss: 7.0428  loss_lm: 7.3607 (7.4208)  time: 2.6614  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 101/4722]  eta: 3:24:57  lr: 0.000010  loss: 7.0786  loss_lm: 7.5076 (7.4887)  time: 2.6613  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 101/4722]  eta: 3:24:57  lr: 0.000010  loss: 7.3197  loss_lm: 7.4775 (7.4813)  time: 2.6612  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 101/4722]  eta: 3:24:58  lr: 0.000010  loss: 7.4909  loss_lm: 7.4724 (7.4461)  time: 2.6614  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 102/4722]  eta: 3:19:33  lr: 0.000010  loss: 7.0712  loss_lm: 7.3825 (7.4111)  time: 2.5916  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 102/4722]  eta: 3:19:34  lr: 0.000010  loss: 7.3184  loss_lm: 7.4188 (7.4285)  time: 2.5919  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 102/4722]  eta: 3:19:32  lr: 0.000010  loss: 6.7635  loss_lm: 7.5076 (7.4460)  time: 2.5915  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 102/4722]  eta: 3:19:33  lr: 0.000010  loss: 7.6691  loss_lm: 7.4873 (7.4593)  time: 2.5916  data: 0.0000  max mem: 18951Train: data epoch: [0]  [ 102/4722]  eta: 3:19:32  lr: 0.000010  loss: 7.4485  loss_lm: 7.3683 (7.4224)  time: 2.5915  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 102/4722]  eta: 3:19:32  lr: 0.000010  loss: 7.2938  loss_lm: 7.4775 (7.4702)  time: 2.5914  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.2729  loss_lm: 7.3829 (7.4199)  time: 2.5301  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.0954  loss_lm: 7.3704 (7.3935)  time: 2.5298  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:43  lr: 0.000010  loss: 7.3414  loss_lm: 7.3782 (7.4631)  time: 2.5295  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.2760  loss_lm: 7.4724 (7.4491)  time: 2.5297  data: 0.0000  max mem: 18951Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.4557  loss_lm: 7.3683 (7.4243)  time: 2.5297  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 103/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.1322  loss_lm: 7.4245 (7.4286)  time: 2.5296  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:26  lr: 0.000010  loss: 7.4886  loss_lm: 7.4188 (7.4235)  time: 2.4744  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:25  lr: 0.000010  loss: 7.1629  loss_lm: 7.3704 (7.3814)  time: 2.4742  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:24  lr: 0.000010  loss: 7.3159  loss_lm: 7.4245 (7.4226)  time: 2.4740  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:25  lr: 0.000010  loss: 7.6785  loss_lm: 7.4873 (7.4612)  time: 2.4741  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:24  lr: 0.000010  loss: 7.2776  loss_lm: 7.3782 (7.4533)  time: 2.4740  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 104/4722]  eta: 3:10:25  lr: 0.000010  loss: 7.3884  loss_lm: 7.3845 (7.4224)  time: 2.4741  data: 0.0000  max mem: 18864
2023-11-13 00:04:36,103 [INFO] Saving checkpoint at iters: 105 and epoch: 0
2023-11-13 00:04:36,108 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:04:41,391 [INFO] Saved successfully
2023-11-13 00:04:41,392 [INFO] Averaged stats: lr: 0.0000  loss: 7.4036  loss_lm: 7.4036
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:54  lr: 0.000010  loss: 7.0262  loss_lm: 7.3829 (7.4036)  time: 2.6888  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:51  lr: 0.000010  loss: 7.2324  loss_lm: 7.3746 (7.4423)  time: 2.6883  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 7.2930  loss_lm: 7.3617 (7.3770)  time: 2.6885  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 6.9167  loss_lm: 7.3683 (7.3971)  time: 2.6884  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 7.4105  loss_lm: 7.4105 (7.4220)  time: 2.6883  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 105/4722]  eta: 3:26:52  lr: 0.000010  loss: 7.2033  loss_lm: 7.4724 (7.4483)  time: 2.6884  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 106/4722]  eta: 3:22:38  lr: 0.000010  loss: 7.0544  loss_lm: 7.3858 (7.4295)  time: 2.5660  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 106/4722]  eta: 3:22:39  lr: 0.000010  loss: 7.2077  loss_lm: 7.3515 (7.3689)  time: 2.5660  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 106/4722]  eta: 3:22:38  lr: 0.000010  loss: 7.0589  loss_lm: 7.3581 (7.4240)  time: 2.5659  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [ 106/4722]  eta: 3:22:38  lr: 0.000010  loss: 7.0579  loss_lm: 7.3910 (7.4047)  time: 2.5660  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 106/4722]  eta: 3:22:38  lr: 0.000010  loss: 6.9199  loss_lm: 7.3607 (7.3744)  time: 2.5660  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 106/4722]  eta: 3:22:40  lr: 0.000010  loss: 7.6018  loss_lm: 7.3829 (7.4131)  time: 2.5661  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:50  lr: 0.000010  loss: 6.9200  loss_lm: 7.3421 (7.3906)  time: 2.5692  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:48  lr: 0.000010  loss: 7.2956  loss_lm: 7.3903 (7.3997)  time: 2.5691  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:48  lr: 0.000010  loss: 7.4685  loss_lm: 7.4685 (7.4313)  time: 2.5690  data: 0.0000  max mem: 18951Train: data epoch: [0]  [ 107/4722]  eta: 3:18:48  lr: 0.000010  loss: 7.2217  loss_lm: 7.3015 (7.3622)  time: 2.5690  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 107/4722]  eta: 3:18:48  lr: 0.000010  loss: 6.9944  loss_lm: 7.3531 (7.3571)  time: 2.5691  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 107/4722]  eta: 3:18:47  lr: 0.000010  loss: 7.4501  loss_lm: 7.3581 (7.4252)  time: 2.5690  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 108/4722]  eta: 3:15:13  lr: 0.000010  loss: 7.2040  loss_lm: 7.3261 (7.3825)  time: 2.5714  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 108/4722]  eta: 3:15:11  lr: 0.000010  loss: 7.0902  loss_lm: 7.3159 (7.3863)  time: 2.5712  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 108/4722]  eta: 3:15:12  lr: 0.000010  loss: 7.6095  loss_lm: 7.4724 (7.4390)  time: 2.5712  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 108/4722]  eta: 3:15:12  lr: 0.000010  loss: 7.6469  loss_lm: 7.3515 (7.3746)  time: 2.5712  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 108/4722]  eta: 3:15:11  lr: 0.000010  loss: 7.2846  loss_lm: 7.3074 (7.3540)  time: 2.5712  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 108/4722]  eta: 3:15:11  lr: 0.000010  loss: 6.8327  loss_lm: 7.3414 (7.3994)  time: 2.5712  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 109/4722]  eta: 3:11:56  lr: 0.000010  loss: 7.0769  loss_lm: 7.3184 (7.3698)  time: 2.5740  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 109/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.7706  loss_lm: 7.4873 (7.4528)  time: 2.5738  data: 0.0000  max mem: 18951Train: data epoch: [0]  [ 109/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.0880  loss_lm: 7.3015 (7.3627)  time: 2.5738  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 109/4722]  eta: 3:11:54  lr: 0.000010  loss: 7.5017  loss_lm: 7.3159 (7.3911)  time: 2.5739  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 109/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.0642  loss_lm: 7.3074 (7.3419)  time: 2.5738  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 109/4722]  eta: 3:11:54  lr: 0.000010  loss: 7.1540  loss_lm: 7.3197 (7.3892)  time: 2.5738  data: 0.0000  max mem: 19014
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:10:18.663940] Starting job[2023-11-13 00:10:18.663946] Starting job[2023-11-13 00:10:18.663943] Starting job[2023-11-13 00:10:18.663944] Starting job[2023-11-13 00:10:18.663944] Starting job




[2023-11-13 00:10:18.663949] Starting job
| distributed init (rank 5, world 12): env://
| distributed init (rank 4, world 12): env://
| distributed init (rank 0, world 12): env://
| distributed init (rank 2, world 12): env://
| distributed init (rank 3, world 12): env://
| distributed init (rank 1, world 12): env://
2023-11-13 00:10:22,137 [INFO] 
=====  Running Parameters    =====
2023-11-13 00:10:22,137 [INFO] {
    "amp": true,
    "batch_size_eval": 10,
    "batch_size_train": 10,
    "checkpoint_freq": 5,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "eval_freq": 2,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 1e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 20,
    "min_len": 5,
    "min_lr": 0,
    "num_beams": 3,
    "num_workers": 3,
    "output_dir": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint",
    "rank": 0,
    "resume_ckpt_path": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth",
    "runner": "runner_base",
    "seed": 42,
    "task": "captioning",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "weight_decay": 0.05,
    "world_size": 12
}
2023-11-13 00:10:22,137 [INFO] 
======  Dataset Attributes  ======
2023-11-13 00:10:22,137 [INFO] 
======== coco_caption =======
2023-11-13 00:10:22,138 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/mnt/.node1/Open-Datasets/coco"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        },
        "train": {
            "name": "blip_caption",
            "prompt": "a picture of "
        }
    },
    "vis_processor": {
        "eval": {
            "name": "blip_image_eval"
        },
        "train": {
            "name": "blip_image_train"
        }
    }
}
2023-11-13 00:10:22,138 [INFO] 
======  Model Attributes  ======
2023-11-13 00:10:22,138 [INFO] {
    "arch": "blip_caption",
    "finetuned": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth",
    "image_size": 384,
    "load_finetuned": false,
    "load_pretrained": false,
    "med_config_path": "configs/models/med_large_config.json",
    "model_type": "large_coco",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth",
    "prompt": "a picture of ",
    "vit_ckpt_layer": 5,
    "vit_grad_ckpt": true,
    "vit_type": "large"
}
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-11-13 00:10:22,140 [INFO] Building datasets...
2023-11-13 00:10:37,152 [INFO] number of trainable parameters: 446290492
2023-11-13 00:10:37,449 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-11-13 00:10:37,450 [INFO] Loaded 566747 records for train split from the dataset.
2023-11-13 00:10:37,450 [INFO] Loaded 5000 records for val split from the dataset.
2023-11-13 00:10:37,450 [INFO] Loaded 5000 records for test split from the dataset.
2023-11-13 00:10:37,450 [INFO] Resume checkpoint from /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth
2023-11-13 00:10:37,453 [INFO] Start training
2023-11-13 00:10:37,463 [INFO] Start training epoch 0, 4722 iters per inner epoch.
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 106/4722]  eta: 4:56:03  lr: 0.000010  loss: 7.5953  loss_lm: 7.5953 (7.5953)  time: 3.8482  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 106/4722]  eta: 4:55:39  lr: 0.000010  loss: 7.0614  loss_lm: 7.0614 (7.0614)  time: 3.8431  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 106/4722]  eta: 4:55:35  lr: 0.000010  loss: 6.8952  loss_lm: 6.8952 (6.8952)  time: 3.8421  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 106/4722]  eta: 4:55:48  lr: 0.000010  loss: 7.2057  loss_lm: 7.2057 (7.2057)  time: 3.8449  data: 0.0000  max mem: 18863

Train: data epoch: [0]  [ 106/4722]  eta: 4:55:34  lr: 0.000010  loss: 7.0930  loss_lm: 7.0930 (7.0930)  time: 3.8420  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 106/4722]  eta: 4:55:36  lr: 0.000010  loss: 7.0770  loss_lm: 7.0770 (7.0770)  time: 3.8423  data: 0.0000  max mem: 18865

2023-11-13 00:10:41,320 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:30  lr: 0.000010  loss: 7.4733  loss_lm: 7.0614 (7.2674)  time: 2.6589  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:42  lr: 0.000010  loss: 6.9130  loss_lm: 6.9130 (7.2541)  time: 2.6614  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:28  lr: 0.000010  loss: 7.3137  loss_lm: 7.0770 (7.1953)  time: 2.6584  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 107/4722]  eta: 3:24:34  lr: 0.000010  loss: 7.2199  loss_lm: 7.2057 (7.2128)  time: 2.6597  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 107/4722]  eta: 3:24:28  lr: 0.000010  loss: 6.9716  loss_lm: 6.8952 (6.9334)  time: 2.6583  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 107/4722]  eta: 3:24:28  lr: 0.000010  loss: 7.4189  loss_lm: 7.0930 (7.2559)  time: 2.6583  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 108/4722]  eta: 2:54:42  lr: 0.000010  loss: 7.2147  loss_lm: 7.2147 (7.2410)  time: 2.2718  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 108/4722]  eta: 2:54:34  lr: 0.000010  loss: 7.6003  loss_lm: 7.4733 (7.3783)  time: 2.2702  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 108/4722]  eta: 2:54:33  lr: 0.000010  loss: 7.0929  loss_lm: 7.0929 (7.1612)  time: 2.2699  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 108/4722]  eta: 2:54:37  lr: 0.000010  loss: 7.6720  loss_lm: 7.2199 (7.3659)  time: 2.2707  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 108/4722]  eta: 2:54:32  lr: 0.000010  loss: 6.8475  loss_lm: 7.0930 (7.1198)  time: 2.2698  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 108/4722]  eta: 2:54:32  lr: 0.000010  loss: 7.2918  loss_lm: 6.9716 (7.0529)  time: 2.2698  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:11  lr: 0.000010  loss: 7.7767  loss_lm: 7.4733 (7.4779)  time: 2.0705  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 109/4722]  eta: 2:39:17  lr: 0.000010  loss: 7.0699  loss_lm: 7.0699 (7.1982)  time: 2.0718  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 109/4722]  eta: 2:39:10  lr: 0.000010  loss: 7.5106  loss_lm: 7.0929 (7.2485)  time: 2.0703  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:13  lr: 0.000010  loss: 7.1251  loss_lm: 7.2057 (7.3057)  time: 2.0709  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:09  lr: 0.000010  loss: 7.0721  loss_lm: 6.9716 (7.0577)  time: 2.0702  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 109/4722]  eta: 2:39:09  lr: 0.000010  loss: 7.1447  loss_lm: 7.0930 (7.1260)  time: 2.0702  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:10:47,219 [INFO] Saving checkpoint at iters: 110 and epoch: 0
2023-11-13 00:10:47,224 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:10:52,583 [INFO] Saved successfully
2023-11-13 00:10:52,584 [INFO] Averaged stats: lr: 0.0000  loss: 7.2423  loss_lm: 7.2423
Train: data epoch: [0]  [ 110/4722]  eta: 3:52:25  lr: 0.000010  loss: 7.4188  loss_lm: 7.2147 (7.2423)  time: 3.0238  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 110/4722]  eta: 3:52:21  lr: 0.000010  loss: 7.1988  loss_lm: 7.4733 (7.4221)  time: 3.0228  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 110/4722]  eta: 3:52:22  lr: 0.000010  loss: 7.5597  loss_lm: 7.2199 (7.3565)  time: 3.0231  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 110/4722]  eta: 3:52:19  lr: 0.000010  loss: 7.3014  loss_lm: 7.0721 (7.1064)  time: 3.0225  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 110/4722]  eta: 3:52:20  lr: 0.000010  loss: 7.6790  loss_lm: 7.3137 (7.3346)  time: 3.0226  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 110/4722]  eta: 3:52:19  lr: 0.000010  loss: 7.3392  loss_lm: 7.1447 (7.1686)  time: 3.0225  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 111/4722]  eta: 3:32:34  lr: 0.000010  loss: 7.2865  loss_lm: 7.1447 (7.1883)  time: 2.7662  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 111/4722]  eta: 3:32:35  lr: 0.000010  loss: 7.4555  loss_lm: 7.3137 (7.3548)  time: 2.7662  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 111/4722]  eta: 3:32:40  lr: 0.000010  loss: 7.0452  loss_lm: 7.0699 (7.2095)  time: 2.7674  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 111/4722]  eta: 3:32:36  lr: 0.000010  loss: 7.6421  loss_lm: 7.4733 (7.4588)  time: 2.7664  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 111/4722]  eta: 3:32:37  lr: 0.000010  loss: 7.0904  loss_lm: 7.2057 (7.3121)  time: 2.7667  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 111/4722]  eta: 3:32:35  lr: 0.000010  loss: 7.3265  loss_lm: 7.0721 (7.1431)  time: 2.7662  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:25  lr: 0.000010  loss: 7.2032  loss_lm: 7.2032 (7.1517)  time: 2.5825  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:26  lr: 0.000010  loss: 7.1576  loss_lm: 7.4733 (7.4157)  time: 2.5827  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:27  lr: 0.000010  loss: 7.7249  loss_lm: 7.2199 (7.3711)  time: 2.5829  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:29  lr: 0.000010  loss: 7.4826  loss_lm: 7.2147 (7.2485)  time: 2.5835  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 112/4722]  eta: 3:18:25  lr: 0.000010  loss: 7.3826  loss_lm: 7.2865 (7.2160)  time: 2.5825  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 112/4722]  eta: 3:18:25  lr: 0.000010  loss: 7.2390  loss_lm: 7.3137 (7.3382)  time: 2.5825  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 113/4722]  eta: 3:07:43  lr: 0.000010  loss: 7.2478  loss_lm: 7.2478 (7.3947)  time: 2.4438  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 113/4722]  eta: 3:07:42  lr: 0.000010  loss: 7.3180  loss_lm: 7.2032 (7.1725)  time: 2.4436  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 113/4722]  eta: 3:07:44  lr: 0.000010  loss: 7.2495  loss_lm: 7.2199 (7.3559)  time: 2.4439  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 113/4722]  eta: 3:07:42  lr: 0.000010  loss: 7.9727  loss_lm: 7.3137 (7.4175)  time: 2.4436  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 113/4722]  eta: 3:07:46  lr: 0.000010  loss: 7.6055  loss_lm: 7.2147 (7.2931)  time: 2.4445  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 113/4722]  eta: 3:07:42  lr: 0.000010  loss: 7.4034  loss_lm: 7.2865 (7.2395)  time: 2.4436  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 114/4722]  eta: 2:59:22  lr: 0.000010  loss: 6.9046  loss_lm: 7.3137 (7.3605)  time: 2.3355  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 114/4722]  eta: 2:59:22  lr: 0.000010  loss: 7.2116  loss_lm: 7.2116 (7.1768)  time: 2.3356  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 114/4722]  eta: 2:59:23  lr: 0.000010  loss: 7.5831  loss_lm: 7.2495 (7.3811)  time: 2.3358  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 114/4722]  eta: 2:59:25  lr: 0.000010  loss: 7.2129  loss_lm: 7.2147 (7.2842)  time: 2.3363  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 114/4722]  eta: 2:59:21  lr: 0.000010  loss: 7.1470  loss_lm: 7.2865 (7.2292)  time: 2.3355  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 114/4722]  eta: 2:59:22  lr: 0.000010  loss: 7.2846  loss_lm: 7.2846 (7.3825)  time: 2.3357  data: 0.0000  max mem: 18865
2023-11-13 00:10:59,970 [INFO] Saving checkpoint at iters: 115 and epoch: 0
2023-11-13 00:10:59,975 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:11:05,226 [INFO] Saved successfully
2023-11-13 00:11:05,227 [INFO] Averaged stats: lr: 0.0000  loss: 7.3060  loss_lm: 7.3060
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:08  lr: 0.000010  loss: 7.5018  loss_lm: 7.2147 (7.3060)  time: 2.7759  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:06  lr: 0.000010  loss: 7.2032  loss_lm: 7.2199 (7.3634)  time: 2.7755  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:05  lr: 0.000010  loss: 8.0120  loss_lm: 7.2865 (7.3075)  time: 2.7752  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:05  lr: 0.000010  loss: 6.9651  loss_lm: 7.2390 (7.3210)  time: 2.7753  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:06  lr: 0.000010  loss: 7.3829  loss_lm: 7.2846 (7.3826)  time: 2.7754  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 115/4722]  eta: 3:33:05  lr: 0.000010  loss: 6.9727  loss_lm: 7.2032 (7.1564)  time: 2.7753  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 116/4722]  eta: 3:24:00  lr: 0.000010  loss: 7.2622  loss_lm: 7.2495 (7.3542)  time: 2.6576  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 116/4722]  eta: 3:24:00  lr: 0.000010  loss: 7.2696  loss_lm: 7.2846 (7.3723)  time: 2.6574  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 116/4722]  eta: 3:23:59  lr: 0.000010  loss: 7.4411  loss_lm: 7.2116 (7.1823)  time: 2.6573  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 116/4722]  eta: 3:23:59  lr: 0.000010  loss: 7.6027  loss_lm: 7.3392 (7.3343)  time: 2.6573  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 116/4722]  eta: 3:23:59  lr: 0.000010  loss: 7.2470  loss_lm: 7.2470 (7.3143)  time: 2.6573  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 116/4722]  eta: 3:24:02  lr: 0.000010  loss: 6.9834  loss_lm: 7.2147 (7.2767)  time: 2.6580  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:25  lr: 0.000010  loss: 7.0400  loss_lm: 7.2696 (7.3446)  time: 2.5592  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:24  lr: 0.000010  loss: 7.3231  loss_lm: 7.2116 (7.1940)  time: 2.5591  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:25  lr: 0.000010  loss: 7.3184  loss_lm: 7.2495 (7.3512)  time: 2.5593  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:24  lr: 0.000010  loss: 7.0300  loss_lm: 7.2865 (7.3090)  time: 2.5591  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:24  lr: 0.000010  loss: 7.0937  loss_lm: 7.2390 (7.2959)  time: 2.5591  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 117/4722]  eta: 3:16:27  lr: 0.000010  loss: 6.9527  loss_lm: 7.2129 (7.2497)  time: 2.5597  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:58  lr: 0.000010  loss: 7.1380  loss_lm: 7.2696 (7.3287)  time: 2.4758  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:58  lr: 0.000010  loss: 7.7398  loss_lm: 7.2918 (7.2360)  time: 2.4757  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:58  lr: 0.000010  loss: 7.3134  loss_lm: 7.2470 (7.2972)  time: 2.4758  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:59  lr: 0.000010  loss: 7.4919  loss_lm: 7.2622 (7.3620)  time: 2.4760  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 118/4722]  eta: 3:09:58  lr: 0.000010  loss: 6.9454  loss_lm: 7.2865 (7.2810)  time: 2.4757  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 118/4722]  eta: 3:10:00  lr: 0.000010  loss: 7.2005  loss_lm: 7.2129 (7.2459)  time: 2.4763  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:28  lr: 0.000010  loss: 6.9734  loss_lm: 7.2116 (7.2173)  time: 2.4045  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:29  lr: 0.000010  loss: 6.9624  loss_lm: 7.2495 (7.3335)  time: 2.4047  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:28  lr: 0.000010  loss: 6.8146  loss_lm: 7.2478 (7.2920)  time: 2.4046  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:28  lr: 0.000010  loss: 7.3217  loss_lm: 7.2470 (7.2990)  time: 2.4045  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:27  lr: 0.000010  loss: 7.2304  loss_lm: 7.2304 (7.2774)  time: 2.4045  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 119/4722]  eta: 3:04:30  lr: 0.000010  loss: 6.7962  loss_lm: 7.2005 (7.2138)  time: 2.4050  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:11:12,613 [INFO] Saving checkpoint at iters: 120 and epoch: 0
2023-11-13 00:11:12,618 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:11:18,048 [INFO] Saved successfully
2023-11-13 00:11:18,049 [INFO] Averaged stats: lr: 0.0000  loss: 7.2078  loss_lm: 7.2078
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:29  lr: 0.000010  loss: 7.1236  loss_lm: 7.2005 (7.2078)  time: 2.7051  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:27  lr: 0.000010  loss: 7.3831  loss_lm: 7.2696 (7.2981)  time: 2.7048  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:26  lr: 0.000010  loss: 7.3284  loss_lm: 7.3134 (7.3009)  time: 2.7047  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:26  lr: 0.000010  loss: 7.4645  loss_lm: 7.2865 (7.2898)  time: 2.7046  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:27  lr: 0.000010  loss: 6.8547  loss_lm: 7.2495 (7.3015)  time: 2.7049  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 120/4722]  eta: 3:27:26  lr: 0.000010  loss: 7.2062  loss_lm: 7.2116 (7.2165)  time: 2.7047  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 121/4722]  eta: 3:21:29  lr: 0.000010  loss: 6.9222  loss_lm: 7.2062 (7.1981)  time: 2.6277  data: 0.0000  max mem: 19014Train: data epoch: [0]  [ 121/4722]  eta: 3:21:30  lr: 0.000010  loss: 7.3318  loss_lm: 7.2495 (7.3034)  time: 2.6278  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 121/4722]  eta: 3:21:29  lr: 0.000010  loss: 7.0306  loss_lm: 7.2470 (7.2840)  time: 2.6277  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 121/4722]  eta: 3:21:30  lr: 0.000010  loss: 7.3388  loss_lm: 7.2696 (7.3006)  time: 2.6278  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 121/4722]  eta: 3:21:32  lr: 0.000010  loss: 7.1137  loss_lm: 7.1236 (7.2019)  time: 2.6281  data: 0.0000  max mem: 18866


Train: data epoch: [0]  [ 121/4722]  eta: 3:21:29  lr: 0.000010  loss: 7.3218  loss_lm: 7.2865 (7.2918)  time: 2.6276  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 122/4722]  eta: 3:16:14  lr: 0.000010  loss: 7.1127  loss_lm: 7.2495 (7.2922)  time: 2.5597  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 122/4722]  eta: 3:16:13  lr: 0.000010  loss: 7.2571  loss_lm: 7.2116 (7.2016)  time: 2.5595  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [ 122/4722]  eta: 3:16:15  lr: 0.000010  loss: 7.3323  loss_lm: 7.2005 (7.2095)  time: 2.5599  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 122/4722]  eta: 3:16:13  lr: 0.000010  loss: 7.1690  loss_lm: 7.2865 (7.2846)  time: 2.5595  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 122/4722]  eta: 3:16:13  lr: 0.000010  loss: 7.1156  loss_lm: 7.2470 (7.2741)  time: 2.5595  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 122/4722]  eta: 3:16:13  lr: 0.000010  loss: 7.4102  loss_lm: 7.2846 (7.3070)  time: 2.5596  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 123/4722]  eta: 3:11:31  lr: 0.000010  loss: 7.1812  loss_lm: 7.2062 (7.2005)  time: 2.4988  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 123/4722]  eta: 3:11:32  lr: 0.000010  loss: 7.4316  loss_lm: 7.2495 (7.3000)  time: 2.4989  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 123/4722]  eta: 3:11:33  lr: 0.000010  loss: 7.1378  loss_lm: 7.1378 (7.2056)  time: 2.4992  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 123/4722]  eta: 3:11:31  lr: 0.000010  loss: 7.0842  loss_lm: 7.2390 (7.2636)  time: 2.4988  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 123/4722]  eta: 3:11:32  lr: 0.000010  loss: 7.1220  loss_lm: 7.2696 (7.2968)  time: 2.4988  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 123/4722]  eta: 3:11:31  lr: 0.000010  loss: 7.1650  loss_lm: 7.2304 (7.2780)  time: 2.4987  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:20  lr: 0.000010  loss: 7.0107  loss_lm: 7.2495 (7.2847)  time: 2.4447  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:19  lr: 0.000010  loss: 7.1370  loss_lm: 7.2062 (7.1971)  time: 2.4445  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:19  lr: 0.000010  loss: 7.1065  loss_lm: 7.2390 (7.2553)  time: 2.4445  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:21  lr: 0.000010  loss: 7.1983  loss_lm: 7.1983 (7.2052)  time: 2.4449  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:20  lr: 0.000010  loss: 7.3421  loss_lm: 7.2846 (7.2991)  time: 2.4446  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 124/4722]  eta: 3:07:19  lr: 0.000010  loss: 7.6712  loss_lm: 7.2865 (7.2987)  time: 2.4445  data: 0.0000  max mem: 18866
2023-11-13 00:11:25,396 [INFO] Saving checkpoint at iters: 125 and epoch: 0
2023-11-13 00:11:25,401 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:11:30,492 [INFO] Saved successfully
2023-11-13 00:11:30,493 [INFO] Averaged stats: lr: 0.0000  loss: 7.1950  loss_lm: 7.1950
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:06  lr: 0.000010  loss: 7.0019  loss_lm: 7.1378 (7.1950)  time: 2.6509  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:04  lr: 0.000010  loss: 7.2132  loss_lm: 7.2132 (7.2532)  time: 2.6505  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:04  lr: 0.000010  loss: 6.9962  loss_lm: 7.2304 (7.2836)  time: 2.6505  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:05  lr: 0.000010  loss: 7.2378  loss_lm: 7.2378 (7.2824)  time: 2.6507  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:04  lr: 0.000010  loss: 7.6097  loss_lm: 7.2846 (7.3147)  time: 2.6506  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 125/4722]  eta: 3:23:04  lr: 0.000010  loss: 6.9746  loss_lm: 7.2032 (7.1860)  time: 2.6506  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 126/4722]  eta: 3:18:43  lr: 0.000010  loss: 7.4643  loss_lm: 7.3388 (7.3218)  time: 2.5319  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 126/4722]  eta: 3:18:44  lr: 0.000010  loss: 7.0907  loss_lm: 7.2378 (7.2733)  time: 2.5319  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 126/4722]  eta: 3:18:43  lr: 0.000010  loss: 7.2246  loss_lm: 7.2062 (7.1878)  time: 2.5319  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 126/4722]  eta: 3:18:43  lr: 0.000010  loss: 7.3377  loss_lm: 7.2390 (7.2572)  time: 2.5319  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 126/4722]  eta: 3:18:43  lr: 0.000010  loss: 7.0312  loss_lm: 7.2304 (7.2715)  time: 2.5319  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 126/4722]  eta: 3:18:45  lr: 0.000010  loss: 7.2084  loss_lm: 7.1378 (7.1957)  time: 2.5320  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 127/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.1044  loss_lm: 7.2062 (7.1840)  time: 2.5318  data: 0.0000  max mem: 19014Train: data epoch: [0]  [ 127/4722]  eta: 3:14:46  lr: 0.000010  loss: 6.7845  loss_lm: 7.2846 (7.2974)  time: 2.5319  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 127/4722]  eta: 3:14:47  lr: 0.000010  loss: 7.0899  loss_lm: 7.2378 (7.2649)  time: 2.5319  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 127/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.1587  loss_lm: 7.2132 (7.2528)  time: 2.5318  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 127/4722]  eta: 3:14:46  lr: 0.000010  loss: 7.0533  loss_lm: 7.1690 (7.2616)  time: 2.5318  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 127/4722]  eta: 3:14:48  lr: 0.000010  loss: 7.1022  loss_lm: 7.1378 (7.1914)  time: 2.5320  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:11  lr: 0.000010  loss: 7.4503  loss_lm: 7.2846 (7.3040)  time: 2.5310  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 128/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.7157  loss_lm: 7.2062 (7.2071)  time: 2.5310  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [ 128/4722]  eta: 3:11:12  lr: 0.000010  loss: 6.7875  loss_lm: 7.1236 (7.1738)  time: 2.5311  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:10  lr: 0.000010  loss: 6.8646  loss_lm: 7.2132 (7.2359)  time: 2.5310  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:11  lr: 0.000010  loss: 7.6433  loss_lm: 7.2378 (7.2814)  time: 2.5311  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 128/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.1943  loss_lm: 7.1943 (7.2587)  time: 2.5310  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 129/4722]  eta: 3:07:51  lr: 0.000010  loss: 7.0681  loss_lm: 7.2696 (7.2942)  time: 2.5308  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 129/4722]  eta: 3:07:51  lr: 0.000010  loss: 6.5835  loss_lm: 7.2062 (7.1812)  time: 2.5308  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [ 129/4722]  eta: 3:07:52  lr: 0.000010  loss: 7.0992  loss_lm: 7.2378 (7.2738)  time: 2.5309  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 129/4722]  eta: 3:07:53  lr: 0.000010  loss: 7.3408  loss_lm: 7.1378 (7.1808)  time: 2.5309  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 129/4722]  eta: 3:07:51  lr: 0.000010  loss: 7.2371  loss_lm: 7.2132 (7.2359)  time: 2.5308  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 129/4722]  eta: 3:07:51  lr: 0.000010  loss: 6.7447  loss_lm: 7.1943 (7.2373)  time: 2.5308  data: 0.0000  max mem: 18866
2023-11-13 00:11:37,856 [INFO] Saving checkpoint at iters: 130 and epoch: 0
2023-11-13 00:11:37,861 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:15:04.445195] Starting job[2023-11-13 00:15:04.445201] Starting job
[2023-11-13 00:15:04.445198] Starting job
[2023-11-13 00:15:04.445204] Starting job

[2023-11-13 00:15:04.446542] Starting job[2023-11-13 00:15:04.446541] Starting job

| distributed init (rank 4, world 12): env://
| distributed init (rank 1, world 12): env://
| distributed init (rank 5, world 12): env://
| distributed init (rank 0, world 12): env://
| distributed init (rank 2, world 12): env://
| distributed init (rank 3, world 12): env://
2023-11-13 00:15:11,359 [INFO] 
=====  Running Parameters    =====
2023-11-13 00:15:11,360 [INFO] {
    "amp": true,
    "batch_size_eval": 10,
    "batch_size_train": 10,
    "checkpoint_freq": 5,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "eval_freq": 2,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 1e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 20,
    "min_len": 5,
    "min_lr": 0,
    "num_beams": 3,
    "num_workers": 3,
    "output_dir": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint",
    "rank": 0,
    "resume_ckpt_path": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth",
    "runner": "runner_base",
    "seed": 42,
    "task": "captioning",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "weight_decay": 0.05,
    "world_size": 12
}
2023-11-13 00:15:11,360 [INFO] 
======  Dataset Attributes  ======
2023-11-13 00:15:11,360 [INFO] 
======== coco_caption =======
2023-11-13 00:15:11,360 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/mnt/.node1/Open-Datasets/coco"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        },
        "train": {
            "name": "blip_caption",
            "prompt": "a picture of "
        }
    },
    "vis_processor": {
        "eval": {
            "name": "blip_image_eval"
        },
        "train": {
            "name": "blip_image_train"
        }
    }
}
2023-11-13 00:15:11,360 [INFO] 
======  Model Attributes  ======
2023-11-13 00:15:11,360 [INFO] {
    "arch": "blip_caption",
    "finetuned": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth",
    "image_size": 384,
    "load_finetuned": false,
    "load_pretrained": false,
    "med_config_path": "configs/models/med_large_config.json",
    "model_type": "large_coco",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth",
    "prompt": "a picture of ",
    "vit_ckpt_layer": 5,
    "vit_grad_ckpt": true,
    "vit_type": "large"
}
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-11-13 00:15:11,362 [INFO] Building datasets...
2023-11-13 00:15:26,848 [INFO] number of trainable parameters: 446290492
2023-11-13 00:15:27,145 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-11-13 00:15:27,145 [INFO] Loaded 566747 records for train split from the dataset.
2023-11-13 00:15:27,145 [INFO] Loaded 5000 records for val split from the dataset.
2023-11-13 00:15:27,145 [INFO] Loaded 5000 records for test split from the dataset.
2023-11-13 00:15:27,146 [INFO] Resume checkpoint from /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth
2023-11-13 00:15:27,148 [INFO] Start training
2023-11-13 00:15:27,158 [INFO] Start training epoch 0, 4722 iters per inner epoch.
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 126/4722]  eta: 5:19:12  lr: 0.000010  loss: 7.2147  loss_lm: 7.2147 (7.2147)  time: 4.1672  data: 0.0000  max mem: 18936Train: data epoch: [0]  [ 126/4722]  eta: 5:19:08  lr: 0.000010  loss: 7.4342  loss_lm: 7.4342 (7.4342)  time: 4.1663  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 126/4722]  eta: 5:19:15  lr: 0.000010  loss: 7.1045  loss_lm: 7.1045 (7.1045)  time: 4.1679  data: 0.0000  max mem: 18869
Train: data epoch: [0]  [ 126/4722]  eta: 5:19:06  lr: 0.000010  loss: 7.0437  loss_lm: 7.0437 (7.0437)  time: 4.1659  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 126/4722]  eta: 5:19:42  lr: 0.000010  loss: 7.2087  loss_lm: 7.2087 (7.2087)  time: 4.1737  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 126/4722]  eta: 5:19:08  lr: 0.000010  loss: 7.3465  loss_lm: 7.3465 (7.3465)  time: 4.1664  data: 0.0000  max mem: 18864


2023-11-13 00:15:31,339 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:26  lr: 0.000010  loss: 6.7796  loss_lm: 6.7796 (7.1069)  time: 2.8262  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:28  lr: 0.000010  loss: 7.0985  loss_lm: 7.0985 (7.1566)  time: 2.8266  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:29  lr: 0.000010  loss: 7.0719  loss_lm: 7.0719 (7.0882)  time: 2.8269  data: 0.0000  max mem: 18869
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:26  lr: 0.000010  loss: 7.1579  loss_lm: 7.1579 (7.2522)  time: 2.8262  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 127/4722]  eta: 3:36:43  lr: 0.000010  loss: 7.1148  loss_lm: 7.1148 (7.1617)  time: 2.8298  data: 0.0000  max mem: 18868Train: data epoch: [0]  [ 127/4722]  eta: 3:36:25  lr: 0.000010  loss: 7.0456  loss_lm: 7.0437 (7.0447)  time: 2.8260  data: 0.0000  max mem: 18864

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 128/4722]  eta: 3:01:56  lr: 0.000010  loss: 7.4758  loss_lm: 7.4342 (7.2298)  time: 2.3762  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 128/4722]  eta: 3:01:57  lr: 0.000010  loss: 7.7130  loss_lm: 7.2147 (7.3421)  time: 2.3765  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 128/4722]  eta: 3:01:58  lr: 0.000010  loss: 7.6368  loss_lm: 7.1045 (7.2711)  time: 2.3767  data: 0.0000  max mem: 18869Train: data epoch: [0]  [ 128/4722]  eta: 3:01:56  lr: 0.000010  loss: 6.8894  loss_lm: 7.1579 (7.1313)  time: 2.3762  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 128/4722]  eta: 3:02:07  lr: 0.000010  loss: 6.7656  loss_lm: 7.1148 (7.0297)  time: 2.3786  data: 0.0000  max mem: 18868
Train: data epoch: [0]  [ 128/4722]  eta: 3:01:55  lr: 0.000010  loss: 7.2058  loss_lm: 7.0456 (7.0984)  time: 2.3760  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:43  lr: 0.000010  loss: 7.0778  loss_lm: 7.0778 (7.2228)  time: 2.1518  data: 0.0000  max mem: 18869
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:41  lr: 0.000010  loss: 7.0646  loss_lm: 7.0646 (7.1885)  time: 2.1514  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:49  lr: 0.000010  loss: 7.3791  loss_lm: 7.1148 (7.1170)  time: 2.1532  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 129/4722]  eta: 2:44:40  lr: 0.000010  loss: 6.7438  loss_lm: 7.0437 (7.0098)  time: 2.1512  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 129/4722]  eta: 2:44:42  lr: 0.000010  loss: 6.6197  loss_lm: 7.0985 (7.1615)  time: 2.1516  data: 0.0000  max mem: 18936

Train: data epoch: [0]  [ 129/4722]  eta: 2:44:41  lr: 0.000010  loss: 7.2286  loss_lm: 7.1579 (7.1556)  time: 2.1514  data: 0.0000  max mem: 18864
2023-11-13 00:15:37,253 [INFO] Saving checkpoint at iters: 130 and epoch: 0
2023-11-13 00:15:37,258 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:15:42,804 [INFO] Saved successfully
2023-11-13 00:15:42,805 [INFO] Averaged stats: lr: 0.0000  loss: 7.1212  loss_lm: 7.1212
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:29  lr: 0.000010  loss: 7.1379  loss_lm: 7.1379 (7.1212)  time: 3.1293  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:22  lr: 0.000010  loss: 6.8938  loss_lm: 7.0437 (6.9866)  time: 3.1276  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:23  lr: 0.000010  loss: 6.8577  loss_lm: 7.0778 (7.1498)  time: 3.1280  data: 0.0000  max mem: 18869
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:23  lr: 0.000010  loss: 6.9896  loss_lm: 7.0985 (7.1271)  time: 3.1279  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:22  lr: 0.000010  loss: 7.6213  loss_lm: 7.4342 (7.2751)  time: 3.1278  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 130/4722]  eta: 3:59:22  lr: 0.000010  loss: 7.1309  loss_lm: 7.1579 (7.1507)  time: 3.1277  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:21  lr: 0.000010  loss: 6.9220  loss_lm: 7.0646 (7.2162)  time: 2.8538  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:22  lr: 0.000010  loss: 7.4162  loss_lm: 7.0778 (7.1942)  time: 2.8540  data: 0.0000  max mem: 18869
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:21  lr: 0.000010  loss: 6.8769  loss_lm: 6.8938 (6.9683)  time: 2.8537  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:22  lr: 0.000010  loss: 7.2423  loss_lm: 7.0985 (7.1463)  time: 2.8539  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:21  lr: 0.000010  loss: 7.0519  loss_lm: 7.1309 (7.1342)  time: 2.8537  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 131/4722]  eta: 3:38:27  lr: 0.000010  loss: 7.0763  loss_lm: 7.1148 (7.1137)  time: 2.8551  data: 0.0000  max mem: 19015
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 132/4722]  eta: 3:23:17  lr: 0.000010  loss: 7.2233  loss_lm: 7.1045 (7.1983)  time: 2.6573  data: 0.0000  max mem: 18869
Train: data epoch: [0]  [ 132/4722]  eta: 3:23:16  lr: 0.000010  loss: 7.2674  loss_lm: 7.2674 (7.2235)  time: 2.6571  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 132/4722]  eta: 3:23:21  lr: 0.000010  loss: 7.2398  loss_lm: 7.1379 (7.1317)  time: 2.6583  data: 0.0000  max mem: 19015Train: data epoch: [0]  [ 132/4722]  eta: 3:23:15  lr: 0.000010  loss: 6.8594  loss_lm: 6.8938 (6.9527)  time: 2.6571  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 132/4722]  eta: 3:23:16  lr: 0.000010  loss: 7.1686  loss_lm: 7.1579 (7.1391)  time: 2.6571  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 132/4722]  eta: 3:23:16  lr: 0.000010  loss: 6.9640  loss_lm: 7.0985 (7.1202)  time: 2.6573  data: 0.0000  max mem: 18936

Train: data epoch: [0]  [ 133/4722]  eta: 3:11:56  lr: 0.000010  loss: 7.0588  loss_lm: 7.0778 (7.1809)  time: 2.5096  data: 0.0000  max mem: 18869
Train: data epoch: [0]  [ 133/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.3208  loss_lm: 6.8938 (6.9987)  time: 2.5093  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 133/4722]  eta: 3:11:55  lr: 0.000010  loss: 7.2638  loss_lm: 7.2638 (7.2286)  time: 2.5094  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 133/4722]  eta: 3:11:55  lr: 0.000010  loss: 6.9303  loss_lm: 7.1309 (7.1130)  time: 2.5093  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 133/4722]  eta: 3:11:56  lr: 0.000010  loss: 6.8935  loss_lm: 6.9896 (7.0919)  time: 2.5095  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 133/4722]  eta: 3:12:00  lr: 0.000010  loss: 6.8921  loss_lm: 7.1148 (7.1018)  time: 2.5104  data: 0.0000  max mem: 19015
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 134/4722]  eta: 3:03:07  lr: 0.000010  loss: 6.8476  loss_lm: 7.2638 (7.1862)  time: 2.3948  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 134/4722]  eta: 3:03:08  lr: 0.000010  loss: 7.0054  loss_lm: 7.0778 (7.1614)  time: 2.3950  data: 0.0000  max mem: 18869

Train: data epoch: [0]  [ 134/4722]  eta: 3:03:07  lr: 0.000010  loss: 7.5321  loss_lm: 7.0985 (7.1408)  time: 2.3949  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 134/4722]  eta: 3:03:07  lr: 0.000010  loss: 7.0340  loss_lm: 7.1309 (7.1043)  time: 2.3948  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 134/4722]  eta: 3:03:07  lr: 0.000010  loss: 7.4523  loss_lm: 7.0437 (7.0491)  time: 2.3948  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 134/4722]  eta: 3:03:11  lr: 0.000010  loss: 6.9954  loss_lm: 7.1148 (7.0900)  time: 2.3957  data: 0.0000  max mem: 19015
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:15:50,220 [INFO] Saving checkpoint at iters: 135 and epoch: 0
2023-11-13 00:15:50,226 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:15:55,274 [INFO] Saved successfully
2023-11-13 00:15:55,275 [INFO] Averaged stats: lr: 0.0000  loss: 7.0738  loss_lm: 7.0738
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:54  lr: 0.000010  loss: 6.9285  loss_lm: 7.0763 (7.0738)  time: 2.8111  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:51  lr: 0.000010  loss: 7.3155  loss_lm: 7.0778 (7.1768)  time: 2.8104  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:50  lr: 0.000010  loss: 7.3788  loss_lm: 7.0437 (7.0821)  time: 2.8102  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:50  lr: 0.000010  loss: 7.0113  loss_lm: 7.0113 (7.1279)  time: 2.8103  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:50  lr: 0.000010  loss: 6.9814  loss_lm: 7.0519 (7.0920)  time: 2.8102  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 135/4722]  eta: 3:34:50  lr: 0.000010  loss: 6.6782  loss_lm: 7.0646 (7.1354)  time: 2.8103  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:34  lr: 0.000010  loss: 6.9694  loss_lm: 7.0778 (7.1579)  time: 2.6896  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:37  lr: 0.000010  loss: 7.2919  loss_lm: 7.1148 (7.0936)  time: 2.6903  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:33  lr: 0.000010  loss: 6.9385  loss_lm: 7.0437 (7.0690)  time: 2.6894  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:33  lr: 0.000010  loss: 7.4206  loss_lm: 7.2638 (7.1614)  time: 2.6895  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:33  lr: 0.000010  loss: 6.8458  loss_lm: 7.0519 (7.0696)  time: 2.6894  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 136/4722]  eta: 3:25:34  lr: 0.000010  loss: 6.9635  loss_lm: 7.0113 (7.1129)  time: 2.6895  data: 0.0000  max mem: 18936
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 137/4722]  eta: 3:17:47  lr: 0.000010  loss: 6.6278  loss_lm: 7.0719 (7.1138)  time: 2.5883  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 137/4722]  eta: 3:17:49  lr: 0.000010  loss: 6.8061  loss_lm: 7.0763 (7.0697)  time: 2.5889  data: 0.0000  max mem: 19015Train: data epoch: [0]  [ 137/4722]  eta: 3:17:46  lr: 0.000010  loss: 7.0249  loss_lm: 7.0249 (7.0654)  time: 2.5881  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 137/4722]  eta: 3:17:46  lr: 0.000010  loss: 7.2161  loss_lm: 7.0519 (7.0818)  time: 2.5881  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 137/4722]  eta: 3:17:46  lr: 0.000010  loss: 6.6036  loss_lm: 6.9896 (7.0705)  time: 2.5882  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 137/4722]  eta: 3:17:46  lr: 0.000010  loss: 7.2824  loss_lm: 7.2638 (7.1714)  time: 2.5881  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:11  lr: 0.000010  loss: 7.2111  loss_lm: 7.0778 (7.1213)  time: 2.5025  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:14  lr: 0.000010  loss: 6.7277  loss_lm: 7.0763 (7.0434)  time: 2.5031  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:11  lr: 0.000010  loss: 6.8605  loss_lm: 6.9896 (7.0543)  time: 2.5024  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:10  lr: 0.000010  loss: 6.7701  loss_lm: 7.2638 (7.1406)  time: 2.5024  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:10  lr: 0.000010  loss: 7.1540  loss_lm: 7.1309 (7.0873)  time: 2.5023  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 138/4722]  eta: 3:11:10  lr: 0.000010  loss: 6.9434  loss_lm: 7.0249 (7.0560)  time: 2.5024  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 139/4722]  eta: 3:05:33  lr: 0.000010  loss: 6.8865  loss_lm: 7.0719 (7.1045)  time: 2.4293  data: 0.0000  max mem: 18951Train: data epoch: [0]  [ 139/4722]  eta: 3:05:35  lr: 0.000010  loss: 7.2143  loss_lm: 7.0763 (7.0556)  time: 2.4298  data: 0.0000  max mem: 19015

Train: data epoch: [0]  [ 139/4722]  eta: 3:05:33  lr: 0.000010  loss: 7.1555  loss_lm: 6.9896 (7.0615)  time: 2.4292  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 139/4722]  eta: 3:05:32  lr: 0.000010  loss: 7.4240  loss_lm: 7.1309 (7.1114)  time: 2.4292  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 139/4722]  eta: 3:05:32  lr: 0.000010  loss: 7.1045  loss_lm: 7.1045 (7.1380)  time: 2.4292  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 139/4722]  eta: 3:05:32  lr: 0.000010  loss: 7.2112  loss_lm: 7.0249 (7.0671)  time: 2.4292  data: 0.0000  max mem: 18864
2023-11-13 00:16:02,668 [INFO] Saving checkpoint at iters: 140 and epoch: 0
2023-11-13 00:16:02,673 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:16:08,674 [INFO] Saved successfully
2023-11-13 00:16:08,675 [INFO] Averaged stats: lr: 0.0000  loss: 7.0441  loss_lm: 7.0441
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:19  lr: 0.000010  loss: 6.8837  loss_lm: 7.0763 (7.0441)  time: 2.7672  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:16  lr: 0.000010  loss: 6.9501  loss_lm: 7.0249 (7.0593)  time: 2.7666  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:16  lr: 0.000010  loss: 7.3693  loss_lm: 7.2638 (7.1534)  time: 2.7666  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:17  lr: 0.000010  loss: 7.2837  loss_lm: 7.0778 (7.1164)  time: 2.7668  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 140/4722]  eta: 3:31:16  lr: 0.000010  loss: 7.1417  loss_lm: 7.0113 (7.0669)  time: 2.7667  data: 0.0000  max mem: 18936Train: data epoch: [0]  [ 140/4722]  eta: 3:31:16  lr: 0.000010  loss: 6.7930  loss_lm: 7.1309 (7.0902)  time: 2.7666  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 141/4722]  eta: 3:25:03  lr: 0.000010  loss: 7.2635  loss_lm: 7.0113 (7.0792)  time: 2.6859  data: 0.0000  max mem: 18936Train: data epoch: [0]  [ 141/4722]  eta: 3:25:04  lr: 0.000010  loss: 7.4739  loss_lm: 7.0778 (7.1388)  time: 2.6859  data: 0.0000  max mem: 18951

Train: data epoch: [0]  [ 141/4722]  eta: 3:25:06  lr: 0.000010  loss: 7.0340  loss_lm: 7.0340 (7.0435)  time: 2.6864  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 141/4722]  eta: 3:25:03  lr: 0.000010  loss: 7.1061  loss_lm: 7.0249 (7.0622)  time: 2.6858  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 141/4722]  eta: 3:25:03  lr: 0.000010  loss: 6.9476  loss_lm: 7.1045 (7.1405)  time: 2.6858  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 141/4722]  eta: 3:25:03  lr: 0.000010  loss: 7.1371  loss_lm: 7.1309 (7.0931)  time: 2.6858  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 142/4722]  eta: 3:19:37  lr: 0.000010  loss: 7.1459  loss_lm: 7.0763 (7.0495)  time: 2.6152  data: 0.0000  max mem: 19015Train: data epoch: [0]  [ 142/4722]  eta: 3:19:35  lr: 0.000010  loss: 6.7961  loss_lm: 7.0778 (7.1186)  time: 2.6148  data: 0.0000  max mem: 18951

Train: data epoch: [0]  [ 142/4722]  eta: 3:19:35  lr: 0.000010  loss: 7.4454  loss_lm: 7.0985 (7.1007)  time: 2.6147  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 142/4722]  eta: 3:19:34  lr: 0.000010  loss: 6.9752  loss_lm: 7.1309 (7.0862)  time: 2.6146  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 142/4722]  eta: 3:19:34  lr: 0.000010  loss: 6.8175  loss_lm: 7.0249 (7.0478)  time: 2.6146  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 142/4722]  eta: 3:19:35  lr: 0.000010  loss: 6.9510  loss_lm: 7.1045 (7.1294)  time: 2.6146  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 143/4722]  eta: 3:14:42  lr: 0.000010  loss: 6.7484  loss_lm: 7.0719 (7.0980)  time: 2.5513  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 143/4722]  eta: 3:14:44  lr: 0.000010  loss: 7.4703  loss_lm: 7.0763 (7.0729)  time: 2.5517  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 143/4722]  eta: 3:14:41  lr: 0.000010  loss: 6.8124  loss_lm: 6.9501 (7.0347)  time: 2.5512  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 143/4722]  eta: 3:14:42  lr: 0.000010  loss: 7.0917  loss_lm: 7.0917 (7.1002)  time: 2.5512  data: 0.0000  max mem: 18936Train: data epoch: [0]  [ 143/4722]  eta: 3:14:41  lr: 0.000010  loss: 7.0317  loss_lm: 7.0646 (7.1240)  time: 2.5512  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 143/4722]  eta: 3:14:41  lr: 0.000010  loss: 6.8880  loss_lm: 7.0519 (7.0752)  time: 2.5511  data: 0.0000  max mem: 18864


Train: data epoch: [0]  [ 144/4722]  eta: 3:10:20  lr: 0.000010  loss: 6.8794  loss_lm: 7.0719 (7.0865)  time: 2.4946  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 144/4722]  eta: 3:10:22  lr: 0.000010  loss: 6.9064  loss_lm: 7.0763 (7.0641)  time: 2.4950  data: 0.0000  max mem: 19015Train: data epoch: [0]  [ 144/4722]  eta: 3:10:19  lr: 0.000010  loss: 6.6053  loss_lm: 6.9501 (7.0121)  time: 2.4945  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 144/4722]  eta: 3:10:19  lr: 0.000010  loss: 7.1554  loss_lm: 7.1309 (7.0794)  time: 2.4945  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 144/4722]  eta: 3:10:19  lr: 0.000010  loss: 7.0317  loss_lm: 7.0917 (7.0966)  time: 2.4945  data: 0.0000  max mem: 18936

Train: data epoch: [0]  [ 144/4722]  eta: 3:10:19  lr: 0.000010  loss: 6.9286  loss_lm: 7.0646 (7.1137)  time: 2.4945  data: 0.0000  max mem: 18866
2023-11-13 00:16:16,051 [INFO] Saving checkpoint at iters: 145 and epoch: 0
2023-11-13 00:16:16,055 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:16:21,104 [INFO] Saved successfully
2023-11-13 00:16:21,105 [INFO] Averaged stats: lr: 0.0000  loss: 7.0623  loss_lm: 7.0623
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:43  lr: 0.000010  loss: 7.0273  loss_lm: 7.0340 (7.0623)  time: 2.6968  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:40  lr: 0.000010  loss: 7.2242  loss_lm: 7.1309 (7.0866)  time: 2.6962  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:40  lr: 0.000010  loss: 7.0370  loss_lm: 7.0370 (7.1098)  time: 2.6963  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:41  lr: 0.000010  loss: 7.3841  loss_lm: 7.0719 (7.1014)  time: 2.6964  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:40  lr: 0.000010  loss: 6.9580  loss_lm: 6.9501 (7.0094)  time: 2.6962  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 145/4722]  eta: 3:25:40  lr: 0.000010  loss: 7.4089  loss_lm: 7.0917 (7.1122)  time: 2.6963  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 146/4722]  eta: 3:21:13  lr: 0.000010  loss: 7.0604  loss_lm: 7.0604 (7.0995)  time: 2.5619  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 146/4722]  eta: 3:21:15  lr: 0.000010  loss: 6.6788  loss_lm: 7.0273 (7.0440)  time: 2.5620  data: 0.0000  max mem: 19015Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 7.1248  loss_lm: 7.0917 (7.1128)  time: 2.5618  data: 0.0000  max mem: 18936

Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 6.9360  loss_lm: 7.0519 (7.0794)  time: 2.5618  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 7.0039  loss_lm: 7.0317 (7.1048)  time: 2.5619  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 146/4722]  eta: 3:21:12  lr: 0.000010  loss: 6.8806  loss_lm: 6.9434 (7.0033)  time: 2.5619  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 147/4722]  eta: 3:17:10  lr: 0.000010  loss: 7.0479  loss_lm: 7.0588 (7.0971)  time: 2.5618  data: 0.0000  max mem: 18951
Train: data epoch: [0]  [ 147/4722]  eta: 3:17:09  lr: 0.000010  loss: 6.9785  loss_lm: 7.0340 (7.0749)  time: 2.5617  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 147/4722]  eta: 3:17:09  lr: 0.000010  loss: 7.0491  loss_lm: 6.9434 (7.0054)  time: 2.5618  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 147/4722]  eta: 3:17:12  lr: 0.000010  loss: 7.0165  loss_lm: 7.0165 (7.0428)  time: 2.5619  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 147/4722]  eta: 3:17:10  lr: 0.000010  loss: 7.0150  loss_lm: 7.0317 (7.1084)  time: 2.5618  data: 0.0000  max mem: 18936Train: data epoch: [0]  [ 147/4722]  eta: 3:17:10  lr: 0.000010  loss: 7.1769  loss_lm: 7.0370 (7.1081)  time: 2.5618  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 148/4722]  eta: 3:13:27  lr: 0.000010  loss: 7.1975  loss_lm: 7.0588 (7.1015)  time: 2.5619  data: 0.0000  max mem: 18951Train: data epoch: [0]  [ 148/4722]  eta: 3:13:27  lr: 0.000010  loss: 7.0434  loss_lm: 7.0434 (7.0735)  time: 2.5618  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 148/4722]  eta: 3:13:29  lr: 0.000010  loss: 6.3164  loss_lm: 7.0165 (7.0112)  time: 2.5620  data: 0.0000  max mem: 19015
Train: data epoch: [0]  [ 148/4722]  eta: 3:13:27  lr: 0.000010  loss: 7.0286  loss_lm: 7.0286 (7.1049)  time: 2.5618  data: 0.0000  max mem: 18936
Train: data epoch: [0]  [ 148/4722]  eta: 3:13:27  lr: 0.000010  loss: 7.0930  loss_lm: 6.9434 (7.0092)  time: 2.5619  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 148/4722]  eta: 3:13:27  lr: 0.000010  loss: 7.0182  loss_lm: 7.0317 (7.1042)  time: 2.5619  data: 0.0000  max mem: 18866
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
[2023-11-13 00:19:47.104091] Starting job[2023-11-13 00:19:47.104093] Starting job[2023-11-13 00:19:47.104092] Starting job


[2023-11-13 00:19:47.104098] Starting job[2023-11-13 00:19:47.104097] Starting job

[2023-11-13 00:19:47.104123] Starting job
| distributed init (rank 1, world 12): env://
| distributed init (rank 5, world 12): env://
| distributed init (rank 2, world 12): env://
| distributed init (rank 3, world 12): env://
| distributed init (rank 4, world 12): env://
| distributed init (rank 0, world 12): env://
2023-11-13 00:19:50,394 [INFO] 
=====  Running Parameters    =====
2023-11-13 00:19:50,394 [INFO] {
    "amp": true,
    "batch_size_eval": 10,
    "batch_size_train": 10,
    "checkpoint_freq": 5,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "eval_freq": 2,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 1e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 20,
    "min_len": 5,
    "min_lr": 0,
    "num_beams": 3,
    "num_workers": 3,
    "output_dir": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint",
    "rank": 0,
    "resume_ckpt_path": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth",
    "runner": "runner_base",
    "seed": 42,
    "task": "captioning",
    "tensorboard_path": "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/logs",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "weight_decay": 0.05,
    "world_size": 12
}
2023-11-13 00:19:50,394 [INFO] 
======  Dataset Attributes  ======
2023-11-13 00:19:50,394 [INFO] 
======== coco_caption =======
2023-11-13 00:19:50,395 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "/mnt/.node1/Open-Datasets/coco"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "eval": {
            "name": "blip_caption"
        },
        "train": {
            "name": "blip_caption",
            "prompt": "a picture of "
        }
    },
    "vis_processor": {
        "eval": {
            "name": "blip_image_eval"
        },
        "train": {
            "name": "blip_image_train"
        }
    }
}
2023-11-13 00:19:50,395 [INFO] 
======  Model Attributes  ======
2023-11-13 00:19:50,395 [INFO] {
    "arch": "blip_caption",
    "finetuned": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth",
    "image_size": 384,
    "load_finetuned": false,
    "load_pretrained": false,
    "med_config_path": "configs/models/med_large_config.json",
    "model_type": "large_coco",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth",
    "prompt": "a picture of ",
    "vit_ckpt_layer": 5,
    "vit_grad_ckpt": true,
    "vit_type": "large"
}
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/stanford/LAVIS/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-11-13 00:19:50,397 [INFO] Building datasets...
2023-11-13 00:20:05,756 [INFO] number of trainable parameters: 446290492
2023-11-13 00:20:06,056 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-11-13 00:20:06,056 [INFO] Loaded 566747 records for train split from the dataset.
2023-11-13 00:20:06,056 [INFO] Loaded 5000 records for val split from the dataset.
2023-11-13 00:20:06,056 [INFO] Loaded 5000 records for test split from the dataset.
2023-11-13 00:20:06,057 [INFO] Resume checkpoint from /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth
2023-11-13 00:20:06,059 [INFO] Start training
2023-11-13 00:20:06,070 [INFO] Start training epoch 0, 4722 iters per inner epoch.
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 146/4722]  eta: 5:19:51  lr: 0.000010  loss: 6.9711  loss_lm: 6.9711 (6.9711)  time: 4.1939  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 146/4722]  eta: 5:20:07  lr: 0.000010  loss: 6.6652  loss_lm: 6.6652 (6.6652)  time: 4.1974  data: 0.0000  max mem: 18863

Train: data epoch: [0]  [ 146/4722]  eta: 5:19:42  lr: 0.000010  loss: 6.8763  loss_lm: 6.8763 (6.8763)  time: 4.1920  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 146/4722]  eta: 5:19:37  lr: 0.000010  loss: 6.9428  loss_lm: 6.9428 (6.9428)  time: 4.1909  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 146/4722]  eta: 5:19:45  lr: 0.000010  loss: 7.1193  loss_lm: 7.1193 (7.1193)  time: 4.1926  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 146/4722]  eta: 5:20:02  lr: 0.000010  loss: 7.0583  loss_lm: 7.0583 (7.0583)  time: 4.1962  data: 0.0000  max mem: 18863
2023-11-13 00:20:10,280 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [ 147/4722]  eta: 3:36:24  lr: 0.000010  loss: 7.0591  loss_lm: 7.0583 (7.0587)  time: 2.8380  data: 0.0000  max mem: 18863Train: data epoch: [0]  [ 147/4722]  eta: 3:36:26  lr: 0.000010  loss: 7.0346  loss_lm: 6.6652 (6.8499)  time: 2.8387  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 147/4722]  eta: 3:36:18  lr: 0.000010  loss: 7.1742  loss_lm: 6.9711 (7.0726)  time: 2.8369  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 147/4722]  eta: 3:36:14  lr: 0.000010  loss: 7.0538  loss_lm: 6.8763 (6.9650)  time: 2.8359  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 147/4722]  eta: 3:36:11  lr: 0.000010  loss: 6.9940  loss_lm: 6.9428 (6.9684)  time: 2.8354  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 147/4722]  eta: 3:36:15  lr: 0.000010  loss: 7.0213  loss_lm: 7.0213 (7.0703)  time: 2.8362  data: 0.0000  max mem: 18866

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 148/4722]  eta: 3:01:36  lr: 0.000010  loss: 7.0305  loss_lm: 7.0305 (7.0586)  time: 2.3822  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 148/4722]  eta: 3:01:41  lr: 0.000010  loss: 6.3132  loss_lm: 6.6652 (6.6710)  time: 2.3834  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 148/4722]  eta: 3:01:39  lr: 0.000010  loss: 7.1597  loss_lm: 7.0591 (7.0923)  time: 2.3829  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 148/4722]  eta: 3:01:33  lr: 0.000010  loss: 7.0937  loss_lm: 7.0538 (7.0079)  time: 2.3815  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 148/4722]  eta: 3:01:31  lr: 0.000010  loss: 7.0384  loss_lm: 6.9940 (6.9918)  time: 2.3811  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 148/4722]  eta: 3:01:33  lr: 0.000010  loss: 7.0074  loss_lm: 7.0213 (7.0493)  time: 2.3817  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 149/4722]  eta: 2:44:16  lr: 0.000010  loss: 6.9413  loss_lm: 6.9711 (7.0293)  time: 2.1554  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 149/4722]  eta: 2:44:20  lr: 0.000010  loss: 6.7272  loss_lm: 6.6652 (6.6850)  time: 2.1563  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 149/4722]  eta: 2:44:14  lr: 0.000010  loss: 7.2335  loss_lm: 7.0538 (7.0643)  time: 2.1549  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 149/4722]  eta: 2:44:15  lr: 0.000010  loss: 6.6816  loss_lm: 7.0074 (6.9574)  time: 2.1551  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 149/4722]  eta: 2:44:13  lr: 0.000010  loss: 7.0724  loss_lm: 6.9940 (7.0119)  time: 2.1546  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 149/4722]  eta: 2:44:19  lr: 0.000010  loss: 6.8968  loss_lm: 7.0583 (7.0434)  time: 2.1560  data: 0.0000  max mem: 18864
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:20:16,178 [INFO] Saving checkpoint at iters: 150 and epoch: 0
2023-11-13 00:20:16,184 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:20:21,558 [INFO] Saved successfully
2023-11-13 00:20:21,559 [INFO] Averaged stats: lr: 0.0000  loss: 6.7424  loss_lm: 6.7424
Train: data epoch: [0]  [ 150/4722]  eta: 3:55:56  lr: 0.000010  loss: 6.9717  loss_lm: 6.7272 (6.7424)  time: 3.0964  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 150/4722]  eta: 3:55:51  lr: 0.000010  loss: 6.9248  loss_lm: 7.0538 (7.0364)  time: 3.0953  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 150/4722]  eta: 3:55:55  lr: 0.000010  loss: 6.8801  loss_lm: 7.0583 (7.0108)  time: 3.0961  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 150/4722]  eta: 3:55:52  lr: 0.000010  loss: 7.2707  loss_lm: 7.0213 (7.0201)  time: 3.0954  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 150/4722]  eta: 3:55:50  lr: 0.000010  loss: 6.9020  loss_lm: 6.9940 (6.9899)  time: 3.0950  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 150/4722]  eta: 3:55:53  lr: 0.000010  loss: 6.8335  loss_lm: 6.9711 (6.9901)  time: 3.0957  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:25  lr: 0.000010  loss: 6.6512  loss_lm: 6.9248 (6.9722)  time: 2.8277  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:26  lr: 0.000010  loss: 7.1158  loss_lm: 6.9711 (7.0111)  time: 2.8280  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:24  lr: 0.000010  loss: 7.0401  loss_lm: 6.9940 (6.9983)  time: 2.8275  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:25  lr: 0.000010  loss: 6.6708  loss_lm: 7.0074 (6.9619)  time: 2.8278  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 151/4722]  eta: 3:35:28  lr: 0.000010  loss: 7.0048  loss_lm: 7.0048 (7.0098)  time: 2.8284  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 151/4722]  eta: 3:35:30  lr: 0.000010  loss: 7.1460  loss_lm: 6.7272 (6.8096)  time: 2.8287  data: 0.0000  max mem: 18865

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 152/4722]  eta: 3:20:44  lr: 0.000010  loss: 7.2274  loss_lm: 7.0384 (7.0310)  time: 2.6356  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 152/4722]  eta: 3:20:45  lr: 0.000010  loss: 7.3090  loss_lm: 7.0538 (7.0203)  time: 2.6358  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 152/4722]  eta: 3:20:46  lr: 0.000010  loss: 6.8204  loss_lm: 6.9711 (6.9838)  time: 2.6361  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 152/4722]  eta: 3:20:45  lr: 0.000010  loss: 7.2765  loss_lm: 7.0213 (7.0068)  time: 2.6359  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 152/4722]  eta: 3:20:48  lr: 0.000010  loss: 6.9219  loss_lm: 7.0048 (6.9972)  time: 2.6364  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 152/4722]  eta: 3:20:49  lr: 0.000010  loss: 7.0513  loss_lm: 6.9717 (6.8442)  time: 2.6367  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 153/4722]  eta: 3:09:40  lr: 0.000010  loss: 6.6171  loss_lm: 6.9248 (6.9699)  time: 2.4908  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 153/4722]  eta: 3:09:42  lr: 0.000010  loss: 7.4185  loss_lm: 7.0048 (7.0499)  time: 2.4913  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 153/4722]  eta: 3:09:43  lr: 0.000010  loss: 6.9677  loss_lm: 6.9677 (6.8596)  time: 2.4916  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 153/4722]  eta: 3:09:39  lr: 0.000010  loss: 6.8062  loss_lm: 6.9940 (7.0029)  time: 2.4906  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 153/4722]  eta: 3:09:40  lr: 0.000010  loss: 6.9078  loss_lm: 7.0074 (6.9944)  time: 2.4908  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 153/4722]  eta: 3:09:41  lr: 0.000010  loss: 6.9539  loss_lm: 6.9539 (6.9801)  time: 2.4910  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 154/4722]  eta: 3:01:05  lr: 0.000010  loss: 6.6876  loss_lm: 7.0048 (7.0096)  time: 2.3787  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 154/4722]  eta: 3:01:03  lr: 0.000010  loss: 6.8911  loss_lm: 6.9940 (6.9905)  time: 2.3781  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 154/4722]  eta: 3:01:03  lr: 0.000010  loss: 7.2285  loss_lm: 7.0538 (6.9987)  time: 2.3782  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 154/4722]  eta: 3:01:04  lr: 0.000010  loss: 7.0603  loss_lm: 6.9711 (6.9890)  time: 2.3784  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 154/4722]  eta: 3:01:03  lr: 0.000010  loss: 7.1789  loss_lm: 7.0213 (7.0149)  time: 2.3783  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 154/4722]  eta: 3:01:06  lr: 0.000010  loss: 6.8983  loss_lm: 6.9677 (6.8639)  time: 2.3789  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:20:28,964 [INFO] Saving checkpoint at iters: 155 and epoch: 0
2023-11-13 00:20:28,969 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:20:34,249 [INFO] Saved successfully
2023-11-13 00:20:34,250 [INFO] Averaged stats: lr: 0.0000  loss: 6.8347  loss_lm: 6.8347
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:24  lr: 0.000010  loss: 6.5722  loss_lm: 6.8983 (6.8347)  time: 2.8169  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:23  lr: 0.000010  loss: 7.1667  loss_lm: 7.0048 (7.0253)  time: 2.8167  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:22  lr: 0.000010  loss: 6.4948  loss_lm: 6.9248 (6.9483)  time: 2.8163  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 155/4722]  eta: 3:34:22  lr: 0.000010  loss: 7.0251  loss_lm: 7.0213 (7.0159)  time: 2.8163  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 155/4722]  eta: 3:34:22  lr: 0.000010  loss: 6.9665  loss_lm: 6.9665 (6.9868)  time: 2.8165  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 155/4722]  eta: 3:34:21  lr: 0.000010  loss: 7.3958  loss_lm: 6.9940 (7.0310)  time: 2.8162  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 156/4722]  eta: 3:25:05  lr: 0.000010  loss: 6.9071  loss_lm: 6.9248 (6.9445)  time: 2.6951  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 156/4722]  eta: 3:25:08  lr: 0.000010  loss: 6.9983  loss_lm: 6.9677 (6.8496)  time: 2.6957  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 156/4722]  eta: 3:25:06  lr: 0.000010  loss: 6.7982  loss_lm: 6.9665 (6.9696)  time: 2.6952  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 156/4722]  eta: 3:25:05  lr: 0.000010  loss: 6.6738  loss_lm: 6.9940 (6.9985)  time: 2.6949  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 156/4722]  eta: 3:25:07  lr: 0.000010  loss: 7.1132  loss_lm: 7.0583 (7.0333)  time: 2.6954  data: 0.0000  max mem: 18864Train: data epoch: [0]  [ 156/4722]  eta: 3:25:05  lr: 0.000010  loss: 7.3796  loss_lm: 7.0251 (7.0490)  time: 2.6951  data: 0.0000  max mem: 18866

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 157/4722]  eta: 3:17:20  lr: 0.000010  loss: 6.9485  loss_lm: 7.0213 (7.0406)  time: 2.5939  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 157/4722]  eta: 3:17:21  lr: 0.000010  loss: 7.0132  loss_lm: 6.9665 (6.9732)  time: 2.5940  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 157/4722]  eta: 3:17:20  lr: 0.000010  loss: 6.6977  loss_lm: 6.9071 (6.9240)  time: 2.5938  data: 0.0000  max mem: 18995Train: data epoch: [0]  [ 157/4722]  eta: 3:17:20  lr: 0.000010  loss: 7.0772  loss_lm: 6.9940 (7.0051)  time: 2.5937  data: 0.0000  max mem: 18865



Train: data epoch: [0]  [ 157/4722]  eta: 3:17:22  lr: 0.000010  loss: 6.7715  loss_lm: 7.0048 (7.0115)  time: 2.5942  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 157/4722]  eta: 3:17:23  lr: 0.000010  loss: 7.4222  loss_lm: 6.9677 (6.8973)  time: 2.5944  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:45  lr: 0.000010  loss: 6.7400  loss_lm: 6.9071 (6.9098)  time: 2.5077  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:45  lr: 0.000010  loss: 7.2064  loss_lm: 7.0251 (7.0534)  time: 2.5077  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:44  lr: 0.000010  loss: 7.1362  loss_lm: 7.0384 (7.0152)  time: 2.5076  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:47  lr: 0.000010  loss: 7.1513  loss_lm: 6.9717 (6.9169)  time: 2.5082  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:46  lr: 0.000010  loss: 7.0017  loss_lm: 7.0048 (7.0108)  time: 2.5080  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 158/4722]  eta: 3:10:45  lr: 0.000010  loss: 6.9748  loss_lm: 6.9711 (6.9734)  time: 2.5079  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 159/4722]  eta: 3:05:05  lr: 0.000010  loss: 6.9524  loss_lm: 6.9071 (6.9128)  time: 2.4338  data: 0.0000  max mem: 18995Train: data epoch: [0]  [ 159/4722]  eta: 3:05:06  lr: 0.000010  loss: 6.5520  loss_lm: 7.0017 (6.9780)  time: 2.4341  data: 0.0000  max mem: 18864

Train: data epoch: [0]  [ 159/4722]  eta: 3:05:05  lr: 0.000010  loss: 7.2482  loss_lm: 7.0384 (7.0318)  time: 2.4337  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 159/4722]  eta: 3:05:06  lr: 0.000010  loss: 7.4372  loss_lm: 6.9711 (7.0065)  time: 2.4339  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 159/4722]  eta: 3:05:05  lr: 0.000010  loss: 6.8814  loss_lm: 7.0213 (7.0411)  time: 2.4338  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 159/4722]  eta: 3:05:07  lr: 0.000010  loss: 6.3622  loss_lm: 6.9677 (6.8772)  time: 2.4343  data: 0.0000  max mem: 18865
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:20:41,638 [INFO] Saving checkpoint at iters: 160 and epoch: 0
2023-11-13 00:20:41,643 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:20:46,909 [INFO] Saved successfully
2023-11-13 00:20:46,910 [INFO] Averaged stats: lr: 0.0000  loss: 6.8772  loss_lm: 6.8772
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:56  lr: 0.000010  loss: 6.8774  loss_lm: 6.9677 (6.8772)  time: 2.7218  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:54  lr: 0.000010  loss: 6.8211  loss_lm: 7.0213 (7.0264)  time: 2.7213  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:54  lr: 0.000010  loss: 6.5482  loss_lm: 6.9071 (6.8885)  time: 2.7213  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 160/4722]  eta: 3:26:55  lr: 0.000010  loss: 6.2820  loss_lm: 6.9711 (6.9582)  time: 2.7214  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 160/4722]  eta: 3:26:54  lr: 0.000010  loss: 6.5171  loss_lm: 7.0384 (6.9975)  time: 2.7212  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 160/4722]  eta: 3:26:55  lr: 0.000010  loss: 7.1183  loss_lm: 7.0048 (6.9873)  time: 2.7216  data: 0.0000  max mem: 18864


Train: data epoch: [0]  [ 161/4722]  eta: 3:21:17  lr: 0.000010  loss: 6.7954  loss_lm: 6.9665 (6.9480)  time: 2.6480  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 161/4722]  eta: 3:21:16  lr: 0.000010  loss: 6.7643  loss_lm: 6.9940 (6.9829)  time: 2.6478  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 161/4722]  eta: 3:21:17  lr: 0.000010  loss: 7.0547  loss_lm: 6.9071 (6.8989)  time: 2.6479  data: 0.0000  max mem: 18995Train: data epoch: [0]  [ 161/4722]  eta: 3:21:17  lr: 0.000010  loss: 6.9961  loss_lm: 7.0074 (7.0245)  time: 2.6479  data: 0.0000  max mem: 18866

Train: data epoch: [0]  [ 161/4722]  eta: 3:21:18  lr: 0.000010  loss: 6.6752  loss_lm: 7.0017 (6.9678)  time: 2.6481  data: 0.0000  max mem: 18864
Train: data epoch: [0]  [ 161/4722]  eta: 3:21:19  lr: 0.000010  loss: 6.9421  loss_lm: 6.9421 (6.8813)  time: 2.6483  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:01  lr: 0.000010  loss: 6.7528  loss_lm: 6.9940 (6.9694)  time: 2.5793  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:02  lr: 0.000010  loss: 6.9629  loss_lm: 6.9665 (6.9489)  time: 2.5794  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:01  lr: 0.000010  loss: 7.0622  loss_lm: 6.9248 (6.9085)  time: 2.5794  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:02  lr: 0.000010  loss: 6.7744  loss_lm: 7.0017 (6.9565)  time: 2.5796  data: 0.0000  max mem: 18879
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:03  lr: 0.000010  loss: 6.5714  loss_lm: 6.9421 (6.8631)  time: 2.5798  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 162/4722]  eta: 3:16:01  lr: 0.000010  loss: 6.4444  loss_lm: 7.0074 (6.9904)  time: 2.5794  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 163/4722]  eta: 3:11:18  lr: 0.000010  loss: 6.8483  loss_lm: 6.9961 (6.9825)  time: 2.5179  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 163/4722]  eta: 3:11:18  lr: 0.000010  loss: 7.2491  loss_lm: 6.9940 (6.9849)  time: 2.5178  data: 0.0000  max mem: 18865Train: data epoch: [0]  [ 163/4722]  eta: 3:11:18  lr: 0.000010  loss: 6.9959  loss_lm: 6.9248 (6.9134)  time: 2.5179  data: 0.0000  max mem: 18995

Train: data epoch: [0]  [ 163/4722]  eta: 3:11:19  lr: 0.000010  loss: 7.4310  loss_lm: 6.9665 (6.9757)  time: 2.5180  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 163/4722]  eta: 3:11:19  lr: 0.000010  loss: 6.6286  loss_lm: 6.9219 (6.9383)  time: 2.5181  data: 0.0000  max mem: 18879
Train: data epoch: [0]  [ 163/4722]  eta: 3:11:20  lr: 0.000010  loss: 7.5420  loss_lm: 6.9421 (6.9008)  time: 2.5183  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:06  lr: 0.000010  loss: 7.1814  loss_lm: 7.0384 (6.9953)  time: 2.4630  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:06  lr: 0.000010  loss: 6.9352  loss_lm: 6.9665 (6.9736)  time: 2.4631  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:06  lr: 0.000010  loss: 6.9088  loss_lm: 6.9248 (6.9131)  time: 2.4631  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:07  lr: 0.000010  loss: 7.0072  loss_lm: 7.0017 (6.9419)  time: 2.4632  data: 0.0000  max mem: 18879
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:08  lr: 0.000010  loss: 6.8751  loss_lm: 6.9421 (6.8994)  time: 2.4634  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 164/4722]  eta: 3:07:06  lr: 0.000010  loss: 6.9469  loss_lm: 6.9961 (6.9806)  time: 2.4631  data: 0.0000  max mem: 18866
2023-11-13 00:20:54,364 [INFO] Saving checkpoint at iters: 165 and epoch: 0
2023-11-13 00:20:54,369 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
2023-11-13 00:20:59,545 [INFO] Saved successfully
2023-11-13 00:20:59,546 [INFO] Averaged stats: lr: 0.0000  loss: 6.8973  loss_lm: 6.8973
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:01  lr: 0.000010  loss: 6.8571  loss_lm: 6.8983 (6.8973)  time: 2.6732  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:01  lr: 0.000010  loss: 6.6261  loss_lm: 6.9219 (6.9261)  time: 2.6730  data: 0.0000  max mem: 18879
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:00  lr: 0.000010  loss: 6.4410  loss_lm: 6.9088 (6.8895)  time: 2.6729  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:00  lr: 0.000010  loss: 7.4927  loss_lm: 6.9665 (6.9995)  time: 2.6729  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 165/4722]  eta: 3:22:59  lr: 0.000010  loss: 6.7667  loss_lm: 6.9940 (6.9838)  time: 2.6728  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 165/4722]  eta: 3:23:00  lr: 0.000010  loss: 6.7045  loss_lm: 6.9485 (6.9668)  time: 2.6729  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:39  lr: 0.000010  loss: 6.9323  loss_lm: 6.9219 (6.9264)  time: 2.5371  data: 0.0000  max mem: 18879
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:38  lr: 0.000010  loss: 6.6908  loss_lm: 6.9469 (6.9537)  time: 2.5371  data: 0.0000  max mem: 18866
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:38  lr: 0.000010  loss: 7.2480  loss_lm: 6.9665 (7.0113)  time: 2.5371  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:38  lr: 0.000010  loss: 7.0735  loss_lm: 6.9248 (6.8983)  time: 2.5371  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:37  lr: 0.000010  loss: 7.0048  loss_lm: 7.0048 (6.9848)  time: 2.5371  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 166/4722]  eta: 3:18:39  lr: 0.000010  loss: 6.8395  loss_lm: 6.8983 (6.8946)  time: 2.5373  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:42  lr: 0.000010  loss: 6.8072  loss_lm: 6.8774 (6.8906)  time: 2.5375  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:41  lr: 0.000010  loss: 6.8729  loss_lm: 6.9088 (6.8971)  time: 2.5373  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:42  lr: 0.000010  loss: 6.9949  loss_lm: 6.9219 (6.9295)  time: 2.5373  data: 0.0000  max mem: 18879Train: data epoch: [0]  [ 167/4722]  eta: 3:14:40  lr: 0.000010  loss: 7.2317  loss_lm: 7.0384 (6.9961)  time: 2.5373  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 167/4722]  eta: 3:14:41  lr: 0.000010  loss: 6.4678  loss_lm: 6.9629 (6.9866)  time: 2.5373  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 167/4722]  eta: 3:14:41  lr: 0.000010  loss: 6.5552  loss_lm: 6.9078 (6.9356)  time: 2.5373  data: 0.0000  max mem: 18866
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [ 168/4722]  eta: 3:11:04  lr: 0.000010  loss: 6.8783  loss_lm: 6.8783 (6.8901)  time: 2.5376  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 168/4722]  eta: 3:11:03  lr: 0.000010  loss: 6.7841  loss_lm: 6.9071 (6.8922)  time: 2.5375  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 168/4722]  eta: 3:11:03  lr: 0.000010  loss: 7.1166  loss_lm: 6.9219 (6.9376)  time: 2.5375  data: 0.0000  max mem: 18977Train: data epoch: [0]  [ 168/4722]  eta: 3:11:03  lr: 0.000010  loss: 6.4593  loss_lm: 6.8814 (6.9149)  time: 2.5375  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 168/4722]  eta: 3:11:03  lr: 0.000010  loss: 6.7683  loss_lm: 6.9539 (6.9771)  time: 2.5375  data: 0.0000  max mem: 19014

Train: data epoch: [0]  [ 168/4722]  eta: 3:11:02  lr: 0.000010  loss: 6.8539  loss_lm: 7.0048 (6.9899)  time: 2.5375  data: 0.0000  max mem: 18865

Train: data epoch: [0]  [ 169/4722]  eta: 3:07:42  lr: 0.000010  loss: 7.0550  loss_lm: 6.9071 (6.8990)  time: 2.5373  data: 0.0000  max mem: 18995
Train: data epoch: [0]  [ 169/4722]  eta: 3:07:42  lr: 0.000010  loss: 6.7375  loss_lm: 6.9219 (6.9293)  time: 2.5373  data: 0.0000  max mem: 18977
Train: data epoch: [0]  [ 169/4722]  eta: 3:07:42  lr: 0.000010  loss: 6.7598  loss_lm: 6.9539 (6.9681)  time: 2.5373  data: 0.0000  max mem: 19014
Train: data epoch: [0]  [ 169/4722]  eta: 3:07:43  lr: 0.000010  loss: 6.8283  loss_lm: 6.8783 (6.8875)  time: 2.5374  data: 0.0000  max mem: 18865
Train: data epoch: [0]  [ 169/4722]  eta: 3:07:42  lr: 0.000010  loss: 6.6347  loss_lm: 6.8814 (6.9032)  time: 2.5373  data: 0.0000  max mem: 18866Train: data epoch: [0]  [ 169/4722]  eta: 3:07:41  lr: 0.000010  loss: 7.0468  loss_lm: 7.0048 (6.9923)  time: 2.5373  data: 0.0000  max mem: 18865

/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
2023-11-13 00:21:06,937 [INFO] Saving checkpoint at iters: 170 and epoch: 0
2023-11-13 00:21:06,942 [INFO] Saving checkpoint at epoch 0 to /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/checkpoint/caption_latest.pth.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/train.py", line 33, in <module>
    from lavis.runners import *
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/__init__.py", line 8, in <module>
    from lavis.runners.runner_base import RunnerBase
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/lavis/runners/runner_base.py", line 14, in <module>
    import torch.utils.tensorboard as tensorboard
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3879583) of binary: /mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/bin/python
Traceback (most recent call last):
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/Client/Lachlahy4daeusijealkmjsh6qf6mchi/laclacoyiihruvjzhb3kmexw7qlks5yy/isc-demos/stanford/LAVIS/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-11-13_00:24:37
  host      : sc1
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3879584)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-11-13_00:24:37
  host      : sc1
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3879585)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-11-13_00:24:37
  host      : sc1
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3879586)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-11-13_00:24:37
  host      : sc1
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 3879587)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-11-13_00:24:37
  host      : sc1
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 3879588)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-13_00:24:37
  host      : sc1
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3879583)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

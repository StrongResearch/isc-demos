import os
from typing import List

import timm
import torch

__all__ = ["create_linear_cls", "HRNetLinearCls"]


def create_linear_cls(
    backbone: str,
    num_classes: int,
    num_channels: int,
    ckpt_path: os.PathLike,
    freeze: bool = True,
    prefix: str = "encoder.",
    model_keys: List[str] = None,
    reset_head: bool = True,
    **kwargs,
):
    """Models generated by timm with everything frozen by default except for the fc layer."""

    # create model
    kwargs = {
        **kwargs,
        "model_name": backbone,
        "in_chans": num_channels,
        "num_classes": num_classes,
    }

    model = timm.create_model(**kwargs)
    new_keys = list(model.state_dict().keys())

    if ckpt_path is not None:
        state_dict = torch.load(ckpt_path, map_location="cpu")

        # find the state_dict for nested models
        if model_keys is not None:
            for key in model_keys:
                state_dict = state_dict[key]

        if isinstance(prefix, str):
            for k in list(state_dict.keys()):
                if k.startswith(prefix):
                    state_dict[k[len(prefix) :]] = state_dict[k]  # noqa: E203

                del state_dict[k]

        # Loading state dict
        if reset_head:
            missing_keys, _ = model.load_state_dict(state_dict, strict=False)
            print(f"=> Checkpoint loaded [path={ckpt_path}]")

            if "fc.weight" in new_keys and "fc.bias" in new_keys:
                assert missing_keys == ["fc.weight", "fc.bias"]

            if "head.weight" in new_keys and "head.bias" in new_keys:
                assert missing_keys == ["head.weight", "head.bias"]

            if "classifier.weight" in new_keys and "classifier.bias" in new_keys:
                assert missing_keys == ["classifier.weight", "classifier.bias"]

        else:
            model.load_state_dict(state_dict, strict=True)
            print(f"=> Checkpoint [unknown model] loaded [path={ckpt_path}]")

    else:
        print("=> Warning: no checkpoint was loaded!")

    # Freeze everything but the classification layer
    if freeze:
        for name, param in model.named_parameters():
            if name not in [
                "fc.weight",
                "fc.bias",
                "head.weight",
                "head.bias",
                "classifier.weight",
                "classifier.bias",
            ]:
                param.requires_grad = False

    # Reinitialize the classification layer
    if reset_head:
        if "fc.weight" in new_keys:
            model.fc.weight.data.normal_(mean=0.0, std=0.01)

        if "fc.bias" in new_keys:
            model.fc.bias.data.zero_()

        if "head.weight" in new_keys:
            model.head.weight.data.normal_(mean=0.0, std=0.01)

        if "head.bias" in new_keys:
            model.head.bias.data.zero_()

        if "classifier.weight" in new_keys:
            model.classifier.weight.data.normal_(mean=0.0, std=0.01)

        if "classifier.bias" in new_keys:
            model.classifier.bias.data.zero_()

    return model


class HRNetLinearCls(torch.nn.Module):
    def __init__(self, model_name, num_classes, in_chans, pretrained):
        super().__init__()
        self.net = timm.create_model(
            model_name, num_classes=num_classes, in_chans=in_chans, pretrained=pretrained
        )
        for name, param in self.net.named_parameters():
            # if name.startswith('classifier') is False:# and name.startswith('final_layer') is False:
            if name.startswith("classifier") is False:
                param.requires_grad = False

    def forward(self, input):
        return self.net(input)
